{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# miller\n",
    "\n",
    "---\n",
    "\n",
    "- \"Miller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON\" \n",
    "\n",
    "- Github [repo](https://github.com/johnkerl/miller), [intro](http://johnkerl.org/miller/doc/10-min.html), \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "# remove preinstalled versions (usually outdated)\n",
    "sudo apt remove miller\n",
    "\n",
    "# download the new file\n",
    "wget https://github.com/johnkerl/miller/releases/download/v5.1.0/mlr-5.1.0.tar.gz\n",
    "\n",
    "# untar\n",
    "tar -xzf mlr-5.1.0.tar.gz\n",
    "\n",
    "# the usual\n",
    "cd mlr-5.1.0.tar.gz\n",
    "./configure\n",
    "make\n",
    "sudo make install\n",
    "\n",
    "# check if the mlr executable is in /usr/bin (it could have been placed in /usr/local/bin)\n",
    "# move it if required to /usr/bin/mlr\n",
    "sudo cp /usr/local/bin/mlr /usr/bin/mlr\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "\n",
    "- useful for data cleaning, data reduction, statistical reporting, format conversion and so on.\n",
    "\n",
    "\n",
    "- written in C\n",
    "\n",
    "- is **format-aware**, and retains headers\n",
    "\n",
    "- has high-throughput performance on par with the Unix toolkit\n",
    "\n",
    "- complements `dplyr` and `pandas` by helping you clean-filter-aggregate your data for EDA\n",
    "\n",
    "- **in-place** mutations to files\n",
    "\n",
    "- But most importantly,\n",
    "\n",
    "> Miller is **streaming**; most operations need only a single record in memory at a time (rather needing to hold the entire file in RAM). <br> Miller retains only as much data as needed for operations like `sort` and `stats`, so you can **operate on files which are larger than RAM**\n",
    "\n",
    "\n",
    "\n",
    "> Miller complements **data-analysis tools** such as **R**, **pandas**, etc.:\n",
    "you can use Miller to **clean** and **prepare** your data. While you can do\n",
    "**basic statistics** entirely in Miller, its streaming-data feature and\n",
    "single-pass algorithms enable you to **reduce very large data sets**.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Commands\n",
    "\n",
    "- Syntax\n",
    "\n",
    "```bash\n",
    "mlr <command> <options>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "|Commands|Description|\n",
    "|---|---|\n",
    "|`cat, cut, grep, head, join, sort, tac, tail, top, uniq`|Analogs of their Unix-toolkit namesakes, discussed below as well as in Miller features in the context of the Unix toolkit|\n",
    "|`filter, put, sec2gmt, sec2gmtdate, step, tee`|awk-like functionality|\n",
    "|`bar, bootstrap, decimate, histogram, least-frequent, most-frequent, sample, shuffle, stats1, stats2`|Statistically oriented|\n",
    "|`group-by, group-like, having-fields`|Particularly oriented toward Record-heterogeneity, although all Miller commands can handle heterogeneous records|\n",
    "|`check, count-distinct, label, merge-fields, nest, nothing, rename, rename, reorder, reshape, seqgen`|These draw from other sources (see also How original is Miller?): count-distinct is SQL-ish, and rename can be done by sed (which does it faster: see Performance).|\n",
    "\n",
    "---\n",
    "\n",
    "All Verbs:\n",
    "\n",
    "```\n",
    "   bar bootstrap cat check count-distinct cut decimate filter grep group-by\n",
    "   group-like having-fields head histogram join label least-frequent\n",
    "   merge-fields most-frequent nest nothing fraction put regularize rename\n",
    "   reorder repeat reshape sample sec2gmt sec2gmtdate seqgen shuffle sort stats1\n",
    "   stats2 step tac tail tee top uniq unsparsify\n",
    "```\n",
    "\n",
    "Functions for the `filter` and `put` verbs:\n",
    "\n",
    "```            \n",
    "   + + - - * / // % ** | ^ & ~ << >> == != =~ !=~ > >= < <= && || ^^ ! ? : .\n",
    "   gsub strlen sub substr tolower toupper abs acos acosh asin asinh atan atan2\n",
    "   atanh cbrt ceil cos cosh erf erfc exp expm1 floor invqnorm log log10 log1p\n",
    "   logifit madd max mexp min mmul msub pow qnorm round roundm sgn sin sinh sqrt\n",
    "   tan tanh urand urand32 urandint dhms2fsec dhms2sec fsec2dhms fsec2hms\n",
    "   gmt2sec hms2fsec hms2sec sec2dhms sec2gmt sec2gmt sec2gmtdate sec2hms\n",
    "   strftime strptime systime is_absent is_bool is_boolean is_empty is_empty_map\n",
    "   is_float is_int is_map is_nonempty_map is_not_empty is_not_map is_not_null\n",
    "   is_null is_numeric is_present is_string asserting_absent asserting_bool\n",
    "   asserting_boolean asserting_empty asserting_empty_map asserting_float\n",
    "   asserting_int asserting_map asserting_nonempty_map asserting_not_empty\n",
    "   asserting_not_map asserting_not_null asserting_null asserting_numeric\n",
    "   asserting_present asserting_string boolean float fmtnum hexfmt int string\n",
    "   typeof depth haskey joink joinkv joinv leafcount length mapdiff mapsum\n",
    "   splitkv splitkvx splitnv splitnvx\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Options\n",
    "\n",
    "\n",
    "- Use `--csv, --pprint` etc. when the input and output formats are the same.\n",
    "- Use `--icsv --opprint`, etc. when you want format conversion\n",
    "- Use the `mlr -I` flag **to process files in-place**, for example\n",
    "- PLEASE USE `mlr --csv --rs lf` FOR NATIVE UN*X (LINEFEED-TERMINATED) CSV FILES.\n",
    "\n",
    "```bash\n",
    "mlr -I --csv cut -x -f <unwanted_column_name> mydata/*.csv \n",
    "# will remove unwanted_column_name from all your *.csv files in your mydata/ subdirectory.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Examples\n",
    "\n",
    "\n",
    "```bash\n",
    "mlr --csv cut -f hostname,uptime mydata.csv\n",
    "# Both input and output in csv\n",
    "\n",
    "mlr --csv --rs lf --fs tab cut -f hostname,uptime file1.tsv file2.tsv\n",
    "# Read tsv (--fs tab) created on unix (--rs lf) and retain named columns, concat into a csv files (--csv)\n",
    "\n",
    "\n",
    "mlr --csv filter '$status != \"down\" && $upsec >= 10000' *.csv\n",
    "# Retain specific rows\n",
    "\n",
    "mlr --nidx put '$sum = $7 + 2.1*$8' *.dat\n",
    "# NIDX: implicitly numerically indexed (Unix-toolkit style)\n",
    "# create a new column from the values in the 7th and 8th columns\n",
    "\n",
    "grep -v '^#' /etc/group | mlr --ifs : --nidx --opprint label group,pass,gid,member then sort -f group\n",
    "# Ignore rows that begin with '#', input file is colon separated, rename columns, then sort and groupby\n",
    "\n",
    "mlr join -j account_id -f accounts.dat then group-by account_name balances.dat\n",
    "# \n",
    "\n",
    "mlr put '$attr = sub($attr, \"([0-9]+)_([0-9]+)_.*\", \"\\1:\\2\")' data/*\n",
    "#\n",
    "\n",
    "mlr stats1 -a min,mean,max,p10,p50,p90 -f flag,u,v data/*\n",
    "#\n",
    "\n",
    "mlr stats2 -a linreg-pca -f u,v -g shape data/*\n",
    "#\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `mlr rename`\n",
    "\n",
    "- Renames specified fields.\n",
    "- Usage: mlr rename [options] {old1,new1,old2,new2,...}\n",
    "    - use `-g` for global replacement, and `-r` for regex matching\n",
    "    \n",
    "    \n",
    "```\n",
    "Examples:\n",
    "mlr rename old_name,new_name\n",
    "mlr rename old_name_1,new_name_1,old_name_2,new_name_2\n",
    "mlr rename -r 'Date_[0-9]+,Date,'  Rename all such fields to be \"Date\"\n",
    "mlr rename -r '\"Date_[0-9]+\",Date' Same\n",
    "mlr rename -r 'Date_([0-9]+).*,\\1' Rename all such fields to be of the form 20151015\n",
    "mlr rename -r '\"name\"i,Name'       Rename \"name\", \"Name\", \"NAME\", etc. to \"Name\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace spaces with underscores\n",
    "!mlr --csv --ifs '|' --ofs ',' rename -g -r ' ,_' ./raw/Sales.txt > ./cleaned/Sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: POSSales_PK\n",
      "  2: Date_FK\n",
      "  3: Date\n",
      "  4: Store_FK\n",
      "  5: Item_FK\n",
      "  6: Item_PK\n",
      "  7: Promo_FK\n",
      "  8: POSSales_Ticket_No\n",
      "  9: POSSales_GiftList_No\n",
      " 10: POSSales_GiftListLine_No\n",
      " 11: POSSales_Sales_Quantity\n",
      " 12: POSSales_Sales_AmountExVAT\n",
      " 13: POSSales_Sales_AmountInVAT\n",
      " 14: POSSales_Cost_Amount\n",
      " 15: POSSales_Margin_Amount\n",
      " 16: POSSales_Discount_Amount\n",
      " 17: Store_No_BK\n",
      " 18: POS_Terminal_No_BK\n",
      " 19: Transaction_No_BK\n",
      " 20: Line_No_BK\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n ./cleaned/Sales.csv\n",
    "# or !xsv headers ./cleaned/Sales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `mlr cat, head, tail`\n",
    "\n",
    "- `mlr head` and `mlr tail` count records rather than lines\n",
    "- they always return the CSV header\n",
    "    - `mlr head -n 5 myfile.csv` will return 6 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlr: unacceptable empty CSV key at file \"./raw/flights.csv\" line 1.\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv cat flights.csv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- This is a very common error. \n",
    "- Caused becaused files generated on Unix-like systems have LF line terminators while RFC compliant CSVs have CRLF line terminators (default in miller)\n",
    "- fix by including `--rs lf`\n",
    "    - this says that the record separator (`rs`) is `lf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\n",
      "2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,SMF,ONT,389,4,11,0,,0,0,0,0,0,0\n",
      "2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,SMF,PDX,479,5,6,0,,0,0,0,0,0,0\n",
      "2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,SMF,PDX,479,6,9,0,,0,3,0,0,0,31\n",
      "2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,SMF,PDX,479,3,8,0,,0,23,0,0,0,3\n",
      "2007,1,1,1,831,830,957,1000,WN,2278,N480,86,90,74,-3,1,SMF,PDX,479,3,9,0,,0,0,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv --rs lf head -n 5 flights.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read files with other delimiters by specifying `ifs` (or input field separator)\n",
    "    - could be useful for file format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSSales_PK,Date_FK,Date,Store_FK,Item_FK,Item_PK,Promo_FK,POSSales_Ticket No,POSSales_GiftList No,POSSales_GiftListLine No,POSSales_Sales Quantity,POSSales_Sales AmountExVAT,POSSales_Sales AmountInVAT,POSSales_Cost Amount,POSSales_Margin Amount,POSSales_Discount Amount,Store No_BK,POS Terminal No_BK,Transaction No_BK,Line No_BK\n",
      "42169332,41639,2014-01-02 00:00:00,17,316213,20337325,806,3810195837,,0,\"1,00000000000000000000\",\"17,32231000000000000000\",\"20,96000000000000000000\",\"9,98000000000000000000\",\"7,34231000000000000000\",\"-8,99000000000000000000\",S038,P0381,226847,10000\n",
      "42169333,41639,2014-01-02 00:00:00,17,274932,20194564,812,3810195837,,0,\"1,00000000000000000000\",\"43,38017000000000000000\",\"52,49000000000000000000\",\"35,86000000000000000000\",\"7,52017000000000000000\",\",00000000000000000000\",S038,P0381,226847,20000\n",
      "42169334,41639,2014-01-02 00:00:00,17,326727,20347663,63,3810195838,,0,\"1,00000000000000000000\",\"6,60331000000000000000\",\"7,99000000000000000000\",\"5,21000000000000000000\",\"1,39331000000000000000\",\",00000000000000000000\",S038,P0381,226848,10000\n",
      "42169335,41639,2014-01-02 00:00:00,17,311837,20332760,63,3810195838,,0,\"1,00000000000000000000\",\"24,78512000000000000000\",\"29,99000000000000000000\",\"12,23000000000000000000\",\"12,55512000000000000000\",\",00000000000000000000\",S038,P0381,226848,20000\n",
      "42169336,41639,2014-01-02 00:00:00,17,262025,20181754,63,3810195838,,0,\"1,00000000000000000000\",\"12,38843000000000000000\",\"14,99000000000000000000\",\"6,34000000000000000000\",\"6,04843000000000000000\",\",00000000000000000000\",S038,P0381,226848,30000\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv --ifs '|' head -n 5 ./raw/Sales.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------+-----------+--------+-------|\n",
      "|  UniqueCarrier | FlightNum | Origin | Dest  |\n",
      "|----------------+-----------+--------+-------|\n",
      "|  WN            | 2891      | SMF    | ONT   |\n",
      "|  XE            | 2809      | CLE    | CLT   |\n",
      "|  YV            | 2827      | ABQ    | PHX   |\n",
      "|  OH            | 5026      | SAT    | CVG   |\n",
      "|  OO            | 3664      | SUN    | SLC   |\n",
      "|  UA            | 1         | ORD    | HNL   |\n",
      "|  US            | 290       | ABQ    | LAS   |\n",
      "|  DL            | 1772      | ATL    | PNS   |\n",
      "|  EV            | 4083      | ATL    | RDU   |\n",
      "|----------------+-----------+--------+-------|\n"
     ]
    }
   ],
   "source": [
    "# get the first record from every group that appears in the data\n",
    "!mlr --csv --rs lf head -n 1 -g UniqueCarrier then cut -f Origin,Dest,UniqueCarrier,FlightNum flights.csv \\\n",
    "| head | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Chaining\n",
    "\n",
    "- Output of one verb may be chained as input to another using \"then\", e.g.\n",
    "  \n",
    "```\n",
    "mlr stats1 -a min,mean,max -f flag,u,v -g color then sort -f color\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### `mlr cat`\n",
    "\n",
    "- very useful for format conversion (`txt` -> `csv`)\n",
    "    - fast!! (under a minute for a 4GB `txt` file)\n",
    "    - and concatenating multiple same-schema CSV file\n",
    "  \n",
    "---  \n",
    "    \n",
    "```\n",
    "mlr cat [options]\n",
    "Passes input records directly to output. Most useful for format conversion.\n",
    "Options:\n",
    "-n                                 Prepend field \"n\" to each record with record-counter starting at 1\n",
    "-g {comma-separated field name(s)} When used with -n/-N, writes record-counters\n",
    "                                   keyed by specified field name(s).\n",
    "-N {name}                          Prepend field {name} to each record with record-counter starting at 1\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/home/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kddcup.data\r\n",
      "kddcup.names\r\n"
     ]
    }
   ],
   "source": [
    "!ls | grep kdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration\r\n",
      "protocol_type\r\n",
      "service\r\n",
      "flag\r\n",
      "src_bytes\r\n",
      "dst_bytes\r\n",
      "land\r\n",
      "wrong_fragment\r\n",
      "urgent\r\n",
      "hot\r\n",
      "num_failed_logins\r\n",
      "logged_in\r\n",
      "num_compromised\r\n",
      "root_shell\r\n",
      "su_attempted\r\n",
      "num_root\r\n",
      "num_file_creations\r\n",
      "num_shells\r\n",
      "num_access_files\r\n",
      "num_outbound_cmds\r\n",
      "is_host_login\r\n",
      "is_guest_login\r\n",
      "count\r\n",
      "srv_count\r\n",
      "serror_rate\r\n",
      "srv_serror_rate\r\n",
      "rerror_rate\r\n",
      "srv_rerror_rate\r\n",
      "same_srv_rate\r\n",
      "diff_srv_rate\r\n",
      "srv_diff_host_rate\r\n",
      "dst_host_count\r\n",
      "dst_host_srv_count\r\n",
      "dst_host_same_srv_rate\r\n",
      "dst_host_diff_srv_rate\r\n",
      "dst_host_same_src_port_rate\r\n",
      "dst_host_srv_diff_host_rate\r\n",
      "dst_host_serror_rate\r\n",
      "dst_host_srv_serror_rate\r\n",
      "dst_host_rerror_rate\r\n",
      "dst_host_srv_rerror_rate\r\n"
     ]
    }
   ],
   "source": [
    "!cat kddcup.names | sed 1d | cut -d: -f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting pipe-delimited to csv\n",
    "!mlr --csv --ifs '|' cat ./raw/Sales.txt > ./cleaned/Sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSSales_PK,Date_FK,Date,Store_FK,Item_FK,Item_PK,Promo_FK,POSSales_Ticket No,POSSales_GiftList No,POSSales_GiftListLine No,POSSales_Sales Quantity,POSSales_Sales AmountExVAT,POSSales_Sales AmountInVAT,POSSales_Cost Amount,POSSales_Margin Amount,POSSales_Discount Amount,Store No_BK,POS Terminal No_BK,Transaction No_BK,Line No_BK\n",
      "42169332,41639,2014-01-02 00:00:00,17,316213,20337325,806,3810195837,,0,\"1,00000000000000000000\",\"17,32231000000000000000\",\"20,96000000000000000000\",\"9,98000000000000000000\",\"7,34231000000000000000\",\"-8,99000000000000000000\",S038,P0381,226847,10000\n",
      "42169333,41639,2014-01-02 00:00:00,17,274932,20194564,812,3810195837,,0,\"1,00000000000000000000\",\"43,38017000000000000000\",\"52,49000000000000000000\",\"35,86000000000000000000\",\"7,52017000000000000000\",\",00000000000000000000\",S038,P0381,226847,20000\n",
      "42169334,41639,2014-01-02 00:00:00,17,326727,20347663,63,3810195838,,0,\"1,00000000000000000000\",\"6,60331000000000000000\",\"7,99000000000000000000\",\"5,21000000000000000000\",\"1,39331000000000000000\",\",00000000000000000000\",S038,P0381,226848,10000\n",
      "42169335,41639,2014-01-02 00:00:00,17,311837,20332760,63,3810195838,,0,\"1,00000000000000000000\",\"24,78512000000000000000\",\"29,99000000000000000000\",\"12,23000000000000000000\",\"12,55512000000000000000\",\",00000000000000000000\",S038,P0381,226848,20000\n",
      "42169336,41639,2014-01-02 00:00:00,17,262025,20181754,63,3810195838,,0,\"1,00000000000000000000\",\"12,38843000000000000000\",\"14,99000000000000000000\",\"6,34000000000000000000\",\"6,04843000000000000000\",\",00000000000000000000\",S038,P0381,226848,30000\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv head -n 5 ./cleaned/Sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n,Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\n",
      "2,2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,SMF,PDX,479,6,9,0,,0,3,0,0,0,31\n"
     ]
    }
   ],
   "source": [
    "# Create an index and extract a row from a specific index\n",
    "!mlr --csv --rs lf head -n 10 then cat -n then filter '$n==2' flights.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\n",
      "2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,SMF,PDX,479,5,6,0,,0,0,0,0,0,0\n",
      "2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,SMF,PDX,479,3,8,0,,0,23,0,0,0,3\n",
      "2007,1,1,1,1430,1420,1553,1550,WN,2386,N611SW,83,90,74,3,10,SMF,PDX,479,2,7,0,,0,0,0,0,0,0\n",
      "2007,1,1,1,944,935,1223,1225,WN,1131,N749SW,99,110,86,-2,9,SMF,PHX,647,4,9,0,,0,0,0,0,0,0\n",
      "2007,1,1,1,1318,1315,1603,1610,WN,2456,N630WN,105,115,92,-7,3,SMF,PHX,647,5,8,0,,0,0,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "# Even numbered rows (creating an index isnt necessary)\n",
    "!mlr --csv --rs lf head -n 10 then filter 'FNR%2==0' flights.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+---+---+---+------+------+------+------+----+-------|\n",
      "|  1    | 2 | 3 | 4 | 5    | 6    | 7    | 8    | 9  | 10    |\n",
      "|-------+---+---+---+------+------+------+------+----+-------|\n",
      "|  2007 | 1 | 1 | 1 | 1232 | 1225 | 1341 | 1340 | WN | 2891  |\n",
      "|  2007 | 1 | 1 | 1 | 1918 | 1905 | 2043 | 2035 | WN | 462   |\n",
      "|  2007 | 1 | 1 | 1 | 2206 | 2130 | 2334 | 2300 | WN | 1229  |\n",
      "|  2007 | 1 | 1 | 1 | 1230 | 1200 | 1356 | 1330 | WN | 1355  |\n",
      "|  2007 | 1 | 1 | 1 | 831  | 830  | 957  | 1000 | WN | 2278  |\n",
      "|  2007 | 1 | 1 | 1 | 1430 | 1420 | 1553 | 1550 | WN | 2386  |\n",
      "|  2007 | 1 | 1 | 1 | 1936 | 1840 | 2217 | 2130 | WN | 409   |\n",
      "|  2007 | 1 | 1 | 1 | 944  | 935  | 1223 | 1225 | WN | 1131  |\n",
      "|  2007 | 1 | 1 | 1 | 1537 | 1450 | 1819 | 1735 | WN | 1212  |\n",
      "|  2007 | 1 | 1 | 1 | 1318 | 1315 | 1603 | 1610 | WN | 2456  |\n",
      "|-------+---+---+---+------+------+------+------+----+-------|\n",
      "cut: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "#provide implicit header (auto numeric) to headerless files\n",
    "\n",
    "!cat flights.csv | sed 1d | cut -d, -f1-10 | head \\\n",
    "|mlr --csv --rs lf --implicit-csv-header cat \\\n",
    "|csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------+---+---+---+------+------+------+------+----+-------|\n",
      "|  a    | b | c | d | 5    | 6    | 7    | 8    | 9  | 10    |\n",
      "|-------+---+---+---+------+------+------+------+----+-------|\n",
      "|  2007 | 1 | 1 | 1 | 1232 | 1225 | 1341 | 1340 | WN | 2891  |\n",
      "|  2007 | 1 | 1 | 1 | 1918 | 1905 | 2043 | 2035 | WN | 462   |\n",
      "|  2007 | 1 | 1 | 1 | 2206 | 2130 | 2334 | 2300 | WN | 1229  |\n",
      "|  2007 | 1 | 1 | 1 | 1230 | 1200 | 1356 | 1330 | WN | 1355  |\n",
      "|  2007 | 1 | 1 | 1 | 831  | 830  | 957  | 1000 | WN | 2278  |\n",
      "|  2007 | 1 | 1 | 1 | 1430 | 1420 | 1553 | 1550 | WN | 2386  |\n",
      "|  2007 | 1 | 1 | 1 | 1936 | 1840 | 2217 | 2130 | WN | 409   |\n",
      "|  2007 | 1 | 1 | 1 | 944  | 935  | 1223 | 1225 | WN | 1131  |\n",
      "|  2007 | 1 | 1 | 1 | 1537 | 1450 | 1819 | 1735 | WN | 1212  |\n",
      "|  2007 | 1 | 1 | 1 | 1318 | 1315 | 1603 | 1610 | WN | 2456  |\n",
      "|-------+---+---+---+------+------+------+------+----+-------|\n",
      "cut: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# provide column names (if less than num_cols, the implicit names will be kept)\n",
    "\n",
    "!cat flights.csv | sed 1d | cut -d, -f1-10 | head \\\n",
    "|mlr --csv --rs lf --implicit-csv-header label a,b,c,d \\\n",
    "|csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `mlr cut`\n",
    "\n",
    "- select columns by name with `-f`\n",
    "- select all columns except some with `-x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---------+-------|\n",
      "|  Origin | Dest  |\n",
      "|---------+-------|\n",
      "|  SMF    | ONT   |\n",
      "|  SMF    | PDX   |\n",
      "|  SMF    | PDX   |\n",
      "|  SMF    | PDX   |\n",
      "|  SMF    | PDX   |\n",
      "|  SMF    | PDX   |\n",
      "|  SMF    | PHX   |\n",
      "|  SMF    | PHX   |\n",
      "|  SMF    | PHX   |\n",
      "|---------+-------|\n"
     ]
    }
   ],
   "source": [
    "# Print only Origin,Dest\n",
    "!head -n 10 flights.csv \\\n",
    "| mlr --csv --rs lf  cut -f Origin,Dest \\\n",
    "| csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\n",
      "2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,389,4,11,0,,0,0,0,0,0,0\n",
      "2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,479,5,6,0,,0,0,0,0,0,0\n",
      "2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,479,6,9,0,,0,3,0,0,0,31\n",
      "2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,479,3,8,0,,0,23,0,0,0,3\n"
     ]
    }
   ],
   "source": [
    "# print all except Origin, Dest\n",
    "!head -n 5 flights.csv \\\n",
    "| mlr --csv --rs lf  cut -x -f Origin,Dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   POSSales_Sales_AmountExVAT\n",
      "2   POSSales_Sales_AmountInVAT\n",
      "3   POSSales_Cost_Amount\n",
      "4   POSSales_Margin_Amount\n",
      "5   POSSales_Discount_Amount\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv head -n 10 then cut -r -f \"Amount\" ./cleaned/Sales.csv | xsv headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   POSSales_Sales_Quantity\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv head -n 10 then cut -r -f \"Quantity\" ./cleaned/Sales.csv | xsv headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mlr --csv head -n 10 then having-fields --all-matching 'Quantity' ./cleaned/Sales.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `mlr filter`\n",
    "\n",
    "- retain specific records\n",
    "\n",
    "```\n",
    "Examples:\n",
    "  mlr filter 'log10($count) > 4.0'\n",
    "  mlr filter 'FNR == 2'\n",
    "  mlr filter 'urand() < 0.001'\n",
    "  mlr filter '$color != \"blue\" && $value > 4.2'\n",
    "  mlr filter '($x<.5 && $y<.5) || ($x>.5 && $y>.5)'\n",
    "  mlr filter '($name =~ \"^sys.*east$\") || ($name =~ \"^dev.[0-9]+\"i)'\n",
    "  mlr filter '$ab = $a+$b; $cd = $c+$d; $ab != $cd'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---------+-------|\n",
      "|  Origin | Dest  |\n",
      "|---------+-------|\n",
      "|  SFO    | PHX   |\n",
      "|  SFO    | PHX   |\n",
      "|  SFO    | PHX   |\n",
      "|  SFO    | PHX   |\n",
      "|  SFO    | PHX   |\n",
      "|  SFO    | PHX   |\n",
      "|  SFO    | PHX   |\n",
      "|  SFO    | PHX   |\n",
      "|  SFO    | PHX   |\n",
      "|---------+-------|\n"
     ]
    }
   ],
   "source": [
    "# single condition\n",
    "!mlr --csv --rs lf filter '$Origin == \"SFO\"' flights.csv \\\n",
    "|csvcut -c Origin,Dest | head | csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------+-----------+--------+-------|\n",
      "|  UniqueCarrier | FlightNum | Origin | Dest  |\n",
      "|----------------+-----------+--------+-------|\n",
      "|  UA            | 136       | SFO    | DFW   |\n",
      "|  UA            | 336       | SFO    | DFW   |\n",
      "|  UA            | 336       | SFO    | DFW   |\n",
      "|  UA            | 336       | SFO    | DFW   |\n",
      "|  UA            | 336       | SFO    | DFW   |\n",
      "|  UA            | 336       | SFO    | DFW   |\n",
      "|  UA            | 336       | SFO    | DFW   |\n",
      "|  UA            | 336       | SFO    | DFW   |\n",
      "|  UA            | 336       | SFO    | DFW   |\n",
      "|----------------+-----------+--------+-------|\n"
     ]
    }
   ],
   "source": [
    "# compound logic\n",
    "!mlr --csv --rs lf filter '$Origin == \"SFO\" && $Dest == \"DFW\"' flights.csv \\\n",
    "|csvcut -c UniqueCarrier,FlightNum,Origin,Dest | head | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `mlr put`\n",
    "\n",
    "- derive new columns on the fly from existing ones\n",
    "- Adds/updates specified field(s). Expressions are semicolon-separated and must either be assignments, or evaluate to boolean.\n",
    "- Please use a dollar sign for field names and double-quotes for string literals.\n",
    "- Miller built-in variables are NF NR FNR FILENUM FILENAME PI E, and ENV\n",
    "\n",
    "\n",
    "```\n",
    "Examples:\n",
    "  mlr put '$y = log10($x); $z = sqrt($y)'\n",
    "  mlr put '$x>0.0 { $y=log10($x); $z=sqrt($y) }' # does {...} only if $x > 0.0\n",
    "  mlr put '$x>0.0;  $y=log10($x); $z=sqrt($y)'   # does all three statements\n",
    "  mlr put '$a =~ \"([a-z]+)_([0-9]+);  $b = \"left_\\1\"; $c = \"right_\\2\"'\n",
    "  mlr put '$a =~ \"([a-z]+)_([0-9]+) { $b = \"left_\\1\"; $c = \"right_\\2\" }'\n",
    "  mlr put '$filename = FILENAME'\n",
    "  mlr put '$colored_shape = $color . \"_\" . $shape'\n",
    "  mlr put '$y = cos($theta); $z = atan2($y, $x)'\n",
    "  mlr put '$name = sub($name, \"http.*com\"i, \"\")'\n",
    "  mlr put -q '@sum += $x; end {emit @sum}'\n",
    "  mlr put -q '@sum[$a] += $x; end {emit @sum, \"a\"}'\n",
    "  mlr put -q '@sum[$a][$b] += $x; end {emit @sum, \"a\", \"b\"}'\n",
    "  mlr put -q '@min=min(@min,$x);@max=max(@max,$x); end{emitf @min, @max}'\n",
    "  mlr put -q 'is_null(@xmax) || $x > @xmax {@xmax=$x; @recmax=$*}; end {emit @recmax}'\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---------+------+----------+-------------|\n",
      "|  Origin | Dest | Distance | Distance_2  |\n",
      "|---------+------+----------+-------------|\n",
      "|  SMF    | ONT  | 389      | 3.890000    |\n",
      "|  SMF    | PDX  | 479      | 4.790000    |\n",
      "|  SMF    | PDX  | 479      | 4.790000    |\n",
      "|  SMF    | PDX  | 479      | 4.790000    |\n",
      "|  SMF    | PDX  | 479      | 4.790000    |\n",
      "|  SMF    | PDX  | 479      | 4.790000    |\n",
      "|  SMF    | PHX  | 647      | 6.470000    |\n",
      "|  SMF    | PHX  | 647      | 6.470000    |\n",
      "|  SMF    | PHX  | 647      | 6.470000    |\n",
      "|---------+------+----------+-------------|\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c Origin,Dest,Distance flights.csv | head \\\n",
    "| mlr --csv --rs lf put '$Distance_2 = $Distance/100;' \\\n",
    "| csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,reachable\n",
      "barney,false\n",
      "betty,true\n",
      "fred,true\n",
      "wilma,1\n"
     ]
    }
   ],
   "source": [
    "!cat ./raw/het-bool.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name   reachable\n",
      "barney false\n",
      "betty  true\n",
      "fred   true\n",
      "wilma  true\n"
     ]
    }
   ],
   "source": [
    "!mlr --icsv --rs lf --opprint put '$reachable = boolean($reachable)' ./raw/het-bool.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name   reachable\n",
      "barney 0.000000\n",
      "betty  1.000000\n",
      "fred   1.000000\n",
      "wilma  1.000000\n"
     ]
    }
   ],
   "source": [
    "!mlr --icsv --rs lf --opprint put '$reachable = float(boolean($reachable))' ./raw/het-bool.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name   reachable index\n",
      "barney false     1\n",
      "betty  true      2\n",
      "fred   true      3\n",
      "wilma  1         4\n"
     ]
    }
   ],
   "source": [
    "# Creating an index field\n",
    "!mlr --icsv --rs lf --opprint put '$index = NR' ./raw/het-bool.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Functions to use with `put, filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+\t+\t-\t-\t*\t/\t//\t%\t**\t|\t^\t&\t~\t<<\t>>\t==\t!=\t=~\t!=~\t>\t>=\t<\t<=\t&&\t||\t^^\t!\t? :\t.\tgsub\tstrlen\tsub\tsubstr\ttolower\ttoupper\tabs\tacos\tacosh\tasin\tasinh\tatan\tatan2\tatanh\tcbrt\tceil\tcos\tcosh\terf\terfc\texp\texpm1\tfloor\tinvqnorm\tlog\tlog10\tlog1p\tlogifit\tmadd\tmax\tmexp\tmin\tmmul\tmsub\tpow\tqnorm\tround\troundm\tsgn\tsin\tsinh\tsqrt\ttan\ttanh\turand\turand32\turandint\tdhms2fsec\tdhms2sec\tfsec2dhms\tfsec2hms\tgmt2sec\thms2fsec\thms2sec\tsec2dhms\tsec2gmt\tsec2gmt\tsec2gmtdate\tsec2hms\tstrftime\tstrptime\tsystime\tis_absent\tis_bool\tis_boolean\tis_empty\tis_empty_map\tis_float\tis_int\tis_map\tis_nonempty_map\tis_not_empty\tis_not_map\tis_not_null\tis_null\tis_numeric\tis_present\tis_string\tasserting_absent\tasserting_bool\tasserting_boolean\tasserting_empty\tasserting_empty_map\tasserting_float\tasserting_int\tasserting_map\tasserting_nonempty_map\tasserting_not_empty\tasserting_not_map\tasserting_not_null\tasserting_null\tasserting_numeric\tasserting_present\tasserting_string\tboolean\tfloat\tfmtnum\thexfmt\tint\tstring\ttypeof\tdepth\thaskey\tjoink\tjoinkv\tjoinv\tleafcount\tlength\tmapdiff\tmapsum\tsplitkv\tsplitkvx\tsplitnv\tsplitnvx\t"
     ]
    }
   ],
   "source": [
    "!mlr -F | tr '\\n' '\\t' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ (class=arithmetic #args=2): Addition.\n",
      "\n",
      "\n",
      "+ (class=arithmetic #args=1): Unary plus.\n",
      "\n",
      "\n",
      "- (class=arithmetic #args=2): Subtraction.\n",
      "\n",
      "\n",
      "- (class=arithmetic #args=1): Unary minus.\n",
      "\n",
      "\n",
      "* (class=arithmetic #args=2): Multiplication.\n",
      "\n",
      "\n",
      "/ (class=arithmetic #args=2): Division.\n",
      "\n",
      "\n",
      "// (class=arithmetic #args=2): Integer division: rounds to negative (pythonic).\n",
      "\n",
      "\n",
      "% (class=arithmetic #args=2): Remainder; never negative-valued (pythonic).\n",
      "\n",
      "\n",
      "** (class=arithmetic #args=2): Exponentiation; same as pow, but as an infix\n",
      "operator.\n",
      "\n",
      "\n",
      "| (class=arithmetic #args=2): Bitwise OR.\n",
      "\n",
      "\n",
      "^ (class=arithmetic #args=2): Bitwise XOR.\n",
      "\n",
      "\n",
      "& (class=arithmetic #args=2): Bitwise AND.\n",
      "\n",
      "\n",
      "~ (class=arithmetic #args=1): Bitwise NOT. Beware '$y=~$x' since =~ is the\n",
      "regex-match operator: try '$y = ~$x'.\n",
      "\n",
      "\n",
      "<< (class=arithmetic #args=2): Bitwise left-shift.\n",
      "\n",
      "\n",
      ">> (class=arithmetic #args=2): Bitwise right-shift.\n",
      "\n",
      "\n",
      "== (class=boolean #args=2): String/numeric equality. Mixing number and string\n",
      "results in string compare.\n",
      "\n",
      "\n",
      "!= (class=boolean #args=2): String/numeric inequality. Mixing number and string\n",
      "results in string compare.\n",
      "\n",
      "\n",
      "=~ (class=boolean #args=2): String (left-hand side) matches regex (right-hand\n",
      "side), e.g. '$name =~ \"^a.*b$\"'.\n",
      "\n",
      "\n",
      "!=~ (class=boolean #args=2): String (left-hand side) does not match regex\n",
      "(right-hand side), e.g. '$name !=~ \"^a.*b$\"'.\n",
      "\n",
      "\n",
      "> (class=boolean #args=2): String/numeric greater-than. Mixing number and string\n",
      "results in string compare.\n",
      "\n",
      "\n",
      ">= (class=boolean #args=2): String/numeric greater-than-or-equals. Mixing number\n",
      "and string results in string compare.\n",
      "\n",
      "\n",
      "< (class=boolean #args=2): String/numeric less-than. Mixing number and string\n",
      "results in string compare.\n",
      "\n",
      "\n",
      "<= (class=boolean #args=2): String/numeric less-than-or-equals. Mixing number\n",
      "and string results in string compare.\n",
      "\n",
      "\n",
      "&& (class=boolean #args=2): Logical AND.\n",
      "\n",
      "\n",
      "|| (class=boolean #args=2): Logical OR.\n",
      "\n",
      "\n",
      "^^ (class=boolean #args=2): Logical XOR.\n",
      "\n",
      "\n",
      "! (class=boolean #args=1): Logical negation.\n",
      "\n",
      "\n",
      "? : (class=boolean #args=3): Ternary operator.\n",
      "\n",
      "\n",
      ". (class=string #args=2): String concatenation.\n",
      "\n",
      "\n",
      "gsub (class=string #args=3): Example: '$name=gsub($name, \"old\", \"new\")'\n",
      "(replace all).\n",
      "\n",
      "\n",
      "strlen (class=string #args=1): String length.\n",
      "\n",
      "\n",
      "sub (class=string #args=3): Example: '$name=sub($name, \"old\", \"new\")'\n",
      "(replace once).\n",
      "\n",
      "\n",
      "substr (class=string #args=3): substr(s,m,n) gives substring of s from 0-up position m to n \n",
      "inclusive. Negative indices -len .. -1 alias to 0 .. len-1.\n",
      "\n",
      "\n",
      "tolower (class=string #args=1): Convert string to lowercase.\n",
      "\n",
      "\n",
      "toupper (class=string #args=1): Convert string to uppercase.\n",
      "\n",
      "\n",
      "abs (class=math #args=1): Absolute value.\n",
      "\n",
      "\n",
      "acos (class=math #args=1): Inverse trigonometric cosine.\n",
      "\n",
      "\n",
      "acosh (class=math #args=1): Inverse hyperbolic cosine.\n",
      "\n",
      "\n",
      "asin (class=math #args=1): Inverse trigonometric sine.\n",
      "\n",
      "\n",
      "asinh (class=math #args=1): Inverse hyperbolic sine.\n",
      "\n",
      "\n",
      "atan (class=math #args=1): One-argument arctangent.\n",
      "\n",
      "\n",
      "atan2 (class=math #args=2): Two-argument arctangent.\n",
      "\n",
      "\n",
      "atanh (class=math #args=1): Inverse hyperbolic tangent.\n",
      "\n",
      "\n",
      "cbrt (class=math #args=1): Cube root.\n",
      "\n",
      "\n",
      "ceil (class=math #args=1): Ceiling: nearest integer at or above.\n",
      "\n",
      "\n",
      "cos (class=math #args=1): Trigonometric cosine.\n",
      "\n",
      "\n",
      "cosh (class=math #args=1): Hyperbolic cosine.\n",
      "\n",
      "\n",
      "erf (class=math #args=1): Error function.\n",
      "\n",
      "\n",
      "erfc (class=math #args=1): Complementary error function.\n",
      "\n",
      "\n",
      "exp (class=math #args=1): Exponential function e**x.\n",
      "\n",
      "\n",
      "expm1 (class=math #args=1): e**x - 1.\n",
      "\n",
      "\n",
      "floor (class=math #args=1): Floor: nearest integer at or below.\n",
      "\n",
      "\n",
      "invqnorm (class=math #args=1): Inverse of normal cumulative distribution\n",
      "function. Note that invqorm(urand()) is normally distributed.\n",
      "\n",
      "\n",
      "log (class=math #args=1): Natural (base-e) logarithm.\n",
      "\n",
      "\n",
      "log10 (class=math #args=1): Base-10 logarithm.\n",
      "\n",
      "\n",
      "log1p (class=math #args=1): log(1-x).\n",
      "\n",
      "\n",
      "logifit (class=math #args=3): Given m and b from logistic regression, compute\n",
      "fit: $yhat=logifit($x,$m,$b).\n",
      "\n",
      "\n",
      "madd (class=math #args=3): a + b mod m (integers)\n",
      "\n",
      "\n",
      "max (class=math variadic): max of n numbers; null loses\n",
      "\n",
      "\n",
      "mexp (class=math #args=3): a ** b mod m (integers)\n",
      "\n",
      "\n",
      "min (class=math variadic): Min of n numbers; null loses\n",
      "\n",
      "\n",
      "mmul (class=math #args=3): a * b mod m (integers)\n",
      "\n",
      "\n",
      "msub (class=math #args=3): a - b mod m (integers)\n",
      "\n",
      "\n",
      "pow (class=math #args=2): Exponentiation; same as **.\n",
      "\n",
      "\n",
      "qnorm (class=math #args=1): Normal cumulative distribution function.\n",
      "\n",
      "\n",
      "round (class=math #args=1): Round to nearest integer.\n",
      "\n",
      "\n",
      "roundm (class=math #args=2): Round to nearest multiple of m: roundm($x,$m) is\n",
      "the same as round($x/$m)*$m\n",
      "\n",
      "\n",
      "sgn (class=math #args=1): +1 for positive input, 0 for zero input, -1 for\n",
      "negative input.\n",
      "\n",
      "\n",
      "sin (class=math #args=1): Trigonometric sine.\n",
      "\n",
      "\n",
      "sinh (class=math #args=1): Hyperbolic sine.\n",
      "\n",
      "\n",
      "sqrt (class=math #args=1): Square root.\n",
      "\n",
      "\n",
      "tan (class=math #args=1): Trigonometric tangent.\n",
      "\n",
      "\n",
      "tanh (class=math #args=1): Hyperbolic tangent.\n",
      "\n",
      "\n",
      "urand (class=math #args=0): Floating-point numbers on the unit interval.\n",
      "Int-valued example: '$n=floor(20+urand()*11)'.\n",
      "\n",
      "\n",
      "urand32 (class=math #args=0): Integer uniformly distributed 0 and 2**32-1\n",
      "inclusive.\n",
      "\n",
      "\n",
      "urandint (class=math #args=2): Integer uniformly distributed between inclusive\n",
      "integer endpoints.\n",
      "\n",
      "\n",
      "dhms2fsec (class=time #args=1): Recovers floating-point seconds as in\n",
      "dhms2fsec(\"5d18h53m20.250000s\") = 500000.250000\n",
      "\n",
      "\n",
      "dhms2sec (class=time #args=1): Recovers integer seconds as in\n",
      "dhms2sec(\"5d18h53m20s\") = 500000\n",
      "\n",
      "\n",
      "fsec2dhms (class=time #args=1): Formats floating-point seconds as in\n",
      "fsec2dhms(500000.25) = \"5d18h53m20.250000s\"\n",
      "\n",
      "\n",
      "fsec2hms (class=time #args=1): Formats floating-point seconds as in\n",
      "fsec2hms(5000.25) = \"01:23:20.250000\"\n",
      "\n",
      "\n",
      "gmt2sec (class=time #args=1): Parses GMT timestamp as integer seconds since\n",
      "the epoch.\n",
      "\n",
      "\n",
      "hms2fsec (class=time #args=1): Recovers floating-point seconds as in\n",
      "hms2fsec(\"01:23:20.250000\") = 5000.250000\n",
      "\n",
      "\n",
      "hms2sec (class=time #args=1): Recovers integer seconds as in\n",
      "hms2sec(\"01:23:20\") = 5000\n",
      "\n",
      "\n",
      "sec2dhms (class=time #args=1): Formats integer seconds as in sec2dhms(500000)\n",
      "= \"5d18h53m20s\"\n",
      "\n",
      "\n",
      "sec2gmt (class=time #args=1): Formats seconds since epoch (integer part)\n",
      "as GMT timestamp, e.g. sec2gmt(1440768801.7) = \"2015-08-28T13:33:21Z\".\n",
      "Leaves non-numbers as-is.\n",
      "\n",
      "\n",
      "sec2gmt (class=time #args=2): Formats seconds since epoch as GMT timestamp with n\n",
      "decimal places for seconds, e.g. sec2gmt(1440768801.7,1) = \"2015-08-28T13:33:21.7Z\".\n",
      "Leaves non-numbers as-is.\n",
      "\n",
      "\n",
      "sec2gmtdate (class=time #args=1): Formats seconds since epoch (integer part)\n",
      "as GMT timestamp with year-month-date, e.g. sec2gmtdate(1440768801.7) = \"2015-08-28\".\n",
      "Leaves non-numbers as-is.\n",
      "\n",
      "\n",
      "sec2hms (class=time #args=1): Formats integer seconds as in\n",
      "sec2hms(5000) = \"01:23:20\"\n",
      "\n",
      "\n",
      "strftime (class=time #args=2): Formats seconds since the epoch as timestamp, e.g.\n",
      "strftime(1440768801.7,\"%Y-%m-%dT%H:%M:%SZ\") = \"2015-08-28T13:33:21Z\", and\n",
      "strftime(1440768801.7,\"%Y-%m-%dT%H:%M:%3SZ\") = \"2015-08-28T13:33:21.700Z\".\n",
      "Format strings are as in the C library (please see \"man strftime\" on your system),\n",
      "with the Miller-specific addition of \"%1S\" through \"%9S\" which format the seocnds\n",
      "with 1 through 9 decimal places, respectively. (\"%S\" uses no decimal places.)\n",
      "\n",
      "\n",
      "strptime (class=time #args=2): Parses timestamp as floating-point seconds since the epoch,\n",
      "e.g. strptime(\"2015-08-28T13:33:21Z\",\"%Y-%m-%dT%H:%M:%SZ\") = 1440768801.000000,\n",
      "and  strptime(\"2015-08-28T13:33:21.345Z\",\"%Y-%m-%dT%H:%M:%SZ\") = 1440768801.345000.\n",
      "\n",
      "\n",
      "systime (class=time #args=0): Floating-point seconds since the epoch,\n",
      "e.g. 1440768801.748936.\n",
      "\n",
      "\n",
      "is_absent (class=typing #args=1): False if field is present in input, false otherwise\n",
      "\n",
      "\n",
      "is_bool (class=typing #args=1): True if field is present with boolean value. Synonymous with is_boolean.\n",
      "\n",
      "\n",
      "is_boolean (class=typing #args=1): True if field is present with boolean value. Synonymous with is_bool.\n",
      "\n",
      "\n",
      "is_empty (class=typing #args=1): True if field is present in input with empty string value, false otherwise.\n",
      "\n",
      "\n",
      "is_empty_map (class=typing #args=1): True if argument is a map which is empty.\n",
      "\n",
      "\n",
      "is_float (class=typing #args=1): True if field is present with value inferred to be float\n",
      "\n",
      "\n",
      "is_int (class=typing #args=1): True if field is present with value inferred to be int \n",
      "\n",
      "\n",
      "is_map (class=typing #args=1): True if argument is a map.\n",
      "\n",
      "\n",
      "is_nonempty_map (class=typing #args=1): True if argument is a map which is non-empty.\n",
      "\n",
      "\n",
      "is_not_empty (class=typing #args=1): False if field is present in input with empty value, false otherwise\n",
      "\n",
      "\n",
      "is_not_map (class=typing #args=1): True if argument is not a map.\n",
      "\n",
      "\n",
      "is_not_null (class=typing #args=1): False if argument is null (empty or absent), true otherwise.\n",
      "\n",
      "\n",
      "is_null (class=typing #args=1): True if argument is null (empty or absent), false otherwise.\n",
      "\n",
      "\n",
      "is_numeric (class=typing #args=1): True if field is present with value inferred to be int or float\n",
      "\n",
      "\n",
      "is_present (class=typing #args=1): True if field is present in input, false otherwise.\n",
      "\n",
      "\n",
      "is_string (class=typing #args=1): True if field is present with string (including empty-string) value\n",
      "\n",
      "\n",
      "asserting_absent (class=typing #args=1): Returns argument if it is absent in the input data, else\n",
      "throws an error.\n",
      "\n",
      "\n",
      "asserting_bool (class=typing #args=1): Returns argument if it is present with boolean value, else\n",
      "throws an error.\n",
      "\n",
      "\n",
      "asserting_boolean (class=typing #args=1): Returns argument if it is present with boolean value, else\n",
      "throws an error.\n",
      "\n",
      "\n",
      "asserting_empty (class=typing #args=1): Returns argument if it is present in input with empty value,\n",
      "else throws an error.\n",
      "\n",
      "\n",
      "asserting_empty_map (class=typing #args=1): Returns argument if it is a map with empty value, else\n",
      "throws an error.\n",
      "\n",
      "\n",
      "asserting_float (class=typing #args=1): Returns argument if it is present with float value, else\n",
      "throws an error.\n",
      "\n",
      "\n",
      "asserting_int (class=typing #args=1): Returns argument if it is present with int value, else\n",
      "throws an error.\n",
      "\n",
      "\n",
      "asserting_map (class=typing #args=1): Returns argument if it is a map, else throws an error.\n",
      "\n",
      "\n",
      "asserting_nonempty_map (class=typing #args=1): Returns argument if it is a non-empty map, else throws\n",
      "an error.\n",
      "\n",
      "\n",
      "asserting_not_empty (class=typing #args=1): Returns argument if it is present in input with non-empty\n",
      "value, else throws an error.\n",
      "\n",
      "\n",
      "asserting_not_map (class=typing #args=1): Returns argument if it is not a map, else throws an error.\n",
      "\n",
      "\n",
      "asserting_not_null (class=typing #args=1): Returns argument if it is non-null (non-empty and non-absent),\n",
      "else throws an error.\n",
      "\n",
      "\n",
      "asserting_null (class=typing #args=1): Returns argument if it is null (empty or absent), else throws\n",
      "an error.\n",
      "\n",
      "\n",
      "asserting_numeric (class=typing #args=1): Returns argument if it is present with int or float value,\n",
      "else throws an error.\n",
      "\n",
      "\n",
      "asserting_present (class=typing #args=1): Returns argument if it is present in input, else throws\n",
      "an error.\n",
      "\n",
      "\n",
      "asserting_string (class=typing #args=1): Returns argument if it is present with string (including\n",
      "empty-string) value, else throws an error.\n",
      "\n",
      "\n",
      "boolean (class=conversion #args=1): Convert int/float/bool/string to boolean.\n",
      "\n",
      "\n",
      "float (class=conversion #args=1): Convert int/float/bool/string to float.\n",
      "\n",
      "\n",
      "fmtnum (class=conversion #args=2): Convert int/float/bool to string using\n",
      "printf-style format string, e.g. '$s = fmtnum($n, \"%06lld\")'.\n",
      "\n",
      "\n",
      "hexfmt (class=conversion #args=1): Convert int to string, e.g. 255 to \"0xff\".\n",
      "\n",
      "\n",
      "int (class=conversion #args=1): Convert int/float/bool/string to int.\n",
      "\n",
      "\n",
      "string (class=conversion #args=1): Convert int/float/bool/string to string.\n",
      "\n",
      "\n",
      "typeof (class=conversion #args=1): Convert argument to type of argument (e.g.\n",
      "MT_STRING). For debug.\n",
      "\n",
      "\n",
      "depth (class=maps #args=1): Prints maximum depth of hashmap: ''. Scalars have depth 0.\n",
      "\n",
      "\n",
      "haskey (class=maps #args=2): True/false if map has/hasn't key, e.g. 'haskey($*, \"a\")' or\n",
      "'haskey(mymap, mykey)'. Error if 1st argument is not a map.\n",
      "\n",
      "\n",
      "joink (class=maps #args=2): Makes string from map keys. E.g. 'joink($*, \",\")'.\n",
      "\n",
      "\n",
      "joinkv (class=maps #args=3): Makes string from map key-value pairs. E.g. 'joinkv(@v[2], \"=\", \",\")'\n",
      "\n",
      "\n",
      "joinv (class=maps #args=2): Makes string from map keys. E.g. 'joinv(mymap, \",\")'.\n",
      "\n",
      "\n",
      "leafcount (class=maps #args=1): Counts total number of terminal values in hashmap. For single-level maps,\n",
      "same as length.\n",
      "\n",
      "\n",
      "length (class=maps #args=1): Counts number of top-level entries in hashmap. Scalars have length 1.\n",
      "\n",
      "\n",
      "mapdiff (class=maps variadic): With 0 args, returns empty map. With 1 arg, returns copy of arg.\n",
      "With 2 or more, returns copy of arg 1 with all keys from any of remaining argument maps removed.\n",
      "\n",
      "\n",
      "mapsum (class=maps variadic): With 0 args, returns empty map. With >= 1 arg, returns a map with\n",
      "key-value pairs from all arguments. Rightmost collisions win, e.g. 'mapsum({1:2,3,4},{1:5})' is '{1:5,3:4}'.\n",
      "\n",
      "\n",
      "splitkv (class=maps #args=3): Splits string by separators into map with type inference.\n",
      "E.g. 'splitkv(\"a=1,b=2,c=3\", \"=\", \",\")' gives '{\"a\" : 1, \"b\" : 2, \"c\" : 3}'.\n",
      "\n",
      "\n",
      "splitkvx (class=maps #args=3): Splits string by separators into map without type inference (keys and\n",
      "values are strings). E.g. 'splitkv(\"a=1,b=2,c=3\", \"=\", \",\")' gives\n",
      "'{\"a\" : \"1\", \"b\" : \"2\", \"c\" : \"3\"}'.\n",
      "\n",
      "\n",
      "splitnv (class=maps #args=2): Splits string by separator into integer-indexed map with type inference.\n",
      "E.g. 'splitnv(\"a,b,c\" , \",\")' gives '{1 : \"a\", 2 : \"b\", 3 : \"c\"}'.\n",
      "\n",
      "\n",
      "splitnvx (class=maps #args=2): Splits string by separator into integer-indexed map without type\n",
      "inference (values are strings). E.g. 'splitnv(\"4,5,6\" , \",\")' gives '{1 : \"4\", 2 : \"5\", 3 : \"6\"}'.\n",
      "\n",
      "To set the seed for urand, you may specify decimal or hexadecimal 32-bit\n",
      "numbers of the form \"mlr --seed 123456789\" or \"mlr --seed 0xcafefeed\".\n",
      "Miller's built-in variables are NF, NR, FNR, FILENUM, and FILENAME (awk-like)\n",
      "along with the mathematical constants PI and E.\n"
     ]
    }
   ],
   "source": [
    "!mlr -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   POSSales_PK\n",
      "2   Date_FK\n",
      "3   Date\n",
      "4   Store_FK\n",
      "5   Item_FK\n",
      "6   Item_PK\n",
      "7   Promo_FK\n",
      "8   POSSales_Ticket_No\n",
      "9   POSSales_GiftList_No\n",
      "10  POSSales_GiftListLine_No\n",
      "11  POSSales_Sales_Quantity\n",
      "12  POSSales_Sales_AmountExVAT\n",
      "13  POSSales_Sales_AmountInVAT\n",
      "14  POSSales_Cost_Amount\n",
      "15  POSSales_Margin_Amount\n",
      "16  POSSales_Discount_Amount\n",
      "17  Store_No_BK\n",
      "18  POS_Terminal_No_BK\n",
      "19  Transaction_No_BK\n",
      "20  Line_No_BK\n"
     ]
    }
   ],
   "source": [
    "!xsv headers ./cleaned/Sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSSales_Sales_AmountExVAT,POSSales_Sales_AmountExVAT2\n",
      "\"17,32231000000000000000\",17.322310\n",
      "\"43,38017000000000000000\",43.380170\n",
      "\"6,60331000000000000000\",6.603310\n",
      "\"24,78512000000000000000\",24.785120\n",
      "\"12,38843000000000000000\",12.388430\n",
      "\",94340000000000000000\",0.943400\n",
      "\"2,22314000000000000000\",2.223140\n",
      "\"10,70248000000000000000\",10.702480\n",
      "\"3,29752000000000000000\",3.297520\n",
      "\"28,91736000000000000000\",28.917360\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv head -n 10 \\\n",
    "then put '$POSSales_Sales_AmountExVAT2=float(gsub($POSSales_Sales_AmountExVAT,\",\",\".\"))' \\\n",
    "then cut -r -f 'POSSales_Sales_AmountExVAT' ./cleaned/Sales.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `sec2gmt, sec2gmtdate` for epoch timestamps\n",
    "\n",
    "- Replaces a numeric field representing seconds since the epoch with the corresponding GMT timestamp\n",
    "\n",
    "```\n",
    "mlr sec2gmt time1,time2\n",
    "# is the same as\n",
    "mlr put '$time1=sec2gmt($time1);$time2=sec2gmt($time2)'\n",
    "```\n",
    "\n",
    "\n",
    "- Alternatively, use the following functions with `put`\n",
    "\n",
    "```\n",
    "strftime: Formats seconds since epoch (integer part) as timestamp, e.g.\n",
    "    strftime(1440768801.7,\"%Y-%m-%dT%H:%M:%SZ\") = \"2015-08-28T13:33:21Z\".\n",
    "\n",
    "\n",
    "strptime: Parses timestamp as integer seconds since epoch, e.g. \n",
    "    strptime(\"2015-08-28T13:33:21Z\",\"%Y-%m-%dT%H:%M:%SZ\") = 1440768801.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2014-01-02 00:00:00\n",
      "2014-01-02 00:00:00\n",
      "2014-01-02 00:00:00\n",
      "2014-01-02 00:00:00\n",
      "2014-01-02 00:00:00\n",
      "2014-01-02 00:00:00\n",
      "2014-01-02 00:00:00\n",
      "2014-01-02 00:00:00\n",
      "2014-01-02 00:00:00\n",
      "2014-01-02 00:00:00\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv head -n 10 then cut -f 'Date' ./cleaned/Sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------+-------------|\n",
      "|  Date                | epoch       |\n",
      "|----------------------+-------------|\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|  2014-01-02 00:00:00 | 1388620800  |\n",
      "|----------------------+-------------|\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv head -n 10 \\\n",
    "then cut -f 'Date' \\\n",
    "then put '$epoch=strptime($Date, \"%Y-%m-%d %H:%M:%S\")' ./cleaned/Sales.csv \\\n",
    "| csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `mlr label`\n",
    "\n",
    "- use `--implicit-csv-header` with `mlr cat` to auto-create numeric column names\n",
    "- create column labels for files using `mlr <options> label <colnames>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1,col2,col3\n",
      "SMF,PDX,462\n",
      "SMF,PDX,1229\n",
      "SMF,PDX,1355\n",
      "SMF,PDX,2278\n",
      "SMF,PDX,2386\n",
      "SMF,PHX,409\n",
      "SMF,PHX,1131\n",
      "SMF,PHX,1212\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c Origin,Dest,FlightNum flights.csv | head | sed 1d \\\n",
    "| mlr --csv --rs lf label col1,col2,col3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `mlr rename`\n",
    "\n",
    "- Renames specified fields.\n",
    "- Usage: mlr rename [options] {old1,new1,old2,new2,...}\n",
    "    - use `-g` for global replacement, and `-r` for regex matching\n",
    "    \n",
    "    \n",
    "```\n",
    "Examples:\n",
    "mlr rename old_name,new_name\n",
    "mlr rename old_name_1,new_name_1,old_name_2,new_name_2\n",
    "mlr rename -r 'Date_[0-9]+,Date,'  Rename all such fields to be \"Date\"\n",
    "mlr rename -r '\"Date_[0-9]+\",Date' Same\n",
    "mlr rename -r 'Date_([0-9]+).*,\\1' Rename all such fields to be of the form 20151015\n",
    "mlr rename -r '\"name\"i,Name'       Rename \"name\", \"Name\", \"NAME\", etc. to \"Name\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace spaces with underscores\n",
    "!mlr --csv --ifs '|' --ofs ',' rename -g -r ' ,_' ./raw/Sales.txt > ./cleaned/Sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: POSSales_PK\n",
      "  2: Date_FK\n",
      "  3: Date\n",
      "  4: Store_FK\n",
      "  5: Item_FK\n",
      "  6: Item_PK\n",
      "  7: Promo_FK\n",
      "  8: POSSales_Ticket_No\n",
      "  9: POSSales_GiftList_No\n",
      " 10: POSSales_GiftListLine_No\n",
      " 11: POSSales_Sales_Quantity\n",
      " 12: POSSales_Sales_AmountExVAT\n",
      " 13: POSSales_Sales_AmountInVAT\n",
      " 14: POSSales_Cost_Amount\n",
      " 15: POSSales_Margin_Amount\n",
      " 16: POSSales_Discount_Amount\n",
      " 17: Store_No_BK\n",
      " 18: POS_Terminal_No_BK\n",
      " 19: Transaction_No_BK\n",
      " 20: Line_No_BK\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n ./cleaned/Sales.csv\n",
    "# or !xsv headers ./cleaned/Sales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Random Sampling with `bootstrap, sample, shuffle`\n",
    "\n",
    "- `bootstrap` is for sampling with replacement\n",
    "- The canonical use for bootstrap sampling is to put **error bars** (or, `confints`) on statistical quantities, such as the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniqueCarrier,WeatherDelay_mean\n",
      "NW,0.000000\n",
      "OH,2.000000\n",
      "DL,0.000000\n",
      "9E,0.000000\n",
      "AA,0.000000\n",
      "WN,0.000000\n",
      "UA,0.000000\n",
      "FL,0.000000\n",
      "OO,0.000000\n",
      "EV,46.833333\n",
      "US,0.000000\n",
      "YV,0.000000\n",
      "MQ,0.000000\n",
      "XE,0.000000\n",
      "AS,0.000000\n",
      "CO,0.000000\n",
      "AQ,0.000000\n",
      "HA,0.000000\n",
      "B6,0.000000\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv --rs lf bootstrap -n 100 then stats1 -a mean -f WeatherDelay -g UniqueCarrier flights.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- `sample` is for sampling `-k` records without replacement\n",
    "- Stratified if groups are specified with a `-g` switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\n",
      "2007,4,20,5,3,2353,221,209,US,19,N601AW,138,136,116,12,10,LAS,PDX,762,3,19,0,,0,0,0,0,0,0\n",
      "2007,3,21,3,902,900,1242,1153,YV,7176,N37342,160,113,93,49,2,ORD,CAE,666,4,63,0,,0,49,0,0,0,0\n",
      "2007,8,3,5,2115,2028,2248,2218,FL,59,N932AT,93,110,72,30,47,IAD,ATL,533,9,12,0,,0,0,0,0,0,30\n",
      "2007,10,12,5,810,820,947,1005,AA,1895,N5FHAA,157,165,139,-18,-10,MCO,DFW,984,6,12,0,,0,0,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv --rs lf sample -k 4 flights.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `mlr count-distinct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---------+------+--------|\n",
      "|  Origin | Dest | count  |\n",
      "|---------+------+--------|\n",
      "|  OGG    | HNL  | 16099  |\n",
      "|  HNL    | OGG  | 15876  |\n",
      "|  LAX    | LAS  | 14385  |\n",
      "|  LAS    | LAX  | 13815  |\n",
      "|  HNL    | LIH  | 13156  |\n",
      "|  LIH    | HNL  | 13030  |\n",
      "|  SAN    | LAX  | 12779  |\n",
      "|  LAX    | SAN  | 12767  |\n",
      "|  BOS    | LGA  | 12263  |\n",
      "|  LAS    | PHX  | 12228  |\n",
      "|---------+------+--------|\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv --rs lf count-distinct -f Origin,Dest then sort -nr count then head -n 10 flights.csv | csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "!cat flights.csv | cut -d, -f17 | sed 1d | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `mlr merge-fields`\n",
    "\n",
    "- like mlr stats1 but all accumulation is done across fields within each given record: horizontal rather than vertical statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------+------------+---------+------------+-------------------+----------------+----------|\n",
      "|  DepTime | CRSDepTime | ArrTime | CRSArrTime | ActualElapsedTime | CRSElapsedTime | AirTime  |\n",
      "|----------+------------+---------+------------+-------------------+----------------+----------|\n",
      "|  1232    | 1225       | 1341    | 1340       | 69                | 75             | 54       |\n",
      "|  1918    | 1905       | 2043    | 2035       | 85                | 90             | 74       |\n",
      "|  2206    | 2130       | 2334    | 2300       | 88                | 90             | 73       |\n",
      "|  1230    | 1200       | 1356    | 1330       | 86                | 90             | 75       |\n",
      "|  831     | 830        | 957     | 1000       | 86                | 90             | 74       |\n",
      "|  1430    | 1420       | 1553    | 1550       | 83                | 90             | 74       |\n",
      "|  1936    | 1840       | 2217    | 2130       | 101               | 110            | 89       |\n",
      "|  944     | 935        | 1223    | 1225       | 99                | 110            | 86       |\n",
      "|  1537    | 1450       | 1819    | 1735       | 102               | 105            | 90       |\n",
      "|  1318    | 1315       | 1603    | 1610       | 105               | 115            | 92       |\n",
      "|----------+------------+---------+------------+-------------------+----------------+----------|\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv --rs lf head -n 10 then cut -r -f \"Time\" flights.csv | csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------+------------+---------+------------+-------------------+----------------+---------+-----------|\n",
      "|  DepTime | CRSDepTime | ArrTime | CRSArrTime | ActualElapsedTime | CRSElapsedTime | AirTime | time_sum  |\n",
      "|----------+------------+---------+------------+-------------------+----------------+---------+-----------|\n",
      "|  1232    | 1225       | 1341    | 1340       | 69                | 75             | 54      | 5336      |\n",
      "|  1918    | 1905       | 2043    | 2035       | 85                | 90             | 74      | 8150      |\n",
      "|  2206    | 2130       | 2334    | 2300       | 88                | 90             | 73      | 9221      |\n",
      "|  1230    | 1200       | 1356    | 1330       | 86                | 90             | 75      | 5367      |\n",
      "|  831     | 830        | 957     | 1000       | 86                | 90             | 74      | 3868      |\n",
      "|  1430    | 1420       | 1553    | 1550       | 83                | 90             | 74      | 6200      |\n",
      "|  1936    | 1840       | 2217    | 2130       | 101               | 110            | 89      | 8423      |\n",
      "|  944     | 935        | 1223    | 1225       | 99                | 110            | 86      | 4622      |\n",
      "|  1537    | 1450       | 1819    | 1735       | 102               | 105            | 90      | 6838      |\n",
      "|  1318    | 1315       | 1603    | 1610       | 105               | 115            | 92      | 6158      |\n",
      "|----------+------------+---------+------------+-------------------+----------------+---------+-----------|\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv --rs lf head -n 10 then \\\n",
    "cut -r -f \"Time\" then \\\n",
    "merge-fields -a sum -r 'Time' -k -o time \\\n",
    "flights.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `stats1`\n",
    "\n",
    "- Computes univariate statistics for one or more given fields, accumulated across the input record stream.\n",
    "- switch `-f {a,b,c}`  Value-field names on which to compute statistics\n",
    "- switch `-g {d,e,f}`  Optional group-by-field names\n",
    "- switch `-a {g,h,i}` with the listed stats\n",
    "\n",
    "\n",
    "```\n",
    "  count     Count instances of fields\n",
    "  mode      Find most-frequently-occurring values for fields; first-found wins tie\n",
    "  antimode  Find least-frequently-occurring values for fields; first-found wins tie\n",
    "  sum       Compute sums of specified fields\n",
    "  mean      Compute averages (sample means) of specified fields\n",
    "  stddev    Compute sample standard deviation of specified fields\n",
    "  var       Compute sample variance of specified fields\n",
    "  meaneb    Estimate error bars for averages (assuming no sample autocorrelation)\n",
    "  skewness  Compute sample skewness of specified fields\n",
    "  kurtosis  Compute sample kurtosis of specified fields\n",
    "  min       Compute minimum values of specified fields\n",
    "  max       Compute maximum values of specified fields\n",
    "  \n",
    "Examples -\n",
    "\n",
    "mlr stats1 -a min,p10,p50,p90,max -f value -g size,shape\n",
    "mlr stats1 -a count,mode -f size\n",
    "mlr stats1 -a count,mode -f size -g shape\n",
    "  \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---------+---------------+------------------|\n",
      "|  Origin | ArrDelay_mean | ArrDelay_stddev  |\n",
      "|---------+---------------+------------------|\n",
      "|  ACK    | 46.115523     | 82.715645        |\n",
      "|  SOP    | 40.464865     | 61.941857        |\n",
      "|  PIR    | 35.750000     | 33.260337        |\n",
      "|  MKC    | 25.000000     |                  |\n",
      "|  CEC    | 24.763527     | 47.427527        |\n",
      "|  MCN    | 24.087607     | 79.076628        |\n",
      "|  SPI    | 21.591925     | 62.123661        |\n",
      "|  EWN    | 21.486532     | 53.500822        |\n",
      "|  HHH    | 20.648305     | 61.881898        |\n",
      "|  MEI    | 20.529141     | 52.402560        |\n",
      "|---------+---------------+------------------|\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv --rs lf filter '$ArrDelay!=\"NA\"' \\\n",
    "then stats1 -a mean,stddev -f ArrDelay -g Origin \\\n",
    "then sort -nr ArrDelay_mean \\\n",
    "then head -n 10 \\\n",
    "flights.csv | csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---------+--------------+--------------+--------------+--------------+--------------+---------------|\n",
      "|  Origin | ArrDelay_min | ArrDelay_p05 | ArrDelay_p10 | ArrDelay_p90 | ArrDelay_p95 | ArrDelay_max  |\n",
      "|---------+--------------+--------------+--------------+--------------+--------------+---------------|\n",
      "|  ABE    | -44          | -22          | -18          | 39           | 83           | 690           |\n",
      "|  ABI    | -32          | -14          | -12          | 39           | 78           | 695           |\n",
      "|  ABQ    | -61          | -19          | -15          | 29           | 54           | 1049          |\n",
      "|  ABY    | -27          | -15          | -12          | 45           | 79           | 336           |\n",
      "|  ACK    | -51          | -37          | -31          | 163          | 234          | 386           |\n",
      "|  ACT    | -32          | -17          | -14          | 33           | 67           | 366           |\n",
      "|  ACV    | -30          | -17          | -14          | 62           | 96           | 498           |\n",
      "|  ACY    | -55          | -30          | -24          | 60           | 110          | 741           |\n",
      "|  ADK    | -83          | -73          | -65          | 56           | 78           | 160           |\n",
      "|  ADQ    | -31          | -19          | -16          | 41           | 66           | 275           |\n",
      "|---------+--------------+--------------+--------------+--------------+--------------+---------------|\n"
     ]
    }
   ],
   "source": [
    "!mlr --csv --rs lf filter '$ArrDelay!=\"NA\"' \\\n",
    "then stats1 -a min,p05,p10,p90,p95,max -f ArrDelay -g Origin \\\n",
    "then sort -f Origin \\\n",
    "then head -n 10 \\\n",
    "flights.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `stats2`\n",
    "\n",
    "- for bivariate statistics\n",
    "\n",
    "```\n",
    "-a {linreg-ols,corr,...}  Names of accumulators: one or more of:\n",
    "  linreg-pca   Linear regression using principal component analysis\n",
    "  linreg-ols   Linear regression using ordinary least squares\n",
    "  r2           Quality metric for linreg-ols (linreg-pca emits its own)\n",
    "  logireg      Logistic regression\n",
    "  corr         Sample correlation\n",
    "  cov          Sample covariance\n",
    "  covx         Sample-covariance matrix\n",
    "-f {a,b,c,d}   Value-field name-pairs on which to compute statistics.\n",
    "               There must be an even number of names.\n",
    "-g {e,f,g}     Optional group-by-field names.\n",
    "-v             Print additional output for linreg-pca.\n",
    "-s             Print iterative stats. Useful in tail -f contexts (in which\n",
    "               case please avoid pprint-format output since end of input\n",
    "               stream will never be seen).\n",
    "--fit          Rather than printing regression parameters, applies them to\n",
    "               the input data to compute new fit fields. All input records are\n",
    "               held in memory until end of input stream. Has effect only for\n",
    "               linreg-ols, linreg-pca, and logireg.\n",
    "Only one of -s or --fit may be used.\n",
    "\n",
    "Examples\n",
    "mlr stats2 -a linreg-pca -f x,y\n",
    "mlr stats2 -a linreg-ols,r2 -f x,y -g size,shape\n",
    "mlr stats2 -a corr -f x,y\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mlr --csv --rs lf filter '$ArrDelay!=\"NA\"' \\\n",
    "then stats2 -a corr -f ArrDelay,AirTime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `steps`\n",
    "\n",
    "- Computes values dependent on the previous record, optionally grouped by category.\n",
    "- useful for calculating `lagged` variables\n",
    "\n",
    "> Most Miller commands are **record-at-a-time**, with the exception of `stats1, stats2`, and `histogram` which compute aggregate output. \n",
    "\n",
    "> The `step` command is intermediate: it allows the option of adding fields which are functions of fields from previous records. Rsum is short for running sum.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Options:\n",
    "-a {delta,rsum,...}   Names of steppers: comma-separated, one or more of:\n",
    "  delta    Compute differences in field(s) between successive records\n",
    "  shift    Include value(s) in field(s) from previous record, if any\n",
    "  from-first Compute differences in field(s) from first record\n",
    "  ratio    Compute ratios in field(s) between successive records\n",
    "  rsum     Compute running sums of field(s) between successive records\n",
    "  counter  Count instances of field(s) between successive records\n",
    "  ewma     Exponentially weighted moving average over successive records\n",
    "-f {a,b,c} Value-field names on which to compute statistics\n",
    "-g {d,e,f} Optional group-by-field names\n",
    "-F         Computes integerable things (e.g. counter) in floating point.\n",
    "-d {x,y,z} Weights for ewma. 1 means current sample gets all weight (no\n",
    "           smoothing), near under under 1 is light smoothing, near over 0 is\n",
    "           heavy smoothing. Multiple weights may be specified, e.g.\n",
    "           \"mlr step -a ewma -f sys_load -d 0.01,0.1,0.9\". Default if omitted\n",
    "           is \"-d 0.5\".\n",
    "-o {a,b,c} Custom suffixes for EWMA output fields. If omitted, these default to\n",
    "           the -d values. If supplied, the number of -o values must be the same\n",
    "           as the number of -d values.\n",
    "\n",
    "Examples:\n",
    "  mlr step -a rsum -f request_size\n",
    "  mlr step -a delta -f request_size -g hostname\n",
    "  mlr step -a ewma -d 0.1,0.9 -f x,y\n",
    "  mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y\n",
    "  mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y -g group_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Also see\n",
    "\n",
    "- `uniq` for uniques and frequency tables\n",
    "- `top` for nlargest\n",
    "- some [examples](http://johnkerl.org/miller/doc/data-examples.html)\n",
    "- [DSL for `put`, `filter`](http://johnkerl.org/miller/doc/reference-dsl.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
