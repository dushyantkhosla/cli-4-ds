{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/data/')\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an empty sqlite database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"sqlite.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a _cursor_ object to interact with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Loading Data into the Database\n",
    "\n",
    "- This step might take a while if your CSV file is larger than a few GBs, \n",
    "\n",
    "- But the benefits outweigh the wait time;\n",
    "    - you can use `pd.read_sql` tools to pull data from the database without worrying about memory constraints.\n",
    "    - you can use tools like `Metabase` or any SQL editor to write aggregations and reductions on big data locally.  \n",
    "    \n",
    "    \n",
    "- [Note] Avoid using `SELECT *` as it will load all data into memory. \n",
    "\n",
    "- Use `WHERE` statements and the `LIMIT` clause each time.\n",
    "\n",
    "### Load NYC Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"nyc-taxi/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(COL):\n",
    "    \"\"\"\n",
    "    If the passed COL is numeric,\n",
    "    downcast it to the lowest size.\n",
    "    Else,\n",
    "    Return as-is.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    COL: pandas.Series\n",
    "        The Series to shrink\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    if numeric, a compressed series\n",
    "    \"\"\"\n",
    "    if COL.dtype == np.int64:\n",
    "        return pd.to_numeric(COL, downcast='integer', errors='ignore')\n",
    "    elif COL.dtype == np.float64:\n",
    "        return pd.to_numeric(COL, downcast='float', errors='ignore')\n",
    "    else:\n",
    "        return COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading yellow_tripdata_2017-01.csv\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "for f in files:\n",
    "    \"\"\"\n",
    "    Read each csv in chunks\n",
    "    For each chunk\n",
    "        Compress\n",
    "        Load into DB\n",
    "    \"\"\"\n",
    "    print(\"Reading {}\".format(f))\n",
    "    \n",
    "    f_chunks = pd.read_csv(\n",
    "        \"nyc-taxi/\" + f, \n",
    "        chunksize=10**6, \n",
    "        error_bad_lines=False,\n",
    "        parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "    )\n",
    "\n",
    "    for chunk in f_chunks:\n",
    "        \"\"\"\n",
    "        Fill the table by reading a large text file in chunks.\n",
    "        Each chunk is just a pandas DataFrame\n",
    "        Filter/transform the data as needed here.\n",
    "        \"\"\"\n",
    "        (chunk\n",
    "         .apply(convert_types)\n",
    "         .to_sql(\n",
    "            name='nyc_taxi', \n",
    "            con=con, \n",
    "            if_exists='append',\n",
    "            index=False)\n",
    "        )\n",
    "    \n",
    "print(\"Loading into db finished in {} seconds.\".format(time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Check if loading went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT count(*) FROM nyc_taxi\", con=con)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
