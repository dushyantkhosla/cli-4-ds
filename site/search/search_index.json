{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Command Line Tools for Data Science \u00b6 Command Line Tools for Data Science Introduction Why Some Quotes Tools Docker Image Get the Data Part 2: SQL Analytics with Metabase Appendix tl;dr Further Reading Introduction \u00b6 Say a you have a .csv file with a few columns. You have to run basic descriptive statistics on these columns, and maybe a few group-by operations (okay, pivot-tables.) If the file is a few thousand rows (under 100MB in size), you will probably double-click on it straight away and run the analysis in Excel. Give yourself a pat on the back, you chose the right tool for the right job. Who's a good analyst? Yes, you are! But what if the file was 750MB? Assuming you have enough RAM (8GB or more), of course you'll use dplyr in R, or pandas in Python or (gasp) write a data step in SAS . Right? Excellent. But what if the file was 6GB? 15GB? If the word Hadoop is stuck in your throat, I implore you to swallow it. This repository focues on open-source command-line utilities that can do the same. Yes, there are Python libraries that allow you to work with larger-than-RAM files on a single machine ( Spark , Dask and perhaps some more), but we'll keep that for later. Why \u00b6 Because I've met too many 'data scientists' who have a complete lack of awareness of the limits of their own hardware. See Don't use Hadoop - your data isn't that big are forgetting Statistics! Sometimes you can fit a model on a (representative) sample of data, and you might not need distributed ML. See Don't use deep learning your data isn't that big Also see learning curves Because there is an entire ecosystem of wonderful open-source software for data analysis Because renting servers with more RAM or more cores is now easier and cheaper than ever. Because too many businesses do not have massive data and are spending money and resources trying to solve their problems with the wrong (and expensive) tools The closest analogy I can think of is someone trying to break a pebble with a sledgehammer. Of course, the pebble will break, but wouldn't you rather first try using the hammer hanging in your toolshed? But mostly, because I like to teach! \ud83d\ude07 Some Quotes \u00b6 In forecasting applications, we never observe the whole population. The problem is to forecast from a finite sample. Hence statistics such as means and standard deviations must be estimated with error. \"At Facebook, 90% of the jobs have input sizes under 100GB.\" \"For workloads that process multi-GB rather than multi-TB, a big memory server will provide better performance-per-dollar than a cluster.\" Tools \u00b6 GNU Coreutils everyday tools like grep , sed , cut , shuf and cat for working on text-files GNU awk , a programming language designed for text processing and typically used as a data extraction and reporting tool GNU Datamash , a command-line program which performs basic numeric, textual and statistical operations on input textual data files. xsv , a fast CSV toolkit written in Rust csvkit , a suite of command-line tools for converting to and working with CSV. Written in Python Miller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON. Written in C. csvtk A cross-platform, efficient, practical and pretty CSV/TSV toolkit in Golang. textql Execute SQL against structured text like CSV or TSV. Written in Golang. SQLlite-like datetime support! q allows direct execution of SQL-like queries on CSVs/TSVs (and any other tabular text files) Docker Image \u00b6 I've created a Docker image with all of these tools, and tutorials on how to use them. It also contains Miniconda3 A conda environment ds-py3 configured with the PyData stack ( pandas , scikit-learn ...) Build (or pull) the docker image # clone this repo, and cd docker/ docker build -t cli-4-ds . # or docker pull dushyantkhosla/cli-4-ds:latest Run a container with the image docker run -it --privileged \\ -v $(pwd):/home \\ -p 8888:8888 \\ -p 5000:5000 \\ -p 3128:3128 \\ dushyantkhosla/cli-4-ds:latest Learn how to use these tools using the notebooks in tutorials/ There is a dedicated notebook for each of the tools above Run the start.sh script to see helpful messages bash /root/start.sh Get the Data \u00b6 To generate data for these tutorials, cd into the data/ directory and Run the get-csvs.sh script to download flightDelays and KDDCup datasets PS: This will download ~1.5GB data cd data/ bash get-csvs.sh python make-data.py Run the make-data.py to create a synthetic dataset with 10 million rows Part 2: SQL Analytics with Metabase \u00b6 You might want to try out Metabase , which has a nice front-end for writing SQL docker pull metabase/metabase:v0.19.0 - I recommend this version against the latest because it works with SQLite - If you want to run other DBs like Postgresql, you can get the latest image instead Then, run a container docker run -d -v $(pwd):/tmp -p 3000:3000 metabase/metabase:v0.19.0 The -d switch is for running the container in detached mode Navigate to localhost:3000 , connect to a .db file or run another DB and connect to it Appendix \u00b6 There are no rules, of thumb; but we can try Data Size Remedy up to 1GB Pandas up to 10GB Get more RAM. Try Spark/Dask. up to 100GB Postgres 500GB+ Hadoop tl;dr \u00b6 As long as your data fits-on-disk (ie, a few hundred GBs or less,) For filter or transform jobs (like WHERE and CASE WHEN ) , use cli-tools or python scripts (line-by-line or stream processing) break files into chunks, use pandas (chunk processing) For reductions or groupby jobs (like AVERAGE and PIVOT ), think deeply about your data, draw representative samples you're a better statistician than a programmer afterall, aren't you? use bootstrap measures to quantify uncertainty For machine-learning jobs, Cluster your data, then pull samples from each group. (stratify) Fit your first model on a 10% sample. Build a Learning Curve. Use cross-validated measures to quantify uncertainty Further Reading \u00b6 In a later post, I'd like to talk about extracting more juice out of your hardware with parallel-processing For now, here's something to munch on","title":"Home"},{"location":"#command-line-tools-for-data-science","text":"Command Line Tools for Data Science Introduction Why Some Quotes Tools Docker Image Get the Data Part 2: SQL Analytics with Metabase Appendix tl;dr Further Reading","title":"Command Line Tools for Data Science"},{"location":"#introduction","text":"Say a you have a .csv file with a few columns. You have to run basic descriptive statistics on these columns, and maybe a few group-by operations (okay, pivot-tables.) If the file is a few thousand rows (under 100MB in size), you will probably double-click on it straight away and run the analysis in Excel. Give yourself a pat on the back, you chose the right tool for the right job. Who's a good analyst? Yes, you are! But what if the file was 750MB? Assuming you have enough RAM (8GB or more), of course you'll use dplyr in R, or pandas in Python or (gasp) write a data step in SAS . Right? Excellent. But what if the file was 6GB? 15GB? If the word Hadoop is stuck in your throat, I implore you to swallow it. This repository focues on open-source command-line utilities that can do the same. Yes, there are Python libraries that allow you to work with larger-than-RAM files on a single machine ( Spark , Dask and perhaps some more), but we'll keep that for later.","title":"Introduction"},{"location":"#why","text":"Because I've met too many 'data scientists' who have a complete lack of awareness of the limits of their own hardware. See Don't use Hadoop - your data isn't that big are forgetting Statistics! Sometimes you can fit a model on a (representative) sample of data, and you might not need distributed ML. See Don't use deep learning your data isn't that big Also see learning curves Because there is an entire ecosystem of wonderful open-source software for data analysis Because renting servers with more RAM or more cores is now easier and cheaper than ever. Because too many businesses do not have massive data and are spending money and resources trying to solve their problems with the wrong (and expensive) tools The closest analogy I can think of is someone trying to break a pebble with a sledgehammer. Of course, the pebble will break, but wouldn't you rather first try using the hammer hanging in your toolshed? But mostly, because I like to teach! \ud83d\ude07","title":"Why"},{"location":"#some-quotes","text":"In forecasting applications, we never observe the whole population. The problem is to forecast from a finite sample. Hence statistics such as means and standard deviations must be estimated with error. \"At Facebook, 90% of the jobs have input sizes under 100GB.\" \"For workloads that process multi-GB rather than multi-TB, a big memory server will provide better performance-per-dollar than a cluster.\"","title":"Some Quotes"},{"location":"#tools","text":"GNU Coreutils everyday tools like grep , sed , cut , shuf and cat for working on text-files GNU awk , a programming language designed for text processing and typically used as a data extraction and reporting tool GNU Datamash , a command-line program which performs basic numeric, textual and statistical operations on input textual data files. xsv , a fast CSV toolkit written in Rust csvkit , a suite of command-line tools for converting to and working with CSV. Written in Python Miller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON. Written in C. csvtk A cross-platform, efficient, practical and pretty CSV/TSV toolkit in Golang. textql Execute SQL against structured text like CSV or TSV. Written in Golang. SQLlite-like datetime support! q allows direct execution of SQL-like queries on CSVs/TSVs (and any other tabular text files)","title":"Tools"},{"location":"#docker-image","text":"I've created a Docker image with all of these tools, and tutorials on how to use them. It also contains Miniconda3 A conda environment ds-py3 configured with the PyData stack ( pandas , scikit-learn ...) Build (or pull) the docker image # clone this repo, and cd docker/ docker build -t cli-4-ds . # or docker pull dushyantkhosla/cli-4-ds:latest Run a container with the image docker run -it --privileged \\ -v $(pwd):/home \\ -p 8888:8888 \\ -p 5000:5000 \\ -p 3128:3128 \\ dushyantkhosla/cli-4-ds:latest Learn how to use these tools using the notebooks in tutorials/ There is a dedicated notebook for each of the tools above Run the start.sh script to see helpful messages bash /root/start.sh","title":"Docker Image"},{"location":"#get-the-data","text":"To generate data for these tutorials, cd into the data/ directory and Run the get-csvs.sh script to download flightDelays and KDDCup datasets PS: This will download ~1.5GB data cd data/ bash get-csvs.sh python make-data.py Run the make-data.py to create a synthetic dataset with 10 million rows","title":"Get the Data"},{"location":"#part-2-sql-analytics-with-metabase","text":"You might want to try out Metabase , which has a nice front-end for writing SQL docker pull metabase/metabase:v0.19.0 - I recommend this version against the latest because it works with SQLite - If you want to run other DBs like Postgresql, you can get the latest image instead Then, run a container docker run -d -v $(pwd):/tmp -p 3000:3000 metabase/metabase:v0.19.0 The -d switch is for running the container in detached mode Navigate to localhost:3000 , connect to a .db file or run another DB and connect to it","title":"Part 2: SQL Analytics with Metabase"},{"location":"#appendix","text":"There are no rules, of thumb; but we can try Data Size Remedy up to 1GB Pandas up to 10GB Get more RAM. Try Spark/Dask. up to 100GB Postgres 500GB+ Hadoop","title":"Appendix"},{"location":"#tldr","text":"As long as your data fits-on-disk (ie, a few hundred GBs or less,) For filter or transform jobs (like WHERE and CASE WHEN ) , use cli-tools or python scripts (line-by-line or stream processing) break files into chunks, use pandas (chunk processing) For reductions or groupby jobs (like AVERAGE and PIVOT ), think deeply about your data, draw representative samples you're a better statistician than a programmer afterall, aren't you? use bootstrap measures to quantify uncertainty For machine-learning jobs, Cluster your data, then pull samples from each group. (stratify) Fit your first model on a 10% sample. Build a Learning Curve. Use cross-validated measures to quantify uncertainty","title":"tl;dr"},{"location":"#further-reading","text":"In a later post, I'd like to talk about extracting more juice out of your hardware with parallel-processing For now, here's something to munch on","title":"Further Reading"},{"location":"00_dbs_catalogue-of-life/","text":"DBs 1: Catalogue of Life \u00b6 import os os . chdir ( \"/home/data/\" ) import subprocess as sbp import sqlite3 import pandas as pd import numpy as np run_on_bash = lambda i : sbp . check_output ( \" {} \" . format ( i ), shell = True ) . decode ( 'utf-8' ) Download and Unzip Data \u00b6 !wget 'http://www.catalogueoflife.org/DCA_Export/zip-fixed/2018-02-26-archive-complete.zip\"' !dtrx 2018 -02-26-archive-complete.zip Check files \u00b6 FILES = filter ( lambda i : 'txt' in i or 'csv' in i , os . listdir ( os . getcwd ())) dict_files = {} for file in FILES : rows_ = int ( run_on_bash ( \"wc -l {} \" . format ( file )) . split ( \" \" )[ 0 ]) size_ = os . path . getsize ( file ) / 10 ** 6 dict_files [ file ] = { 'rows' : rows_ , 'size' : size_ } print ( \" {:25} is {:10.2f} MB and has {:10.0f} rows\" . format ( file , size_ , rows_ )) flights.csv is 702.88 MB and has 7453216 rows get-csvs.sh is 0.00 MB and has 37 rows kdd.csv is 742.58 MB and has 4898432 rows zen.txt is 0.00 MB and has 21 rows Create DB, Connection, Cursor \u00b6 if os . path . exists ( \"life.db\" ): print ( \"Connecting to Existing DB\" ) conn = sqlite3 . connect ( \"life.db\" ) else : print ( \"Initialising new SQLite DB\" ) conn = sqlite3 . connect ( \"life.db\" ) curs = conn . cursor () Import files into DB with pandas \u00b6 for file in dict_files : \"\"\" For each file, check its size in MB If smaller than 250, read directly If larger, read in chunks Load the file into the database (.db file) \"\"\" if dict_files . get ( file ) . get ( 'size' ) < 250 : print ( \" {} is a small file. Importing directly.\" . format ( file )) df_ = pd . read_csv ( file , sep = \" \\t \" , low_memory = False , error_bad_lines = False ) df_ . to_sql ( name = file . replace ( \".txt\" , '' ), con = conn , index = False , if_exists = 'append' ) print ( \"Done.\" ) else : print ( \" {} is large. Importing in chunks.\" . format ( file )) size = int ( np . ceil ( dict_files . get ( file ) . get ( 'rows' ) / 10 )) chunks = pd . read_csv ( file , sep = \" \\t \" , chunksize = size , error_bad_lines = False , low_memory = False ) for c in chunks : c . to_sql ( name = file . replace ( \".txt\" , '' ), con = conn , index = False , if_exists = 'append' ) print ( \"Done\" ) description.txt is a small file. Importing directly. Done. distribution.txt is a small file. Importing directly. Done. reference.txt is large. Importing in chunks. Done speciesprofile.txt is a small file. Importing directly. Done. taxa.txt is large. Importing in chunks. Done vernacular.txt is a small file. Importing directly. Done. Check DB \u00b6 print ( \"The database is {:.2f} MB in size\" . format ( os . path . getsize ( 'life.db' ) / 10 ** 6 )) The database is 2236.95 MB in size curs . execute ( \"SELECT name FROM sqlite_master WHERE type='table'\" ) . fetchall () [('description',), ('distribution',), ('reference',), ('speciesprofile',), ('taxa',), ('vernacular',)] Run Queries \u00b6 pd . read_sql_query ( sql = \"\"\" SELECT genus, count(*) FROM taxa WHERE isExtinct = 0.0 AND genus IS NOT NULL GROUP BY 1 ORDER BY 2 DESC LIMIT 10 \"\"\" , con = conn ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } genus count(*) 0 Hieracium 6029 1 Astragalus 3375 2 Puccinia 3193 3 Carabus 3125 4 Cortinarius 3066 5 Agrilus 3034 6 Carex 2670 7 Tipula 2507 8 Taraxacum 2423 9 Euphorbia 2416 If you make changes to the DB, save them \u00b6 conn . commit () conn . close () Improve Speed With a New Index \u00b6 If you know you will be pulling records according to the value of a certain column(s) very frequently, make a new index for your database on that column. In the example below, we're setting the id column as the new and assigning the name id_idx to it. curs . execute ( \"CREATE INDEX id_idx ON data (id);\" ) conn . commit ()","title":"Example 1"},{"location":"00_dbs_catalogue-of-life/#dbs-1-catalogue-of-life","text":"import os os . chdir ( \"/home/data/\" ) import subprocess as sbp import sqlite3 import pandas as pd import numpy as np run_on_bash = lambda i : sbp . check_output ( \" {} \" . format ( i ), shell = True ) . decode ( 'utf-8' )","title":"DBs 1: Catalogue of Life"},{"location":"00_dbs_catalogue-of-life/#download-and-unzip-data","text":"!wget 'http://www.catalogueoflife.org/DCA_Export/zip-fixed/2018-02-26-archive-complete.zip\"' !dtrx 2018 -02-26-archive-complete.zip","title":"Download and Unzip Data"},{"location":"00_dbs_catalogue-of-life/#check-files","text":"FILES = filter ( lambda i : 'txt' in i or 'csv' in i , os . listdir ( os . getcwd ())) dict_files = {} for file in FILES : rows_ = int ( run_on_bash ( \"wc -l {} \" . format ( file )) . split ( \" \" )[ 0 ]) size_ = os . path . getsize ( file ) / 10 ** 6 dict_files [ file ] = { 'rows' : rows_ , 'size' : size_ } print ( \" {:25} is {:10.2f} MB and has {:10.0f} rows\" . format ( file , size_ , rows_ )) flights.csv is 702.88 MB and has 7453216 rows get-csvs.sh is 0.00 MB and has 37 rows kdd.csv is 742.58 MB and has 4898432 rows zen.txt is 0.00 MB and has 21 rows","title":"Check files"},{"location":"00_dbs_catalogue-of-life/#create-db-connection-cursor","text":"if os . path . exists ( \"life.db\" ): print ( \"Connecting to Existing DB\" ) conn = sqlite3 . connect ( \"life.db\" ) else : print ( \"Initialising new SQLite DB\" ) conn = sqlite3 . connect ( \"life.db\" ) curs = conn . cursor ()","title":"Create DB, Connection, Cursor"},{"location":"00_dbs_catalogue-of-life/#import-files-into-db-with-pandas","text":"for file in dict_files : \"\"\" For each file, check its size in MB If smaller than 250, read directly If larger, read in chunks Load the file into the database (.db file) \"\"\" if dict_files . get ( file ) . get ( 'size' ) < 250 : print ( \" {} is a small file. Importing directly.\" . format ( file )) df_ = pd . read_csv ( file , sep = \" \\t \" , low_memory = False , error_bad_lines = False ) df_ . to_sql ( name = file . replace ( \".txt\" , '' ), con = conn , index = False , if_exists = 'append' ) print ( \"Done.\" ) else : print ( \" {} is large. Importing in chunks.\" . format ( file )) size = int ( np . ceil ( dict_files . get ( file ) . get ( 'rows' ) / 10 )) chunks = pd . read_csv ( file , sep = \" \\t \" , chunksize = size , error_bad_lines = False , low_memory = False ) for c in chunks : c . to_sql ( name = file . replace ( \".txt\" , '' ), con = conn , index = False , if_exists = 'append' ) print ( \"Done\" ) description.txt is a small file. Importing directly. Done. distribution.txt is a small file. Importing directly. Done. reference.txt is large. Importing in chunks. Done speciesprofile.txt is a small file. Importing directly. Done. taxa.txt is large. Importing in chunks. Done vernacular.txt is a small file. Importing directly. Done.","title":"Import files into DB with pandas"},{"location":"00_dbs_catalogue-of-life/#check-db","text":"print ( \"The database is {:.2f} MB in size\" . format ( os . path . getsize ( 'life.db' ) / 10 ** 6 )) The database is 2236.95 MB in size curs . execute ( \"SELECT name FROM sqlite_master WHERE type='table'\" ) . fetchall () [('description',), ('distribution',), ('reference',), ('speciesprofile',), ('taxa',), ('vernacular',)]","title":"Check DB"},{"location":"00_dbs_catalogue-of-life/#run-queries","text":"pd . read_sql_query ( sql = \"\"\" SELECT genus, count(*) FROM taxa WHERE isExtinct = 0.0 AND genus IS NOT NULL GROUP BY 1 ORDER BY 2 DESC LIMIT 10 \"\"\" , con = conn ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } genus count(*) 0 Hieracium 6029 1 Astragalus 3375 2 Puccinia 3193 3 Carabus 3125 4 Cortinarius 3066 5 Agrilus 3034 6 Carex 2670 7 Tipula 2507 8 Taraxacum 2423 9 Euphorbia 2416","title":"Run Queries"},{"location":"00_dbs_catalogue-of-life/#if-you-make-changes-to-the-db-save-them","text":"conn . commit () conn . close ()","title":"If you make changes to the DB, save them"},{"location":"00_dbs_catalogue-of-life/#improve-speed-with-a-new-index","text":"If you know you will be pulling records according to the value of a certain column(s) very frequently, make a new index for your database on that column. In the example below, we're setting the id column as the new and assigning the name id_idx to it. curs . execute ( \"CREATE INDEX id_idx ON data (id);\" ) conn . commit ()","title":"Improve Speed With a New Index"},{"location":"00_dbs_flights_kdd/","text":"DBs 2: Flights Data \u00b6 import os os . chdir ( '/home/data/' ) from time import time import sqlite3 import pandas as pd import numpy as np from sqlalchemy import create_engine Create an empty sqlite database file \u00b6 con = sqlite3 . connect ( \"sqlite.db\" ) alternatively, use con = create_engine ( 'sqlite:///tutorial.db' ) Create a cursor object to interact with it \u00b6 cur = con . cursor () def convert_types ( COL ): \"\"\" If the passed COL is numeric, downcast it to the lowest size. Else, Return as-is. Parameters ----------- COL: pandas.Series The Series to shrink Returns ------- if numeric, a compressed series \"\"\" if COL . dtype == np . int64 : return pd . to_numeric ( COL , downcast = 'integer' , errors = 'ignore' ) elif COL . dtype == np . float64 : return pd . to_numeric ( COL , downcast = 'float' , errors = 'ignore' ) else : return COL Loading Data into the Database \u00b6 This step might take a while if your CSV file is larger than a few GBs, But the benefits outweigh the wait time; you can use pd.read_sql tools to pull data from the database without worrying about memory constraints. you can use tools like Metabase or any SQL editor to write aggregations and reductions on big data locally. [Note] Avoid using SELECT * as it will load all data into memory. Use WHERE statements and the LIMIT clause each time. Load KDD Data \u00b6 kdd_chunks = pd . read_csv ( \"kdd_flights/kdd.csv\" , chunksize = 10 ** 6 , error_bad_lines = False ) t0 = time () for chunk in kdd_chunks : \"\"\" Fill the table by reading a large text file in chunks. Each chunk is just a pandas DataFrame Filter/transform the data as needed here. \"\"\" ( chunk . apply ( convert_types ) . to_sql ( name = 'kdd' , con = con , if_exists = 'append' , index = False )) print ( \"Loading into db finished in {} seconds.\" . format ( time () - t0 )) Loading into db finished in 35.13177156448364 seconds. Load Flights Data \u00b6 flights_chunks = pd . read_csv ( 'kdd_flights/flights.csv' , chunksize = 10 ** 6 , error_bad_lines = False ) t0 = time () for chunk in flights_chunks : \"\"\" Fill the table by reading a large text file in chunks. Each chunk is just a pandas DataFrame Filter/transform the data as needed here. \"\"\" ( chunk . apply ( convert_types ) . to_sql ( name = 'flightDelays' , con = con , if_exists = 'append' , index = False )) print ( \"Loading into db finished in {} seconds.\" . format ( time () - t0 )) Loading into db finished in 80.80561757087708 seconds. Check if loading went well \u00b6 cur . execute ( \"SELECT name FROM sqlite_master WHERE type='table'\" ) . fetchall () [('nyc_taxi',), ('kdd',), ('flightDelays',)] Run SQL queries \u00b6 pd . read_sql ( \"\"\" SELECT Origin,Dest,count(*) as Flights FROM flightDelays GROUP BY 1,2 ORDER BY 3 DESC LIMIT 10 \"\"\" , con = con ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Origin Dest Flights 0 OGG HNL 16099 1 HNL OGG 15876 2 LAX LAS 14385 3 LAS LAX 13815 4 HNL LIH 13156 5 LIH HNL 13030 6 SAN LAX 12779 7 LAX SAN 12767 8 BOS LGA 12263 9 LAS PHX 12228 pd . read_sql ( \"SELECT count(*) FROM kdd\" , con = con ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count(*) 0 4898431","title":"Example 2"},{"location":"00_dbs_flights_kdd/#dbs-2-flights-data","text":"import os os . chdir ( '/home/data/' ) from time import time import sqlite3 import pandas as pd import numpy as np from sqlalchemy import create_engine","title":"DBs 2: Flights Data"},{"location":"00_dbs_flights_kdd/#create-an-empty-sqlite-database-file","text":"con = sqlite3 . connect ( \"sqlite.db\" ) alternatively, use con = create_engine ( 'sqlite:///tutorial.db' )","title":"Create an empty sqlite database file"},{"location":"00_dbs_flights_kdd/#create-a-cursor-object-to-interact-with-it","text":"cur = con . cursor () def convert_types ( COL ): \"\"\" If the passed COL is numeric, downcast it to the lowest size. Else, Return as-is. Parameters ----------- COL: pandas.Series The Series to shrink Returns ------- if numeric, a compressed series \"\"\" if COL . dtype == np . int64 : return pd . to_numeric ( COL , downcast = 'integer' , errors = 'ignore' ) elif COL . dtype == np . float64 : return pd . to_numeric ( COL , downcast = 'float' , errors = 'ignore' ) else : return COL","title":"Create a cursor object to interact with it"},{"location":"00_dbs_flights_kdd/#loading-data-into-the-database","text":"This step might take a while if your CSV file is larger than a few GBs, But the benefits outweigh the wait time; you can use pd.read_sql tools to pull data from the database without worrying about memory constraints. you can use tools like Metabase or any SQL editor to write aggregations and reductions on big data locally. [Note] Avoid using SELECT * as it will load all data into memory. Use WHERE statements and the LIMIT clause each time.","title":"Loading Data into the Database"},{"location":"00_dbs_flights_kdd/#load-kdd-data","text":"kdd_chunks = pd . read_csv ( \"kdd_flights/kdd.csv\" , chunksize = 10 ** 6 , error_bad_lines = False ) t0 = time () for chunk in kdd_chunks : \"\"\" Fill the table by reading a large text file in chunks. Each chunk is just a pandas DataFrame Filter/transform the data as needed here. \"\"\" ( chunk . apply ( convert_types ) . to_sql ( name = 'kdd' , con = con , if_exists = 'append' , index = False )) print ( \"Loading into db finished in {} seconds.\" . format ( time () - t0 )) Loading into db finished in 35.13177156448364 seconds.","title":"Load KDD Data"},{"location":"00_dbs_flights_kdd/#load-flights-data","text":"flights_chunks = pd . read_csv ( 'kdd_flights/flights.csv' , chunksize = 10 ** 6 , error_bad_lines = False ) t0 = time () for chunk in flights_chunks : \"\"\" Fill the table by reading a large text file in chunks. Each chunk is just a pandas DataFrame Filter/transform the data as needed here. \"\"\" ( chunk . apply ( convert_types ) . to_sql ( name = 'flightDelays' , con = con , if_exists = 'append' , index = False )) print ( \"Loading into db finished in {} seconds.\" . format ( time () - t0 )) Loading into db finished in 80.80561757087708 seconds.","title":"Load Flights Data"},{"location":"00_dbs_flights_kdd/#check-if-loading-went-well","text":"cur . execute ( \"SELECT name FROM sqlite_master WHERE type='table'\" ) . fetchall () [('nyc_taxi',), ('kdd',), ('flightDelays',)]","title":"Check if loading went well"},{"location":"00_dbs_flights_kdd/#run-sql-queries","text":"pd . read_sql ( \"\"\" SELECT Origin,Dest,count(*) as Flights FROM flightDelays GROUP BY 1,2 ORDER BY 3 DESC LIMIT 10 \"\"\" , con = con ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Origin Dest Flights 0 OGG HNL 16099 1 HNL OGG 15876 2 LAX LAS 14385 3 LAS LAX 13815 4 HNL LIH 13156 5 LIH HNL 13030 6 SAN LAX 12779 7 LAX SAN 12767 8 BOS LGA 12263 9 LAS PHX 12228 pd . read_sql ( \"SELECT count(*) FROM kdd\" , con = con ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count(*) 0 4898431","title":"Run SQL queries"},{"location":"00_dbs_sqllite_nyc_taxi/","text":"DBs 3: NYC Taxis Data \u00b6 import os os . chdir ( '/home/data/' ) from time import time import numpy as np import sqlite3 import pandas as pd from sqlalchemy import create_engine Create an empty sqlite database file \u00b6 con = sqlite3 . connect ( \"sqlite.db\" ) Create a cursor object to interact with it \u00b6 cur = con . cursor () Loading Data into the Database \u00b6 This step might take a while if your CSV file is larger than a few GBs, But the benefits outweigh the wait time; you can use pd.read_sql tools to pull data from the database without worrying about memory constraints. you can use tools like Metabase or any SQL editor to write aggregations and reductions on big data locally. [Note] Avoid using SELECT * as it will load all data into memory. Use WHERE statements and the LIMIT clause each time. Load NYC Taxi Data \u00b6 files = os . listdir ( \"nyc-taxi/\" ) def convert_types ( COL ): \"\"\" If the passed COL is numeric, downcast it to the lowest size. Else, Return as-is. Parameters ----------- COL: pandas.Series The Series to shrink Returns ------- if numeric, a compressed series \"\"\" if COL . dtype == np . int64 : return pd . to_numeric ( COL , downcast = 'integer' , errors = 'ignore' ) elif COL . dtype == np . float64 : return pd . to_numeric ( COL , downcast = 'float' , errors = 'ignore' ) else : return COL for f in files : \"\"\" Read each csv in chunks For each chunk Compress Load into DB \"\"\" t0 = time () print ( \"Reading {} \" . format ( f )) f_chunks = pd . read_csv ( \"nyc-taxi/\" + f , chunksize = 10 ** 6 , error_bad_lines = False , parse_dates = [ 'tpep_pickup_datetime' , 'tpep_dropoff_datetime' ] ) for chunk in f_chunks : \"\"\" Fill the table by reading a large text file in chunks. Each chunk is just a pandas DataFrame Filter/transform the data as needed here. \"\"\" ( chunk . apply ( convert_types ) . to_sql ( name = 'nyc_taxi' , con = con , if_exists = 'append' , index = False ) ) print ( \"Loading into db finished in {} seconds.\" . format ( time () - t0 )) Reading yellow_tripdata_2017-11.csv Loading into db finished in 169.0669174194336 seconds. Reading yellow_tripdata_2017-08.csv Loading into db finished in 154.18128442764282 seconds. Reading yellow_tripdata_2017-01.csv Loading into db finished in 176.58778619766235 seconds. Reading yellow_tripdata_2017-10.csv Loading into db finished in 183.2636420726776 seconds. Reading yellow_tripdata_2017-03.csv Loading into db finished in 186.8081409931183 seconds. Reading yellow_tripdata_2017-02.csv Loading into db finished in 164.89169430732727 seconds. Reading yellow_tripdata_2017-09.csv Loading into db finished in 161.81028938293457 seconds. Reading yellow_tripdata_2017-06.csv Loading into db finished in 175.87790322303772 seconds. Reading yellow_tripdata_2017-04.csv Loading into db finished in 181.84538388252258 seconds. Reading yellow_tripdata_2017-12.csv Loading into db finished in 174.8419873714447 seconds. Reading yellow_tripdata_2017-05.csv Loading into db finished in 183.7002031803131 seconds. Reading yellow_tripdata_2017-07.csv Loading into db finished in 156.459303855896 seconds. Check if loading went well \u00b6 cur . execute ( \"SELECT name FROM sqlite_master WHERE type='table'\" ) . fetchall () [('nyc_taxi',), ('kdd',)] Run SQL queries \u00b6 pd . read_sql ( \"SELECT count(*) FROM nyc_taxi\" , con = con ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count(*) 0 113496874 df_ = pd . read_sql ( \"SELECT * FROM nyc_taxi LIMIT 10\" , con = con ) df_ . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Data columns (total 17 columns): VendorID 10 non-null int64 tpep_pickup_datetime 10 non-null object tpep_dropoff_datetime 10 non-null object passenger_count 10 non-null int64 trip_distance 10 non-null float64 RatecodeID 10 non-null int64 store_and_fwd_flag 10 non-null object PULocationID 10 non-null int64 DOLocationID 10 non-null int64 payment_type 10 non-null int64 fare_amount 10 non-null float64 extra 10 non-null float64 mta_tax 10 non-null float64 tip_amount 10 non-null float64 tolls_amount 10 non-null float64 improvement_surcharge 10 non-null float64 total_amount 10 non-null float64 dtypes: float64(8), int64(6), object(3) memory usage: 1.4+ KB","title":"Example 3"},{"location":"00_dbs_sqllite_nyc_taxi/#dbs-3-nyc-taxis-data","text":"import os os . chdir ( '/home/data/' ) from time import time import numpy as np import sqlite3 import pandas as pd from sqlalchemy import create_engine","title":"DBs 3: NYC Taxis Data"},{"location":"00_dbs_sqllite_nyc_taxi/#create-an-empty-sqlite-database-file","text":"con = sqlite3 . connect ( \"sqlite.db\" )","title":"Create an empty sqlite database file"},{"location":"00_dbs_sqllite_nyc_taxi/#create-a-cursor-object-to-interact-with-it","text":"cur = con . cursor ()","title":"Create a cursor object to interact with it"},{"location":"00_dbs_sqllite_nyc_taxi/#loading-data-into-the-database","text":"This step might take a while if your CSV file is larger than a few GBs, But the benefits outweigh the wait time; you can use pd.read_sql tools to pull data from the database without worrying about memory constraints. you can use tools like Metabase or any SQL editor to write aggregations and reductions on big data locally. [Note] Avoid using SELECT * as it will load all data into memory. Use WHERE statements and the LIMIT clause each time.","title":"Loading Data into the Database"},{"location":"00_dbs_sqllite_nyc_taxi/#load-nyc-taxi-data","text":"files = os . listdir ( \"nyc-taxi/\" ) def convert_types ( COL ): \"\"\" If the passed COL is numeric, downcast it to the lowest size. Else, Return as-is. Parameters ----------- COL: pandas.Series The Series to shrink Returns ------- if numeric, a compressed series \"\"\" if COL . dtype == np . int64 : return pd . to_numeric ( COL , downcast = 'integer' , errors = 'ignore' ) elif COL . dtype == np . float64 : return pd . to_numeric ( COL , downcast = 'float' , errors = 'ignore' ) else : return COL for f in files : \"\"\" Read each csv in chunks For each chunk Compress Load into DB \"\"\" t0 = time () print ( \"Reading {} \" . format ( f )) f_chunks = pd . read_csv ( \"nyc-taxi/\" + f , chunksize = 10 ** 6 , error_bad_lines = False , parse_dates = [ 'tpep_pickup_datetime' , 'tpep_dropoff_datetime' ] ) for chunk in f_chunks : \"\"\" Fill the table by reading a large text file in chunks. Each chunk is just a pandas DataFrame Filter/transform the data as needed here. \"\"\" ( chunk . apply ( convert_types ) . to_sql ( name = 'nyc_taxi' , con = con , if_exists = 'append' , index = False ) ) print ( \"Loading into db finished in {} seconds.\" . format ( time () - t0 )) Reading yellow_tripdata_2017-11.csv Loading into db finished in 169.0669174194336 seconds. Reading yellow_tripdata_2017-08.csv Loading into db finished in 154.18128442764282 seconds. Reading yellow_tripdata_2017-01.csv Loading into db finished in 176.58778619766235 seconds. Reading yellow_tripdata_2017-10.csv Loading into db finished in 183.2636420726776 seconds. Reading yellow_tripdata_2017-03.csv Loading into db finished in 186.8081409931183 seconds. Reading yellow_tripdata_2017-02.csv Loading into db finished in 164.89169430732727 seconds. Reading yellow_tripdata_2017-09.csv Loading into db finished in 161.81028938293457 seconds. Reading yellow_tripdata_2017-06.csv Loading into db finished in 175.87790322303772 seconds. Reading yellow_tripdata_2017-04.csv Loading into db finished in 181.84538388252258 seconds. Reading yellow_tripdata_2017-12.csv Loading into db finished in 174.8419873714447 seconds. Reading yellow_tripdata_2017-05.csv Loading into db finished in 183.7002031803131 seconds. Reading yellow_tripdata_2017-07.csv Loading into db finished in 156.459303855896 seconds.","title":"Load NYC Taxi Data"},{"location":"00_dbs_sqllite_nyc_taxi/#check-if-loading-went-well","text":"cur . execute ( \"SELECT name FROM sqlite_master WHERE type='table'\" ) . fetchall () [('nyc_taxi',), ('kdd',)]","title":"Check if loading went well"},{"location":"00_dbs_sqllite_nyc_taxi/#run-sql-queries","text":"pd . read_sql ( \"SELECT count(*) FROM nyc_taxi\" , con = con ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count(*) 0 113496874 df_ = pd . read_sql ( \"SELECT * FROM nyc_taxi LIMIT 10\" , con = con ) df_ . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 10 entries, 0 to 9 Data columns (total 17 columns): VendorID 10 non-null int64 tpep_pickup_datetime 10 non-null object tpep_dropoff_datetime 10 non-null object passenger_count 10 non-null int64 trip_distance 10 non-null float64 RatecodeID 10 non-null int64 store_and_fwd_flag 10 non-null object PULocationID 10 non-null int64 DOLocationID 10 non-null int64 payment_type 10 non-null int64 fare_amount 10 non-null float64 extra 10 non-null float64 mta_tax 10 non-null float64 tip_amount 10 non-null float64 tolls_amount 10 non-null float64 improvement_surcharge 10 non-null float64 total_amount 10 non-null float64 dtypes: float64(8), int64(6), object(3) memory usage: 1.4+ KB","title":"Run SQL queries"},{"location":"01-Bash-Programming/","text":"Bash Programming \u00b6 Bash Programming Basics Variables Changing Variable Scope with export and source Special characters Loops with for and do ... done Conditionals Special Variables Functions Note Basics \u00b6 Learn Enough Command Line to Be Dangerous Command Line Crash Course Conquering the CLI Variables \u00b6 declared as SOME_VAR=value , where value may be numeric or string (enclosed within \"\" ) there must be no spaces around the = sign all values are stored as strings, but commands which expect a number can treat them as such. special characters must be properly escaped to avoid interpretation by the shell we can interactively set variable names using the read command #!/bin/sh echo What is your name? read MY_NAME echo \"Hello $MY_NAME ! Hope you're well.\" if you try to read an undeclared variable, the result is the empty string. PS : You get no warnings or errors To use values stored inside variables, use the dollar sign with curly braces: echo ${SOME_VAR} SOME_VAR = \"some value\" echo \"just ${ SOME_VAR } stored in a variable\" Changing Variable Scope with export and source \u00b6 When we run a script like bash hello.sh , a new shell is spawned to run the script Once the shell script exits, its environment is destroyed If you want a script to make changes to the global environment, you have to source it This will run the script within the active shell, instead of spawning a new one This is how .profile or .bash_profile files work export SOME_VAR=some_value will create a global Variable You may access it's value inside scripts Special characters \u00b6 Most characters like * and ' are not interpreted if placed inside double quotes (\"\") However, double-quotes \" , dollar-signs $ , back-ticks, and slashes \\ are still interpreted by the shell, even when they're in double quotes. These need to be escaped using the back-slash \\ Loops with for and do ... done \u00b6 Initialize a loop as for i in ... Use seq to generate sequences to iterate over for i in ` seq -s \" \" 1 10 ` do echo $i done Conditionals \u00b6 if [ condition ] ; then # action elif [ condition ] ; then # action else # action fi The condition must be surrounded by spaces Operators, For numeric data, use -eq , -lt , -gt , -le , -ge For strings, use = and != -n for non-zero length -f \"$SOME_PATH\" for checking if a file exists at given path -x \"$SOME_PATH\" for checking if a file is executable at given path \"$PATH_2\" -nt \"$PATH_1\" for checking if file at PATH_2 is newer than the one at PATH_1 #!/bin/bash echo \"enter name of file in pwd: \" read fn if [ -f \" $fn \" ] then echo \"Exists\" else echo \"Cannot find file\" fi For expressing complex logic, use AND, && along with OR, || Special Variables \u00b6 contain useful information, which can be used by the script to know about the environment in which it is running. $0 is the basename of the program (or the name of the script.) $1 .. $9 are the first 9 parameters the script was called with $@ is all parameters passed $$ variable is the PID (Process IDentifier) of the currently running shell. $! variable is the PID of the last run background process. Functions \u00b6 For simple scripts, a function may be simply declared within the same file as it is called. When writing a complex program, it is useful to write a \"library\" of useful functions, and source that file at the start of the other scripts which use the functions. A function may return a value in one of four different ways: Change the state of a variable or variables Use the exit command to end the shell script Use the return command to end the function, and return the supplied value to the calling section of the shell script echo output to stdout, which will be caught by the caller just as c= expr $a + $b is caught Syntax #!/bin/bash # function declarations some_func () { # commands } # function usage some_func arg_1 arg_2 arg_3 ... - example Note \u00b6 > /dev/null 2>&1 redirects any output or errors to the special \"null\" device instead of going to the user's screen. The backtick is used to indicate that the enclosed text is to be executed as a command. For example, $ MYNAME = ` grep \"^ ${ USER } :\" /etc/passwd | cut -d: -f5 ` $ echo $MYNAME","title":"Bash Programming"},{"location":"01-Bash-Programming/#bash-programming","text":"Bash Programming Basics Variables Changing Variable Scope with export and source Special characters Loops with for and do ... done Conditionals Special Variables Functions Note","title":"Bash Programming"},{"location":"01-Bash-Programming/#basics","text":"Learn Enough Command Line to Be Dangerous Command Line Crash Course Conquering the CLI","title":"Basics"},{"location":"01-Bash-Programming/#variables","text":"declared as SOME_VAR=value , where value may be numeric or string (enclosed within \"\" ) there must be no spaces around the = sign all values are stored as strings, but commands which expect a number can treat them as such. special characters must be properly escaped to avoid interpretation by the shell we can interactively set variable names using the read command #!/bin/sh echo What is your name? read MY_NAME echo \"Hello $MY_NAME ! Hope you're well.\" if you try to read an undeclared variable, the result is the empty string. PS : You get no warnings or errors To use values stored inside variables, use the dollar sign with curly braces: echo ${SOME_VAR} SOME_VAR = \"some value\" echo \"just ${ SOME_VAR } stored in a variable\"","title":"Variables"},{"location":"01-Bash-Programming/#changing-variable-scope-with-export-and-source","text":"When we run a script like bash hello.sh , a new shell is spawned to run the script Once the shell script exits, its environment is destroyed If you want a script to make changes to the global environment, you have to source it This will run the script within the active shell, instead of spawning a new one This is how .profile or .bash_profile files work export SOME_VAR=some_value will create a global Variable You may access it's value inside scripts","title":"Changing Variable Scope with export and source"},{"location":"01-Bash-Programming/#special-characters","text":"Most characters like * and ' are not interpreted if placed inside double quotes (\"\") However, double-quotes \" , dollar-signs $ , back-ticks, and slashes \\ are still interpreted by the shell, even when they're in double quotes. These need to be escaped using the back-slash \\","title":"Special characters"},{"location":"01-Bash-Programming/#loops-with-for-and-do-done","text":"Initialize a loop as for i in ... Use seq to generate sequences to iterate over for i in ` seq -s \" \" 1 10 ` do echo $i done","title":"Loops with for and do ... done"},{"location":"01-Bash-Programming/#conditionals","text":"if [ condition ] ; then # action elif [ condition ] ; then # action else # action fi The condition must be surrounded by spaces Operators, For numeric data, use -eq , -lt , -gt , -le , -ge For strings, use = and != -n for non-zero length -f \"$SOME_PATH\" for checking if a file exists at given path -x \"$SOME_PATH\" for checking if a file is executable at given path \"$PATH_2\" -nt \"$PATH_1\" for checking if file at PATH_2 is newer than the one at PATH_1 #!/bin/bash echo \"enter name of file in pwd: \" read fn if [ -f \" $fn \" ] then echo \"Exists\" else echo \"Cannot find file\" fi For expressing complex logic, use AND, && along with OR, ||","title":"Conditionals"},{"location":"01-Bash-Programming/#special-variables","text":"contain useful information, which can be used by the script to know about the environment in which it is running. $0 is the basename of the program (or the name of the script.) $1 .. $9 are the first 9 parameters the script was called with $@ is all parameters passed $$ variable is the PID (Process IDentifier) of the currently running shell. $! variable is the PID of the last run background process.","title":"Special Variables"},{"location":"01-Bash-Programming/#functions","text":"For simple scripts, a function may be simply declared within the same file as it is called. When writing a complex program, it is useful to write a \"library\" of useful functions, and source that file at the start of the other scripts which use the functions. A function may return a value in one of four different ways: Change the state of a variable or variables Use the exit command to end the shell script Use the return command to end the function, and return the supplied value to the calling section of the shell script echo output to stdout, which will be caught by the caller just as c= expr $a + $b is caught Syntax #!/bin/bash # function declarations some_func () { # commands } # function usage some_func arg_1 arg_2 arg_3 ... - example","title":"Functions"},{"location":"01-Bash-Programming/#note","text":"> /dev/null 2>&1 redirects any output or errors to the special \"null\" device instead of going to the user's screen. The backtick is used to indicate that the enclosed text is to be executed as a command. For example, $ MYNAME = ` grep \"^ ${ USER } :\" /etc/passwd | cut -d: -f5 ` $ echo $MYNAME","title":"Note"},{"location":"01_UnixCoreutils/","text":"GNU Coreutils \u00b6 These constitute one of most useful of all GNU Packages that are a intergral part of the daily work of millions of programmers/scientists. (Other well known packages include grep , screen , gzip , tar , time etc. Oh, and the R programming language.) \" The GNU Core Utilities are the basic file, shell and text manipulation utilities of the GNU operating system.These are the core utilities which are expected to exist on every OS . \" Examples include Manipulate Utils files chgrp, chown, chmod, cp, dd, df, dir, du, ln, ls, mkdir, mkfifo, mknod, mv, rm etc. text cat, cksum, head, tail, md5sum, nl, od, pr, tsort, join, wc, tac, paste etc. shell basename, chroot, date, dirname, echo, env, groups, hostname, nice, nohup, printf, sleep etc. Getting Help \u00b6 Documentation is built-in, so whenever you can't remember something, look it up using COMMAND --help # or man COMMAND Know your system \u00b6 # Kernel Info uname -a # all uname -r # exact # Linux Version lsb_release -a # all lsb_release -r # exact lsb_release -a -u # for derived distros like elementary # HDD Partition Info fdisk -l lsblk -o NAME,SIZE # human readable tree # LOCALE # List what locales currently defined for the current user locale # if needed, generate missing locale and reconfigure sudo locale-gen \"en_US.UTF-8\" sudo dpkg-reconfigure locales # ENVIRONMENT VARIABLES # list all active variables env # add a new variable to the list EXPORT ENV_VAR = val # PATH # list the programs on the path echo $PATH # add optional binary to the end of the path PATH = $PATH :~/opt/bin # add something ot the beginning PATH = ~/opt/bin: $PATH Shortcuts \u00b6 Ctrl + U Clears the line from the cursor point back to the beginning. Ctrl + A Moves the cursor to the beginning of the line. Ctrl + E Moves the cursor to the end of the line. Ctrl + R Allows you to search through the previous commands Why Coreutils? \u00b6 The basic tenet of UNIX philosophy is to \"create programs (or processes) that do one thing, and do that one thing well.\" It is a philosophy demanding careful thought about interfaces and ways of joining these smaller (hopefully more simple) processes together to create useful results . Normally text data flows between these interfaces. More advanced text processing tools and languages (like perl, python, and ruby) have been developed, which though very capable in their own right, are not always available, especially in a production environment. These coreutils therefore become an indespensable tool in the data scientist's toolbox if he wants to build systems that work beyond his laptop. Common Util Options \u00b6 can appear in any order (recommended: options before operands ) options can be long (begin with -- ) or abbreviated (begin wiht - ) available to all programs (eg. --help , --version ) Nearly every command invocation yields an integral exit status that can be used to change how other commands work. For the vast majority of commands, an exit status of zero indicates success, nonzero indicates failure . Alright, enough talk, let's dive in. Like most things on the internet these days, our story begins with a ... cat, tac \u00b6 Copies file/stdin to file/stdout (careful with large files!!) Useful in piping text to other programns, and to write output to files ! cat -- help Usage: cat [OPTION]... [FILE]... Concatenate FILE(s) to standard output. With no FILE, or when FILE is -, read standard input. -A, --show-all equivalent to -vET -b, --number-nonblank number nonempty output lines, overrides -n -e equivalent to -vE -E, --show-ends display $ at end of each line -n, --number number all output lines -s, --squeeze-blank suppress repeated empty output lines -t equivalent to -vT -T, --show-tabs display TAB characters as ^I -u (ignored) -v, --show-nonprinting use ^ and M- notation, except for LFD and TAB --help display this help and exit --version output version information and exit Examples: cat f - g Output f's contents, then standard input, then g's contents. cat Copy standard input to standard output. GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/cat> or available locally via: info '(coreutils) cat invocation' %% writefile f01 . txt 1 2 3 4 5 6 7 8 9 10 11 12 Writing f01.txt # create an index ! cat - n f01 . txt 1 1 2 3 2 4 5 6 3 4 7 8 9 5 6 7 10 11 12 # create index for nonmissing ! cat - b f01 . txt 1 1 2 3 2 4 5 6 3 7 8 9 \u200b \u200b 4 10 11 12 # squeeze multiple blank lines to 1 ! cat - bs f01 . txt 1 1 2 3 2 4 5 6 3 7 8 9 4 10 11 12 # reverse rows ! tac f01 . txt 10 11 12 7 8 9 4 5 6 1 2 3 cat \u00b6 ** To append Structured Files ** Concatenating files (same number of columns in the same order, no headers) cat f1.csv f2.csv f3.csv > f1_f2_f3.csv - Appending a file to an existing file using the >> operator, removing headers from subsequent files cat f1.csv > concatenated.csv cat f2.csv | sed \"1 d\" >> concatenated.csv cat f3.csv | sed \"1 d\" >> concatenated.csv Create a new file with the last few records of an existing file head -n 1 foo.csv > new_foo.csv tail -n 9 foo.csv >> new_foo.csv # row binding ! seq 10 21 | paste - d , - - > f01 . txt ! seq 80 89 | paste - d , - - > f02 . txt ` ! cat f01 . txt f02 . txt 10,11 12,13 14,15 16,17 18,19 20,21 80,81 82,83 84,85 86,87 88,89 nl \u00b6 add line numbers (generate and index like in pandas ) see options with nl --help -s to specify delimiter -b to use regexs # for a csv cat flights.csv | nl -s, | tail # for a pipe-delimited file cat sample_2.txt | nl -s '|' | tail head, tail \u00b6 display the first or last n rows ( n=10 by default) of a file (or many files) ! head - n 3 flights . csv Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,SMF,ONT,389,4,11,0,,0,0,0,0,0,0 2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,SMF,PDX,479,5,6,0,,0,0,0,0,0,0 ! tail - n 3 flights . csv 2007,12,15,6,1024,1025,1750,1735,DL,61,N623DL,266,250,233,15,-1,LAX,ATL,1946,14,19,0,,0,0,0,15,0,0 2007,12,15,6,1353,1315,1658,1622,DL,62,N970DL,125,127,100,36,38,DFW,ATL,732,11,14,0,,0,0,0,0,0,36 2007,12,15,6,1824,1800,2001,1928,DL,63,N628DL,97,88,61,33,24,ATL,MCO,403,10,26,0,,0,24,0,9,0,0 split \u00b6 creates output files containing consecutive or interleaved sections (1000 lines by default) of input files created are saved in the cd, named as PREFIXSUFFIX PREFIX is supplied with the call SUFFIX length is supplied (defaults to 2, so generates codes like xaa, xab, xac ...) ! split -- help Usage: split [OPTION]... [FILE [PREFIX]] Output pieces of FILE to PREFIXaa, PREFIXab, ...; default size is 1000 lines, and default PREFIX is 'x'. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. -a, --suffix-length=N generate suffixes of length N (default 2) --additional-suffix=SUFFIX append an additional SUFFIX to file names -b, --bytes=SIZE put SIZE bytes per output file -C, --line-bytes=SIZE put at most SIZE bytes of records per output file -d use numeric suffixes starting at 0, not alphabetic --numeric-suffixes[=FROM] same as -d, but allow setting the start value -e, --elide-empty-files do not generate empty output files with '-n' --filter=COMMAND write to shell COMMAND; file name is $FILE -l, --lines=NUMBER put NUMBER lines/records per output file -n, --number=CHUNKS generate CHUNKS output files; see explanation below -t, --separator=SEP use SEP instead of newline as the record separator; '\\0' (zero) specifies the NUL character -u, --unbuffered immediately copy input to output with '-n r/...' --verbose print a diagnostic just before each output file is opened --help display this help and exit --version output version information and exit The SIZE argument is an integer and optional unit (example: 10K is 10*1024). Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000). CHUNKS may be: N split into N files based on size of input K/N output Kth of N to stdout l/N split into N files without splitting lines/records l/K/N output Kth of N to stdout without splitting lines/records r/N like 'l' but use round robin distribution r/K/N likewise but only output Kth of N to stdout GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/split> or available locally via: info '(coreutils) split invocation' ! tail - n 1000 flights . csv > flights_1k . csv ! split - n 3 flights_1k . csv ls | grep 'xa' xaa xab xac # provide prefix ! split - n 3 flights_1k . csv flights_1k_split_ ls | grep 'split' flights_1k_split_aa flights_1k_split_ab flights_1k_split_ac # provide longer suffix if there are going to be many parts ! split - n 5 - a 4 flights_1k . csv Part_ ls | grep '^Part_' Part_aaaa Part_aaab Part_aaac Part_aaad Part_aaae # Cleaning up ! ls | grep '_split_' | xargs rm ! ls | grep 'Part_' | xargs rm ! ls | grep 'xa' | xargs rm csplit \u00b6 for copy-till-you-see-this kind of splits ! csplit -- help Usage: csplit [OPTION]... FILE PATTERN... Output pieces of FILE separated by PATTERN(s) to files 'xx00', 'xx01', ..., and output byte counts of each piece to standard output. Read standard input if FILE is - Mandatory arguments to long options are mandatory for short options too. -b, --suffix-format=FORMAT use sprintf FORMAT instead of %02d -f, --prefix=PREFIX use PREFIX instead of 'xx' -k, --keep-files do not remove output files on errors --suppress-matched suppress the lines matching PATTERN -n, --digits=DIGITS use specified number of digits instead of 2 -s, --quiet, --silent do not print counts of output file sizes -z, --elide-empty-files remove empty output files --help display this help and exit --version output version information and exit Each PATTERN may be: INTEGER copy up to but not including specified line number /REGEXP/[OFFSET] copy up to but not including a matching line %REGEXP%[OFFSET] skip to, but not including a matching line {INTEGER} repeat the previous pattern specified number of times {*} repeat the previous pattern as many times as possible A line OFFSET is a required '+' or '-' followed by a positive integer. GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/csplit> or available locally via: info '(coreutils) csplit invocation' wc \u00b6 counts the Number of bytes, characters, whitespace-separated words, and newlines ! wc -- help Usage: wc [OPTION]... [FILE]... or: wc [OPTION]... --files0-from=F Print newline, word, and byte counts for each FILE, and a total line if more than one FILE is specified. A word is a non-zero-length sequence of characters delimited by white space. With no FILE, or when FILE is -, read standard input. The options below may be used to select which counts are printed, always in the following order: newline, word, character, byte, maximum line length. -c, --bytes print the byte counts -m, --chars print the character counts -l, --lines print the newline counts --files0-from=F read input from the files specified by NUL-terminated names in file F; If F is - then read names from standard input -L, --max-line-length print the maximum display width -w, --words print the word counts --help display this help and exit --version output version information and exit GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/wc> or available locally via: info '(coreutils) wc invocation' # count and filename ! ls | grep csv | xargs wc - l 7453216 flights.csv 37 get-csvs.sh 4898432 kdd.csv 12351685 total run_on_bash ( 'ls | grep csv | xargs wc -l' ) 7453216 flights.csv 37 get-csvs.sh 4898432 kdd.csv 12351685 total cut \u00b6 select columns by position (comma separated list or hyphenated ranges) OPTIONS -d, --delimiter = DELIM use DELIM instead of TAB for field delimiter -f, --fields = LIST select only these fields -s, --only-delimited do not print lines not containing delimiters --output-delimiter = STRING use STRING as the output delimiter ! head - n 5 kdd . csv | cut - d , - f1 - 6 , 42 duration,protocol_type,service,flag,src_bytes,dst_bytes 0,tcp,http,SF,215,45076,normal. 0,tcp,http,SF,162,4528,normal. 0,tcp,http,SF,236,1228,normal. 0,tcp,http,SF,233,2032,normal. run_on_bash ( 'head -n 5 flights.csv | cut -d, -f9-10,15-18 | csvlook' ) | UniqueCarrier | FlightNum | ArrDelay | DepDelay | Origin | Dest | | ------------- | --------- | -------- | -------- | ------ | ---- | | WN | 2,891 | 1 | 7 | SMF | ONT | | WN | 462 | 8 | 13 | SMF | PDX | | WN | 1,229 | 34 | 36 | SMF | PDX | | WN | 1,355 | 26 | 30 | SMF | PDX | sort \u00b6 sort is quite versatile, and can be used to sort, sort & merge, randomize, and, deduplicate files. Useful sort options case insensitive with -f numbers with -n descending order with -r if lines have leading blanks -b sort by values in column 5 .... -k5 (use -t for delimiter here) sort in random order with -R merge (sorted) multiple files with -m remove dups with -u Note when using -k the syntax is -km,n where m is the starting key and n is the ending key. If the sorting is on the 5 th field alone (for ex.), we speciy -k5,5 Environment variables such as LC_ALL, LC_COLLATE, or LANG can affect the output of sort and other commands. Example To sort a csv file numerically on the 2 nd field in reverse order we'd use sort -t \",\" -k2nr,2 file More examples here Sorting Large Files \u00b6 The sort that you find on Linux comes from the coreutils package and implements an External R-Way merge. It splits up the data into chunks that it can handle in memory, stores them on disc and then merges them. The chunks are done in parallel, if the machine has the processors for that. So if there was to be a limit, it is the free disc space that sort can use to store the temporary files it has to merge, combined with the result. source ! sort -- help Usage: sort [OPTION]... [FILE]... or: sort [OPTION]... --files0-from=F Write sorted concatenation of all FILE(s) to standard output. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. Ordering options: -b, --ignore-leading-blanks ignore leading blanks -d, --dictionary-order consider only blanks and alphanumeric characters -f, --ignore-case fold lower case to upper case characters -g, --general-numeric-sort compare according to general numerical value -i, --ignore-nonprinting consider only printable characters -M, --month-sort compare (unknown) < 'JAN' < ... < 'DEC' -h, --human-numeric-sort compare human readable numbers (e.g., 2K 1G) -n, --numeric-sort compare according to string numerical value -R, --random-sort shuffle, but group identical keys. See shuf(1) --random-source=FILE get random bytes from FILE -r, --reverse reverse the result of comparisons --sort=WORD sort according to WORD: general-numeric -g, human-numeric -h, month -M, numeric -n, random -R, version -V -V, --version-sort natural sort of (version) numbers within text Other options: --batch-size=NMERGE merge at most NMERGE inputs at once; for more use temp files -c, --check, --check=diagnose-first check for sorted input; do not sort -C, --check=quiet, --check=silent like -c, but do not report first bad line --compress-program=PROG compress temporaries with PROG; decompress them with PROG -d --debug annotate the part of the line used to sort, and warn about questionable usage to stderr --files0-from=F read input from the files specified by NUL-terminated names in file F; If F is - then read names from standard input -k, --key=KEYDEF sort via a key; KEYDEF gives location and type -m, --merge merge already sorted files; do not sort -o, --output=FILE write result to FILE instead of standard output -s, --stable stabilize sort by disabling last-resort comparison -S, --buffer-size=SIZE use SIZE for main memory buffer -t, --field-separator=SEP use SEP instead of non-blank to blank transition -T, --temporary-directory=DIR use DIR for temporaries, not $TMPDIR or /tmp; multiple options specify multiple directories --parallel=N change the number of sorts run concurrently to N -u, --unique with -c, check for strict ordering; without -c, output only the first of an equal run -z, --zero-terminated line delimiter is NUL, not newline --help display this help and exit --version output version information and exit KEYDEF is F[.C][OPTS][,F[.C][OPTS]] for start and stop position, where F is a field number and C a character position in the field; both are origin 1, and the stop position defaults to the line's end. If neither -t nor -b is in effect, characters in a field are counted from the beginning of the preceding whitespace. OPTS is one or more single-letter ordering options [bdfgiMhnRrV], which override global ordering options for that key. If no key is given, use the entire line as the key. Use --debug to diagnose incorrect key usage. SIZE may be followed by the following multiplicative suffixes: % 1% of memory, b 1, K 1024 (default), and so on for M, G, T, P, E, Z, Y. *** WARNING *** The locale specified by the environment affects sort order. Set LC_ALL=C to get the traditional sort order that uses native byte values. GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/sort> or available locally via: info '(coreutils) sort invocation' # sort listing of all files in a directory by their size (human-readable) ! ls - lh | sort - k5 , 5 - h - r -rw------- 1 root root 709M Mar 4 01:00 kddcup.data -rw------- 1 root root 671M Mar 4 01:05 flights.csv -rw-r--r-- 1 root root 94K Mar 4 01:26 flights_1k.csv -rw-r--r-- 1 root root 1.3K Mar 4 01:13 kddcup-names -rw-r--r-- 1 root root 1.2K Mar 4 01:01 get-csvs.sh total 1.4G # top 10 durations in the kdd data # Note # sed '1d' removes the header # cut is for retaining the first few columns ! cat kddcup . data | sed '1d' | sort - nr - k1 , 1 | cut - d , - f1 - 6 | head 58329,udp,domain_u,SF,42,44 42908,tcp,private,RSTR,1,0 42888,tcp,private,RSTR,1,0 42862,tcp,private,RSTR,1,0 42837,tcp,private,RSTR,1,0 42804,tcp,private,RSTR,1,0 42778,tcp,private,RSTR,1,0 42746,tcp,echo,RSTR,1,0 42723,tcp,private,RSTR,1,0 42699,tcp,discard,RSTR,1,0 cut: write error: Broken pipe sort has a -u flag that removes duplicate rows so a sorted listing of unique rows is produced. Note The commands sort -u and sort | uniq are equivalent, but this equivalence does not extend to arbitrary sort options. For example, sort -n -u inspects only the value of the initial numeric string when checking for uniqueness, whereas sort -n | uniq inspects the entire line. shuf \u00b6 useful for permutations and random sampling with or without replacement ( bootstrapped samples, anyone?) ! shuf -- help Usage: shuf [OPTION]... [FILE] or: shuf -e [OPTION]... [ARG]... or: shuf -i LO-HI [OPTION]... Write a random permutation of the input lines to standard output. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. -e, --echo treat each ARG as an input line -i, --input-range=LO-HI treat each number LO through HI as an input line -n, --head-count=COUNT output at most COUNT lines -o, --output=FILE write result to FILE instead of standard output --random-source=FILE get random bytes from FILE -r, --repeat output lines can be repeated -z, --zero-terminated line delimiter is NUL, not newline --help display this help and exit --version output version information and exit GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/shuf> or available locally via: info '(coreutils) shuf invocation' # without replacement ! shuf - i1 - 9 - n 5 8 9 5 2 3 # Simulate a coin flip (with replacement) ! shuf - i0 - 1 - r - n 10 0 1 0 1 0 0 1 1 0 1 # random sample from a file ! cat kddcup . data | cut - d , - f1 - 6 | sed '1d' | shuf - n 5 0,icmp,ecr_i,SF,1032,0 0,icmp,ecr_i,SF,1032,0 0,icmp,ecr_i,SF,1032,0 0,icmp,ecr_i,SF,1032,0 0,icmp,ecr_i,SF,1032,0 uniq \u00b6 typically used to uniquely list lines from an input source find/remove duplicate rows in a file frequency tables OPTIONS -u, --unique only print unique lines -c, --count prefix lines by the number of occurrences -d, --repeated only print duplicate lines, one for each group -D print all duplicate lines --all-repeated [= METHOD ] like -D, but allow separating groups with an empty line ; METHOD ={ none ( default ) ,prepend,separate } To operate properly, duplicate lines must be contiguously positioned in the input. So, normally the input to the uniq command is first sorted. # number of uniuqe rows for columns 1-6, 42 in the kdd data ! cat kddcup . data | sed '1d' | cut - d , - f1 - 6 , 42 | sort | uniq - u | wc - l 198175 # these rows have duplicates ! cat kddcup . data | sed '1d' | cut - d , - f3 - 6 , 42 | sort | uniq - d | head IRC,RSTO,0,0,normal. IRC,RSTR,1010,6365,normal. IRC,RSTR,4420,7766,normal. IRC,RSTR,62,116,normal. IRC,RSTR,73,16,normal. IRC,RSTR,76,18,normal. IRC,RSTR,77,18,normal. IRC,S1,0,0,normal. IRC,SF,66,17,normal. IRC,SF,66,18,normal. uniq: write error: Broken pipe # frequency table ! cat kddcup . data | sed '1d' | cut - d , - f42 | sort | uniq - c | sort - nr | head - n 5 2807886 smurf. 1072017 neptune. 972780 normal. 15892 satan. 12481 ipsweep. # most number of flights? ! cat flights . csv | sed '1d' | cut - d , - f9 , 17 - 18 | sort | uniq - c | sort - nr | head - n 5 9658 WN,HOU,DAL 9631 WN,DAL,HOU 7496 WN,LAX,OAK 7467 WN,OAK,LAX 7339 HA,OGG,HNL sort: write failed: standard output: Broken pipe sort: write error comm \u00b6 for comparing sorted files FILE1 and FILE2 line by line. With no options, produce three-column output. Column one contains lines unique to FILE1, column two contains lines unique to FILE2, and column three contains lines common to both files. OPTIONS -1 suppress column 1 ( lines unique to FILE1 ) -2 suppress column 2 ( lines unique to FILE2 ) -3 suppress column 3 ( lines that appear in both files ) EXAMPLES comm -12 file1 file2 # Print only lines present in both file1 and file2. comm -3 file1 file2 # Print lines in file1 not in file2, and vice versa. # pull a random sample of 10k rows ! shuf - n 10000 kddcup . data | sort > kdd_10k . csv # keep the first 7.5k rows as the train set ! head - n 7500 kdd_10k . csv | sort > kdd_10k_train . csv # keep the other 2.5k as the test set ! comm - 3 kdd_10k . csv kdd_10k_train . csv | sort > kdd_10k_test . csv # check no. of rows ! ls | grep '_10k' | xargs wc - l 10000 kdd_10k.csv 2500 kdd_10k_test.csv 7500 kdd_10k_train.csv 20000 total # see if we made a mistake ! comm - 12 kdd_10k_train . csv kdd_10k_test . csv | wc - l 0 # delete created files ! ls | grep '_10k' | xargs rm paste \u00b6 naive, brute force long-to-wide! concat files (column binding) ! paste -- help Usage: paste [OPTION]... [FILE]... Write lines consisting of the sequentially corresponding lines from each FILE, separated by TABs, to standard output. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. -d, --delimiters=LIST reuse characters from LIST instead of TABs -s, --serial paste one file at a time instead of in parallel -z, --zero-terminated line delimiter is NUL, not newline --help display this help and exit --version output version information and exit GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/paste> or available locally via: info '(coreutils) paste invocation' # join consecutive lines into a csv ! shuf - i0 - 99 - rn 25 | paste - d , - - - - - 54,58,50,44,6 25,67,88,11,52 94,75,7,77,83 95,56,82,46,44 2,58,28,18,90 # fold data any which way ! seq 12 | paste - d , - - | paste - d ':' - - - 1,2:3,4:5,6 7,8:9,10:11,12 # column binding! ! seq 10 21 | paste - d , - - > f01 . txt ! seq 80 89 | paste - d , - - > f02 . txt ! paste - d , f01 . txt f02 . txt 10,11,80,81 12,13,82,83 14,15,84,85 16,17,86,87 18,19,88,89 20,21, ! paste - d , - s f01 . txt f02 . txt 10,11,12,13,14,15,16,17,18,19,20,21 80,81,82,83,84,85,86,87,88,89 ! rm f01 . txt f02 . txt ls - lh -- block - size = MB . total 1446MB -rw------- 1 root root 703MB Mar 4 01:05 flights.csv -rw-r--r-- 1 root root 1MB Mar 4 01:26 flights_1k.csv -rw-r--r-- 1 root root 1MB Mar 4 01:01 get-csvs.sh -rw-r--r-- 1 root root 1MB Mar 4 01:13 kddcup-names -rw------- 1 root root 743MB Mar 4 01:00 kddcup.data numfmt \u00b6 numfmt reads numbers in various representations and reformats them as requested numfmt can optionally extract numbers from specific columns, maintaining proper line padding and alignment. Formatting floating point numbers \u00b6 ! head Sales . txt | sed '1d' | cut - d '|' - f12 - 15 | tr ',' '.' | xargs - d '|' numfmt -- format = \" %0.4f \" | xargs - n4 | tr ' ' ',' | xsv table 17.3224 20.9600 9.9800 7.3424 43.38017000000000000000 52.4900 35.8600 7.5202 6.60331000000000000000 7.9900 5.2100 1.3934 24.78512000000000000000 29.9900 12.2300 12.5552 12.38843000000000000000 14.9900 6.3400 6.0485 .94340000000000000000 1.0000 0.4800 0.4634 2.22314000000000000000 2.6900 0.9400 1.2832 10.70248000000000000000 12.9500 5.6000 5.1025 3.29752000000000000000 3.9900 0.9300 2.3676 Looping over files in Bash \u00b6 for f in * ; do # do something, for example # wc -l \"$f\" ; done Also see \u00b6 The tee command copies standard input to standard output and also to any files given as arguments. This is useful when you want not only to send some data down a pipe, but also to save a copy.","title":"Coreutils"},{"location":"01_UnixCoreutils/#gnu-coreutils","text":"These constitute one of most useful of all GNU Packages that are a intergral part of the daily work of millions of programmers/scientists. (Other well known packages include grep , screen , gzip , tar , time etc. Oh, and the R programming language.) \" The GNU Core Utilities are the basic file, shell and text manipulation utilities of the GNU operating system.These are the core utilities which are expected to exist on every OS . \" Examples include Manipulate Utils files chgrp, chown, chmod, cp, dd, df, dir, du, ln, ls, mkdir, mkfifo, mknod, mv, rm etc. text cat, cksum, head, tail, md5sum, nl, od, pr, tsort, join, wc, tac, paste etc. shell basename, chroot, date, dirname, echo, env, groups, hostname, nice, nohup, printf, sleep etc.","title":"GNU Coreutils"},{"location":"01_UnixCoreutils/#getting-help","text":"Documentation is built-in, so whenever you can't remember something, look it up using COMMAND --help # or man COMMAND","title":"Getting Help"},{"location":"01_UnixCoreutils/#know-your-system","text":"# Kernel Info uname -a # all uname -r # exact # Linux Version lsb_release -a # all lsb_release -r # exact lsb_release -a -u # for derived distros like elementary # HDD Partition Info fdisk -l lsblk -o NAME,SIZE # human readable tree # LOCALE # List what locales currently defined for the current user locale # if needed, generate missing locale and reconfigure sudo locale-gen \"en_US.UTF-8\" sudo dpkg-reconfigure locales # ENVIRONMENT VARIABLES # list all active variables env # add a new variable to the list EXPORT ENV_VAR = val # PATH # list the programs on the path echo $PATH # add optional binary to the end of the path PATH = $PATH :~/opt/bin # add something ot the beginning PATH = ~/opt/bin: $PATH","title":"Know your system"},{"location":"01_UnixCoreutils/#shortcuts","text":"Ctrl + U Clears the line from the cursor point back to the beginning. Ctrl + A Moves the cursor to the beginning of the line. Ctrl + E Moves the cursor to the end of the line. Ctrl + R Allows you to search through the previous commands","title":"Shortcuts"},{"location":"01_UnixCoreutils/#why-coreutils","text":"The basic tenet of UNIX philosophy is to \"create programs (or processes) that do one thing, and do that one thing well.\" It is a philosophy demanding careful thought about interfaces and ways of joining these smaller (hopefully more simple) processes together to create useful results . Normally text data flows between these interfaces. More advanced text processing tools and languages (like perl, python, and ruby) have been developed, which though very capable in their own right, are not always available, especially in a production environment. These coreutils therefore become an indespensable tool in the data scientist's toolbox if he wants to build systems that work beyond his laptop.","title":"Why Coreutils?"},{"location":"01_UnixCoreutils/#common-util-options","text":"can appear in any order (recommended: options before operands ) options can be long (begin with -- ) or abbreviated (begin wiht - ) available to all programs (eg. --help , --version ) Nearly every command invocation yields an integral exit status that can be used to change how other commands work. For the vast majority of commands, an exit status of zero indicates success, nonzero indicates failure . Alright, enough talk, let's dive in. Like most things on the internet these days, our story begins with a ...","title":"Common Util Options"},{"location":"01_UnixCoreutils/#cat-tac","text":"Copies file/stdin to file/stdout (careful with large files!!) Useful in piping text to other programns, and to write output to files ! cat -- help Usage: cat [OPTION]... [FILE]... Concatenate FILE(s) to standard output. With no FILE, or when FILE is -, read standard input. -A, --show-all equivalent to -vET -b, --number-nonblank number nonempty output lines, overrides -n -e equivalent to -vE -E, --show-ends display $ at end of each line -n, --number number all output lines -s, --squeeze-blank suppress repeated empty output lines -t equivalent to -vT -T, --show-tabs display TAB characters as ^I -u (ignored) -v, --show-nonprinting use ^ and M- notation, except for LFD and TAB --help display this help and exit --version output version information and exit Examples: cat f - g Output f's contents, then standard input, then g's contents. cat Copy standard input to standard output. GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/cat> or available locally via: info '(coreutils) cat invocation' %% writefile f01 . txt 1 2 3 4 5 6 7 8 9 10 11 12 Writing f01.txt # create an index ! cat - n f01 . txt 1 1 2 3 2 4 5 6 3 4 7 8 9 5 6 7 10 11 12 # create index for nonmissing ! cat - b f01 . txt 1 1 2 3 2 4 5 6 3 7 8 9 \u200b \u200b 4 10 11 12 # squeeze multiple blank lines to 1 ! cat - bs f01 . txt 1 1 2 3 2 4 5 6 3 7 8 9 4 10 11 12 # reverse rows ! tac f01 . txt 10 11 12 7 8 9 4 5 6 1 2 3","title":"cat, tac"},{"location":"01_UnixCoreutils/#cat","text":"** To append Structured Files ** Concatenating files (same number of columns in the same order, no headers) cat f1.csv f2.csv f3.csv > f1_f2_f3.csv - Appending a file to an existing file using the >> operator, removing headers from subsequent files cat f1.csv > concatenated.csv cat f2.csv | sed \"1 d\" >> concatenated.csv cat f3.csv | sed \"1 d\" >> concatenated.csv Create a new file with the last few records of an existing file head -n 1 foo.csv > new_foo.csv tail -n 9 foo.csv >> new_foo.csv # row binding ! seq 10 21 | paste - d , - - > f01 . txt ! seq 80 89 | paste - d , - - > f02 . txt ` ! cat f01 . txt f02 . txt 10,11 12,13 14,15 16,17 18,19 20,21 80,81 82,83 84,85 86,87 88,89","title":"cat"},{"location":"01_UnixCoreutils/#nl","text":"add line numbers (generate and index like in pandas ) see options with nl --help -s to specify delimiter -b to use regexs # for a csv cat flights.csv | nl -s, | tail # for a pipe-delimited file cat sample_2.txt | nl -s '|' | tail","title":"nl"},{"location":"01_UnixCoreutils/#head-tail","text":"display the first or last n rows ( n=10 by default) of a file (or many files) ! head - n 3 flights . csv Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,SMF,ONT,389,4,11,0,,0,0,0,0,0,0 2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,SMF,PDX,479,5,6,0,,0,0,0,0,0,0 ! tail - n 3 flights . csv 2007,12,15,6,1024,1025,1750,1735,DL,61,N623DL,266,250,233,15,-1,LAX,ATL,1946,14,19,0,,0,0,0,15,0,0 2007,12,15,6,1353,1315,1658,1622,DL,62,N970DL,125,127,100,36,38,DFW,ATL,732,11,14,0,,0,0,0,0,0,36 2007,12,15,6,1824,1800,2001,1928,DL,63,N628DL,97,88,61,33,24,ATL,MCO,403,10,26,0,,0,24,0,9,0,0","title":"head, tail"},{"location":"01_UnixCoreutils/#split","text":"creates output files containing consecutive or interleaved sections (1000 lines by default) of input files created are saved in the cd, named as PREFIXSUFFIX PREFIX is supplied with the call SUFFIX length is supplied (defaults to 2, so generates codes like xaa, xab, xac ...) ! split -- help Usage: split [OPTION]... [FILE [PREFIX]] Output pieces of FILE to PREFIXaa, PREFIXab, ...; default size is 1000 lines, and default PREFIX is 'x'. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. -a, --suffix-length=N generate suffixes of length N (default 2) --additional-suffix=SUFFIX append an additional SUFFIX to file names -b, --bytes=SIZE put SIZE bytes per output file -C, --line-bytes=SIZE put at most SIZE bytes of records per output file -d use numeric suffixes starting at 0, not alphabetic --numeric-suffixes[=FROM] same as -d, but allow setting the start value -e, --elide-empty-files do not generate empty output files with '-n' --filter=COMMAND write to shell COMMAND; file name is $FILE -l, --lines=NUMBER put NUMBER lines/records per output file -n, --number=CHUNKS generate CHUNKS output files; see explanation below -t, --separator=SEP use SEP instead of newline as the record separator; '\\0' (zero) specifies the NUL character -u, --unbuffered immediately copy input to output with '-n r/...' --verbose print a diagnostic just before each output file is opened --help display this help and exit --version output version information and exit The SIZE argument is an integer and optional unit (example: 10K is 10*1024). Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000). CHUNKS may be: N split into N files based on size of input K/N output Kth of N to stdout l/N split into N files without splitting lines/records l/K/N output Kth of N to stdout without splitting lines/records r/N like 'l' but use round robin distribution r/K/N likewise but only output Kth of N to stdout GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/split> or available locally via: info '(coreutils) split invocation' ! tail - n 1000 flights . csv > flights_1k . csv ! split - n 3 flights_1k . csv ls | grep 'xa' xaa xab xac # provide prefix ! split - n 3 flights_1k . csv flights_1k_split_ ls | grep 'split' flights_1k_split_aa flights_1k_split_ab flights_1k_split_ac # provide longer suffix if there are going to be many parts ! split - n 5 - a 4 flights_1k . csv Part_ ls | grep '^Part_' Part_aaaa Part_aaab Part_aaac Part_aaad Part_aaae # Cleaning up ! ls | grep '_split_' | xargs rm ! ls | grep 'Part_' | xargs rm ! ls | grep 'xa' | xargs rm","title":"split"},{"location":"01_UnixCoreutils/#csplit","text":"for copy-till-you-see-this kind of splits ! csplit -- help Usage: csplit [OPTION]... FILE PATTERN... Output pieces of FILE separated by PATTERN(s) to files 'xx00', 'xx01', ..., and output byte counts of each piece to standard output. Read standard input if FILE is - Mandatory arguments to long options are mandatory for short options too. -b, --suffix-format=FORMAT use sprintf FORMAT instead of %02d -f, --prefix=PREFIX use PREFIX instead of 'xx' -k, --keep-files do not remove output files on errors --suppress-matched suppress the lines matching PATTERN -n, --digits=DIGITS use specified number of digits instead of 2 -s, --quiet, --silent do not print counts of output file sizes -z, --elide-empty-files remove empty output files --help display this help and exit --version output version information and exit Each PATTERN may be: INTEGER copy up to but not including specified line number /REGEXP/[OFFSET] copy up to but not including a matching line %REGEXP%[OFFSET] skip to, but not including a matching line {INTEGER} repeat the previous pattern specified number of times {*} repeat the previous pattern as many times as possible A line OFFSET is a required '+' or '-' followed by a positive integer. GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/csplit> or available locally via: info '(coreutils) csplit invocation'","title":"csplit"},{"location":"01_UnixCoreutils/#wc","text":"counts the Number of bytes, characters, whitespace-separated words, and newlines ! wc -- help Usage: wc [OPTION]... [FILE]... or: wc [OPTION]... --files0-from=F Print newline, word, and byte counts for each FILE, and a total line if more than one FILE is specified. A word is a non-zero-length sequence of characters delimited by white space. With no FILE, or when FILE is -, read standard input. The options below may be used to select which counts are printed, always in the following order: newline, word, character, byte, maximum line length. -c, --bytes print the byte counts -m, --chars print the character counts -l, --lines print the newline counts --files0-from=F read input from the files specified by NUL-terminated names in file F; If F is - then read names from standard input -L, --max-line-length print the maximum display width -w, --words print the word counts --help display this help and exit --version output version information and exit GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/wc> or available locally via: info '(coreutils) wc invocation' # count and filename ! ls | grep csv | xargs wc - l 7453216 flights.csv 37 get-csvs.sh 4898432 kdd.csv 12351685 total run_on_bash ( 'ls | grep csv | xargs wc -l' ) 7453216 flights.csv 37 get-csvs.sh 4898432 kdd.csv 12351685 total","title":"wc"},{"location":"01_UnixCoreutils/#cut","text":"select columns by position (comma separated list or hyphenated ranges) OPTIONS -d, --delimiter = DELIM use DELIM instead of TAB for field delimiter -f, --fields = LIST select only these fields -s, --only-delimited do not print lines not containing delimiters --output-delimiter = STRING use STRING as the output delimiter ! head - n 5 kdd . csv | cut - d , - f1 - 6 , 42 duration,protocol_type,service,flag,src_bytes,dst_bytes 0,tcp,http,SF,215,45076,normal. 0,tcp,http,SF,162,4528,normal. 0,tcp,http,SF,236,1228,normal. 0,tcp,http,SF,233,2032,normal. run_on_bash ( 'head -n 5 flights.csv | cut -d, -f9-10,15-18 | csvlook' ) | UniqueCarrier | FlightNum | ArrDelay | DepDelay | Origin | Dest | | ------------- | --------- | -------- | -------- | ------ | ---- | | WN | 2,891 | 1 | 7 | SMF | ONT | | WN | 462 | 8 | 13 | SMF | PDX | | WN | 1,229 | 34 | 36 | SMF | PDX | | WN | 1,355 | 26 | 30 | SMF | PDX |","title":"cut"},{"location":"01_UnixCoreutils/#sort","text":"sort is quite versatile, and can be used to sort, sort & merge, randomize, and, deduplicate files. Useful sort options case insensitive with -f numbers with -n descending order with -r if lines have leading blanks -b sort by values in column 5 .... -k5 (use -t for delimiter here) sort in random order with -R merge (sorted) multiple files with -m remove dups with -u Note when using -k the syntax is -km,n where m is the starting key and n is the ending key. If the sorting is on the 5 th field alone (for ex.), we speciy -k5,5 Environment variables such as LC_ALL, LC_COLLATE, or LANG can affect the output of sort and other commands. Example To sort a csv file numerically on the 2 nd field in reverse order we'd use sort -t \",\" -k2nr,2 file More examples here","title":"sort"},{"location":"01_UnixCoreutils/#sorting-large-files","text":"The sort that you find on Linux comes from the coreutils package and implements an External R-Way merge. It splits up the data into chunks that it can handle in memory, stores them on disc and then merges them. The chunks are done in parallel, if the machine has the processors for that. So if there was to be a limit, it is the free disc space that sort can use to store the temporary files it has to merge, combined with the result. source ! sort -- help Usage: sort [OPTION]... [FILE]... or: sort [OPTION]... --files0-from=F Write sorted concatenation of all FILE(s) to standard output. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. Ordering options: -b, --ignore-leading-blanks ignore leading blanks -d, --dictionary-order consider only blanks and alphanumeric characters -f, --ignore-case fold lower case to upper case characters -g, --general-numeric-sort compare according to general numerical value -i, --ignore-nonprinting consider only printable characters -M, --month-sort compare (unknown) < 'JAN' < ... < 'DEC' -h, --human-numeric-sort compare human readable numbers (e.g., 2K 1G) -n, --numeric-sort compare according to string numerical value -R, --random-sort shuffle, but group identical keys. See shuf(1) --random-source=FILE get random bytes from FILE -r, --reverse reverse the result of comparisons --sort=WORD sort according to WORD: general-numeric -g, human-numeric -h, month -M, numeric -n, random -R, version -V -V, --version-sort natural sort of (version) numbers within text Other options: --batch-size=NMERGE merge at most NMERGE inputs at once; for more use temp files -c, --check, --check=diagnose-first check for sorted input; do not sort -C, --check=quiet, --check=silent like -c, but do not report first bad line --compress-program=PROG compress temporaries with PROG; decompress them with PROG -d --debug annotate the part of the line used to sort, and warn about questionable usage to stderr --files0-from=F read input from the files specified by NUL-terminated names in file F; If F is - then read names from standard input -k, --key=KEYDEF sort via a key; KEYDEF gives location and type -m, --merge merge already sorted files; do not sort -o, --output=FILE write result to FILE instead of standard output -s, --stable stabilize sort by disabling last-resort comparison -S, --buffer-size=SIZE use SIZE for main memory buffer -t, --field-separator=SEP use SEP instead of non-blank to blank transition -T, --temporary-directory=DIR use DIR for temporaries, not $TMPDIR or /tmp; multiple options specify multiple directories --parallel=N change the number of sorts run concurrently to N -u, --unique with -c, check for strict ordering; without -c, output only the first of an equal run -z, --zero-terminated line delimiter is NUL, not newline --help display this help and exit --version output version information and exit KEYDEF is F[.C][OPTS][,F[.C][OPTS]] for start and stop position, where F is a field number and C a character position in the field; both are origin 1, and the stop position defaults to the line's end. If neither -t nor -b is in effect, characters in a field are counted from the beginning of the preceding whitespace. OPTS is one or more single-letter ordering options [bdfgiMhnRrV], which override global ordering options for that key. If no key is given, use the entire line as the key. Use --debug to diagnose incorrect key usage. SIZE may be followed by the following multiplicative suffixes: % 1% of memory, b 1, K 1024 (default), and so on for M, G, T, P, E, Z, Y. *** WARNING *** The locale specified by the environment affects sort order. Set LC_ALL=C to get the traditional sort order that uses native byte values. GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/sort> or available locally via: info '(coreutils) sort invocation' # sort listing of all files in a directory by their size (human-readable) ! ls - lh | sort - k5 , 5 - h - r -rw------- 1 root root 709M Mar 4 01:00 kddcup.data -rw------- 1 root root 671M Mar 4 01:05 flights.csv -rw-r--r-- 1 root root 94K Mar 4 01:26 flights_1k.csv -rw-r--r-- 1 root root 1.3K Mar 4 01:13 kddcup-names -rw-r--r-- 1 root root 1.2K Mar 4 01:01 get-csvs.sh total 1.4G # top 10 durations in the kdd data # Note # sed '1d' removes the header # cut is for retaining the first few columns ! cat kddcup . data | sed '1d' | sort - nr - k1 , 1 | cut - d , - f1 - 6 | head 58329,udp,domain_u,SF,42,44 42908,tcp,private,RSTR,1,0 42888,tcp,private,RSTR,1,0 42862,tcp,private,RSTR,1,0 42837,tcp,private,RSTR,1,0 42804,tcp,private,RSTR,1,0 42778,tcp,private,RSTR,1,0 42746,tcp,echo,RSTR,1,0 42723,tcp,private,RSTR,1,0 42699,tcp,discard,RSTR,1,0 cut: write error: Broken pipe sort has a -u flag that removes duplicate rows so a sorted listing of unique rows is produced. Note The commands sort -u and sort | uniq are equivalent, but this equivalence does not extend to arbitrary sort options. For example, sort -n -u inspects only the value of the initial numeric string when checking for uniqueness, whereas sort -n | uniq inspects the entire line.","title":"Sorting Large Files"},{"location":"01_UnixCoreutils/#shuf","text":"useful for permutations and random sampling with or without replacement ( bootstrapped samples, anyone?) ! shuf -- help Usage: shuf [OPTION]... [FILE] or: shuf -e [OPTION]... [ARG]... or: shuf -i LO-HI [OPTION]... Write a random permutation of the input lines to standard output. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. -e, --echo treat each ARG as an input line -i, --input-range=LO-HI treat each number LO through HI as an input line -n, --head-count=COUNT output at most COUNT lines -o, --output=FILE write result to FILE instead of standard output --random-source=FILE get random bytes from FILE -r, --repeat output lines can be repeated -z, --zero-terminated line delimiter is NUL, not newline --help display this help and exit --version output version information and exit GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/shuf> or available locally via: info '(coreutils) shuf invocation' # without replacement ! shuf - i1 - 9 - n 5 8 9 5 2 3 # Simulate a coin flip (with replacement) ! shuf - i0 - 1 - r - n 10 0 1 0 1 0 0 1 1 0 1 # random sample from a file ! cat kddcup . data | cut - d , - f1 - 6 | sed '1d' | shuf - n 5 0,icmp,ecr_i,SF,1032,0 0,icmp,ecr_i,SF,1032,0 0,icmp,ecr_i,SF,1032,0 0,icmp,ecr_i,SF,1032,0 0,icmp,ecr_i,SF,1032,0","title":"shuf"},{"location":"01_UnixCoreutils/#uniq","text":"typically used to uniquely list lines from an input source find/remove duplicate rows in a file frequency tables OPTIONS -u, --unique only print unique lines -c, --count prefix lines by the number of occurrences -d, --repeated only print duplicate lines, one for each group -D print all duplicate lines --all-repeated [= METHOD ] like -D, but allow separating groups with an empty line ; METHOD ={ none ( default ) ,prepend,separate } To operate properly, duplicate lines must be contiguously positioned in the input. So, normally the input to the uniq command is first sorted. # number of uniuqe rows for columns 1-6, 42 in the kdd data ! cat kddcup . data | sed '1d' | cut - d , - f1 - 6 , 42 | sort | uniq - u | wc - l 198175 # these rows have duplicates ! cat kddcup . data | sed '1d' | cut - d , - f3 - 6 , 42 | sort | uniq - d | head IRC,RSTO,0,0,normal. IRC,RSTR,1010,6365,normal. IRC,RSTR,4420,7766,normal. IRC,RSTR,62,116,normal. IRC,RSTR,73,16,normal. IRC,RSTR,76,18,normal. IRC,RSTR,77,18,normal. IRC,S1,0,0,normal. IRC,SF,66,17,normal. IRC,SF,66,18,normal. uniq: write error: Broken pipe # frequency table ! cat kddcup . data | sed '1d' | cut - d , - f42 | sort | uniq - c | sort - nr | head - n 5 2807886 smurf. 1072017 neptune. 972780 normal. 15892 satan. 12481 ipsweep. # most number of flights? ! cat flights . csv | sed '1d' | cut - d , - f9 , 17 - 18 | sort | uniq - c | sort - nr | head - n 5 9658 WN,HOU,DAL 9631 WN,DAL,HOU 7496 WN,LAX,OAK 7467 WN,OAK,LAX 7339 HA,OGG,HNL sort: write failed: standard output: Broken pipe sort: write error","title":"uniq"},{"location":"01_UnixCoreutils/#comm","text":"for comparing sorted files FILE1 and FILE2 line by line. With no options, produce three-column output. Column one contains lines unique to FILE1, column two contains lines unique to FILE2, and column three contains lines common to both files. OPTIONS -1 suppress column 1 ( lines unique to FILE1 ) -2 suppress column 2 ( lines unique to FILE2 ) -3 suppress column 3 ( lines that appear in both files ) EXAMPLES comm -12 file1 file2 # Print only lines present in both file1 and file2. comm -3 file1 file2 # Print lines in file1 not in file2, and vice versa. # pull a random sample of 10k rows ! shuf - n 10000 kddcup . data | sort > kdd_10k . csv # keep the first 7.5k rows as the train set ! head - n 7500 kdd_10k . csv | sort > kdd_10k_train . csv # keep the other 2.5k as the test set ! comm - 3 kdd_10k . csv kdd_10k_train . csv | sort > kdd_10k_test . csv # check no. of rows ! ls | grep '_10k' | xargs wc - l 10000 kdd_10k.csv 2500 kdd_10k_test.csv 7500 kdd_10k_train.csv 20000 total # see if we made a mistake ! comm - 12 kdd_10k_train . csv kdd_10k_test . csv | wc - l 0 # delete created files ! ls | grep '_10k' | xargs rm","title":"comm"},{"location":"01_UnixCoreutils/#paste","text":"naive, brute force long-to-wide! concat files (column binding) ! paste -- help Usage: paste [OPTION]... [FILE]... Write lines consisting of the sequentially corresponding lines from each FILE, separated by TABs, to standard output. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. -d, --delimiters=LIST reuse characters from LIST instead of TABs -s, --serial paste one file at a time instead of in parallel -z, --zero-terminated line delimiter is NUL, not newline --help display this help and exit --version output version information and exit GNU coreutils online help: <http://www.gnu.org/software/coreutils/> Full documentation at: <http://www.gnu.org/software/coreutils/paste> or available locally via: info '(coreutils) paste invocation' # join consecutive lines into a csv ! shuf - i0 - 99 - rn 25 | paste - d , - - - - - 54,58,50,44,6 25,67,88,11,52 94,75,7,77,83 95,56,82,46,44 2,58,28,18,90 # fold data any which way ! seq 12 | paste - d , - - | paste - d ':' - - - 1,2:3,4:5,6 7,8:9,10:11,12 # column binding! ! seq 10 21 | paste - d , - - > f01 . txt ! seq 80 89 | paste - d , - - > f02 . txt ! paste - d , f01 . txt f02 . txt 10,11,80,81 12,13,82,83 14,15,84,85 16,17,86,87 18,19,88,89 20,21, ! paste - d , - s f01 . txt f02 . txt 10,11,12,13,14,15,16,17,18,19,20,21 80,81,82,83,84,85,86,87,88,89 ! rm f01 . txt f02 . txt ls - lh -- block - size = MB . total 1446MB -rw------- 1 root root 703MB Mar 4 01:05 flights.csv -rw-r--r-- 1 root root 1MB Mar 4 01:26 flights_1k.csv -rw-r--r-- 1 root root 1MB Mar 4 01:01 get-csvs.sh -rw-r--r-- 1 root root 1MB Mar 4 01:13 kddcup-names -rw------- 1 root root 743MB Mar 4 01:00 kddcup.data","title":"paste"},{"location":"01_UnixCoreutils/#numfmt","text":"numfmt reads numbers in various representations and reformats them as requested numfmt can optionally extract numbers from specific columns, maintaining proper line padding and alignment.","title":"numfmt"},{"location":"01_UnixCoreutils/#formatting-floating-point-numbers","text":"! head Sales . txt | sed '1d' | cut - d '|' - f12 - 15 | tr ',' '.' | xargs - d '|' numfmt -- format = \" %0.4f \" | xargs - n4 | tr ' ' ',' | xsv table 17.3224 20.9600 9.9800 7.3424 43.38017000000000000000 52.4900 35.8600 7.5202 6.60331000000000000000 7.9900 5.2100 1.3934 24.78512000000000000000 29.9900 12.2300 12.5552 12.38843000000000000000 14.9900 6.3400 6.0485 .94340000000000000000 1.0000 0.4800 0.4634 2.22314000000000000000 2.6900 0.9400 1.2832 10.70248000000000000000 12.9500 5.6000 5.1025 3.29752000000000000000 3.9900 0.9300 2.3676","title":"Formatting floating point numbers"},{"location":"01_UnixCoreutils/#looping-over-files-in-bash","text":"for f in * ; do # do something, for example # wc -l \"$f\" ; done","title":"Looping over files in Bash"},{"location":"01_UnixCoreutils/#also-see","text":"The tee command copies standard input to standard output and also to any files given as arguments. This is useful when you want not only to send some data down a pipe, but also to save a copy.","title":"Also see"},{"location":"02_csvkit/","text":"csvkit \u00b6 csvkit Data csvlook csvcut csvgrep csvsort csvstat csvstack csvsql Data \u00b6 import os os . chdir ( \"/home/data\" ) ls - l -- block - size = MB total 1446MB -rw------- 1 root root 703MB Mar 4 01:05 flights.csv -rw-r--r-- 1 root root 1MB Mar 4 01:26 flights_1k.csv -rw-r--r-- 1 root root 1MB Mar 4 01:01 get-csvs.sh -rw-r--r-- 1 root root 1MB Mar 4 01:13 kddcup-names -rw------- 1 root root 743MB Mar 4 01:00 kddcup.data csvlook \u00b6 ! head flights . csv | cut - d , - f5 - 12 | csvlook | DepTime | CRSDepTime | ArrTime | CRSArrTime | UniqueCarrier | FlightNum | TailNum | ActualElapsedTime | | ------- | ---------- | ------- | ---------- | ------------- | --------- | ------- | ----------------- | | 1,232 | 1,225 | 1,341 | 1,340 | WN | 2,891 | N351 | 69 | | 1,918 | 1,905 | 2,043 | 2,035 | WN | 462 | N370 | 85 | | 2,206 | 2,130 | 2,334 | 2,300 | WN | 1,229 | N685 | 88 | | 1,230 | 1,200 | 1,356 | 1,330 | WN | 1,355 | N364 | 86 | | 831 | 830 | 957 | 1,000 | WN | 2,278 | N480 | 86 | | 1,430 | 1,420 | 1,553 | 1,550 | WN | 2,386 | N611SW | 83 | | 1,936 | 1,840 | 2,217 | 2,130 | WN | 409 | N482 | 101 | | 944 | 935 | 1,223 | 1,225 | WN | 1,131 | N749SW | 99 | | 1,537 | 1,450 | 1,819 | 1,735 | WN | 1,212 | N451 | 102 | # Again, but with an index ! head flights . csv | cut - d , - f5 - 12 | csvlook - l | line_numbers | DepTime | CRSDepTime | ArrTime | CRSArrTime | UniqueCarrier | FlightNum | TailNum | ActualElapsedTime | | ------------ | ------- | ---------- | ------- | ---------- | ------------- | --------- | ------- | ----------------- | | 1 | 1,232 | 1,225 | 1,341 | 1,340 | WN | 2,891 | N351 | 69 | | 2 | 1,918 | 1,905 | 2,043 | 2,035 | WN | 462 | N370 | 85 | | 3 | 2,206 | 2,130 | 2,334 | 2,300 | WN | 1,229 | N685 | 88 | | 4 | 1,230 | 1,200 | 1,356 | 1,330 | WN | 1,355 | N364 | 86 | | 5 | 831 | 830 | 957 | 1,000 | WN | 2,278 | N480 | 86 | | 6 | 1,430 | 1,420 | 1,553 | 1,550 | WN | 2,386 | N611SW | 83 | | 7 | 1,936 | 1,840 | 2,217 | 2,130 | WN | 409 | N482 | 101 | | 8 | 944 | 935 | 1,223 | 1,225 | WN | 1,131 | N749SW | 99 | | 9 | 1,537 | 1,450 | 1,819 | 1,735 | WN | 1,212 | N451 | 102 | csvcut \u00b6 For column subsetting Options -n, --names Display column names and indices from the input CSV and exit. -c COLUMNS, --columns COLUMNS A comma separated list of column indices or names to be extracted. Defaults to all columns. -C NOT_COLUMNS, --not-columns NOT_COLUMNS A comma separated list of column indices or names to be excluded. Defaults to no columns. -x, --delete-empty-rows After cutting, delete rows which are completely empty. # Get column names and positions ! csvcut - n flights . csv | head 1: Year 2: Month 3: DayofMonth 4: DayOfWeek 5: DepTime 6: CRSDepTime 7: ArrTime 8: CRSArrTime 9: UniqueCarrier 10: FlightNum # subset columns by name with -c ! head flights . csv | csvcut - c FlightNum , UniqueCarrier | csvlook | FlightNum | UniqueCarrier | | --------- | ------------- | | 2,891 | WN | | 462 | WN | | 1,229 | WN | | 1,355 | WN | | 2,278 | WN | | 2,386 | WN | | 409 | WN | | 1,131 | WN | | 1,212 | WN | # or by positions ! head flights . csv | csvcut - c 1 - 2 , 6 , 8 | csvlook | Year | Month | CRSDepTime | CRSArrTime | | ----- | ----- | ---------- | ---------- | | 2,007 | True | 1,225 | 1,340 | | 2,007 | True | 1,905 | 2,035 | | 2,007 | True | 2,130 | 2,300 | | 2,007 | True | 1,200 | 1,330 | | 2,007 | True | 830 | 1,000 | | 2,007 | True | 1,420 | 1,550 | | 2,007 | True | 1,840 | 2,130 | | 2,007 | True | 935 | 1,225 | | 2,007 | True | 1,450 | 1,735 | # drop columns with -C, by name or position ! head flights . csv \\ | csvcut - c 10 - 15 \\ | csvcut - C TailNum , CRSElapsedTime \\ | csvlook | FlightNum | ActualElapsedTime | AirTime | ArrDelay | | --------- | ----------------- | ------- | -------- | | 2,891 | 69 | 54 | 1 | | 462 | 85 | 74 | 8 | | 1,229 | 88 | 73 | 34 | | 1,355 | 86 | 75 | 26 | | 2,278 | 86 | 74 | -3 | | 2,386 | 83 | 74 | 3 | | 409 | 101 | 89 | 47 | | 1,131 | 99 | 86 | -2 | | 1,212 | 102 | 90 | 44 | # Combinations work too ! head flights . csv \\ | csvcut - c 1 - 2 , FlightNum , ArrDelay \\ | csvlook | Year | Month | FlightNum | ArrDelay | | ----- | ----- | --------- | -------- | | 2,007 | True | 2,891 | 1 | | 2,007 | True | 462 | 8 | | 2,007 | True | 1,229 | 34 | | 2,007 | True | 1,355 | 26 | | 2,007 | True | 2,278 | -3 | | 2,007 | True | 2,386 | 3 | | 2,007 | True | 409 | 47 | | 2,007 | True | 1,131 | -2 | | 2,007 | True | 1,212 | 44 | csvgrep \u00b6 ** Subset Rows ** Options -c COLUMNS, --columns COLUMNS A comma separated list of column indices or names to be searched. -m PATTERN, --match PATTERN The string to search for. -r REGEX, --regex REGEX If specified, must be followed by a regular expression which will be tested against the specified columns. -f MATCHFILE, --file MATCHFILE If specified, must be the path to a file. For each tested row, if any line in the file (stripped of line separators) is an exact match for the cell value, the row will pass. -i, --invert-match If specified, select non-matching instead of matching rows. ! csvcut - c Origin flights . csv | head Origin SMF SMF SMF SMF SMF SMF SMF SMF SMF # Filter for rows where origin is San Francisco ! csvgrep - c Origin - m SFO flights . csv \\ | head - n 5 \\ | csvcut - c 17 - 22 \\ | csvlook | Origin | Dest | Distance | TaxiIn | TaxiOut | Cancelled | | ------ | ---- | -------- | ------ | ------- | --------- | | SFO | PHX | 651 | 4 | 4 | False | | SFO | PHX | 651 | 7 | 21 | False | | SFO | PHX | 651 | 7 | 13 | False | | SFO | PHX | 651 | 5 | 16 | False | # Filter for rows where Origin is not SFO ! csvgrep - ic Origin - m SFO flights . csv \\ | head - n 5 \\ | csvcut - c 17 - 22 \\ | csvlook | Origin | Dest | Distance | TaxiIn | TaxiOut | Cancelled | | ------ | ---- | -------- | ------ | ------- | --------- | | SMF | ONT | 389 | 4 | 11 | False | | SMF | PDX | 479 | 5 | 6 | False | | SMF | PDX | 479 | 6 | 9 | False | | SMF | PDX | 479 | 3 | 8 | False | csvsort \u00b6 Options -c COLUMNS, --columns COLUMNS A comma separated list of column indices or names to sort by. Defaults to all columns. -r, --reverse Sort in descending order. --no-inference Disable type inference when parsing the input. # ascending ! head - n 1000 flights . csv \\ | csvsort - c ArrDelay \\ | csvcut - c 14 - 20 \\ | head \\ | csvlook | AirTime | ArrDelay | DepDelay | Origin | Dest | Distance | TaxiIn | | ------- | -------- | -------- | ------ | ---- | -------- | ------ | | 230 | -51 | -6 | CLE | LAS | 1,825 | 4 | | 202 | -39 | 0 | BNA | LAS | 1,588 | 4 | | 290 | -39 | 5 | BWI | SAN | 2,295 | 3 | | 208 | -37 | -3 | BNA | LAS | 1,588 | 3 | | 227 | -37 | 2 | BNA | LAX | 1,797 | 8 | | 219 | -37 | 3 | BNA | SAN | 1,751 | 2 | | 251 | -37 | -1 | BUF | LAS | 1,987 | 5 | | 224 | -36 | -4 | BNA | ONT | 1,751 | 5 | | 301 | -36 | -1 | BWI | SAN | 2,295 | 3 | # descending ! head - n 1000 flights . csv \\ | csvsort - rc ArrDelay \\ | csvcut - c 14 - 20 \\ | head \\ | csvlook | AirTime | ArrDelay | DepDelay | Origin | Dest | Distance | TaxiIn | | ------- | -------- | -------- | ------ | ---- | -------- | ------ | | | | | SNA | LAS | 226 | 0 | | | | | AUS | DAL | 189 | 0 | | | | | DAL | AUS | 189 | 0 | | | | | DAL | HOU | 239 | 0 | | | | | DAL | HOU | 239 | 0 | | | | | DAL | HOU | 239 | 0 | | 51 | 219 | 229 | CMH | MDW | 284 | 4 | | 50 | 165 | 162 | SNA | LAS | 226 | 4 | | 49 | 133 | 138 | BWI | ALB | 288 | 3 | csvstat \u00b6 Outputs the statistical summary of all/particular columns Default action is to provide a lot of information (you can choose either all or one. Tells you if there are nulls ina a column (will hog the memory - maybe run inside Docker?) Gives you info on the dtypes Frequency table for free! !!!---painfully slow---!!! (I would crontab this to run overnight) Options --max Only output max. --min Only output min. --sum Only output sum. --mean Only output mean. --median Only output median. --stdev Only output standard deviation. --nulls Only output whether column contains nulls. --unique Only output unique values. --freq Only output frequent values. --len Only output max value length. --count Only output row count ! csvstat - c src_bytes , dst_bytes kddcup . data Killed # Returns top 5 by default ! csvstat - c Origin -- freq flights . csv Killed csvstack \u00b6 Options -g GROUPS, --groups GROUPS A comma-seperated list of values to add as \"grouping factors\", one for each CSV being stacked. These will be added to the stacked CSV as a new column. You may specify a name for the grouping column using the -n flag. -n GROUP_NAME, --group-name GROUP_NAME A name for the grouping column, e.g. \"year\". Only used when also specifying -g. --filenames Use the filename of each input file as its grouping value. When specified, -g will be ignored. # Create files (only one should have the header) ! head flights . csv > flights_01 . csv ! tail flights . csv > flights_02 . csv ! csvstack flights_01 . csv flights_02 . csv | csvcut - c 15 - 20 | csvlook | ArrDelay | DepDelay | Origin | Dest | Distance | TaxiIn | | -------- | -------- | ------ | ---- | -------- | ------ | | 1 | 7 | SMF | ONT | 389 | 4 | | 8 | 13 | SMF | PDX | 479 | 5 | | 34 | 36 | SMF | PDX | 479 | 6 | | 26 | 30 | SMF | PDX | 479 | 3 | | -3 | 1 | SMF | PDX | 479 | 3 | | 3 | 10 | SMF | PDX | 479 | 2 | | 47 | 56 | SMF | PHX | 647 | 5 | | -2 | 9 | SMF | PHX | 647 | 4 | | 44 | 47 | SMF | PHX | 647 | 5 | | 24 | 45 | ATL | SFO | 2,139 | 10 | | 1 | 6 | SLC | CVG | 1,449 | 8 | | -9 | 1 | CVG | SLC | 1,449 | 6 | | 9 | -1 | CVG | ATL | 373 | 13 | | 13 | -7 | MCO | ATL | 403 | 14 | | 5 | 11 | ATL | SLC | 1,589 | 6 | | 15 | -1 | LAX | ATL | 1,946 | 14 | | 36 | 38 | DFW | ATL | 732 | 11 | | 33 | 24 | ATL | MCO | 403 | 10 | # globbing allowed ! csvstack flights_ *. csv | csvcut - c 15 - 20 | head | csvlook | ArrDelay | DepDelay | Origin | Dest | Distance | TaxiIn | | -------- | -------- | ------ | ---- | -------- | ------ | | 1 | 7 | SMF | ONT | 389 | 4 | | 8 | 13 | SMF | PDX | 479 | 5 | | 34 | 36 | SMF | PDX | 479 | 6 | | 26 | 30 | SMF | PDX | 479 | 3 | | -3 | 1 | SMF | PDX | 479 | 3 | | 3 | 10 | SMF | PDX | 479 | 2 | | 47 | 56 | SMF | PHX | 647 | 5 | | -2 | 9 | SMF | PHX | 647 | 4 | | 44 | 47 | SMF | PHX | 647 | 5 | # Keep track of things ! csvstack - g head , tail - n source_dataframe flights_0 * | csvcut - c 1 , 15 - 20 | head | csvlook | source_dataframe | AirTime | ArrDelay | DepDelay | Origin | Dest | Distance | | ---------------- | ------- | -------- | -------- | ------ | ---- | -------- | | head | 54 | 1 | 7 | SMF | ONT | 389 | | head | 74 | 8 | 13 | SMF | PDX | 479 | | head | 73 | 34 | 36 | SMF | PDX | 479 | | head | 75 | 26 | 30 | SMF | PDX | 479 | | head | 74 | -3 | 1 | SMF | PDX | 479 | | head | 74 | 3 | 10 | SMF | PDX | 479 | | head | 89 | 47 | 56 | SMF | PHX | 647 | | head | 86 | -2 | 9 | SMF | PHX | 647 | | head | 90 | 44 | 47 | SMF | PHX | 647 | ! csvstack -- filenames flights_0 * | csvcut - c 1 , 15 - 20 | head | csvlook | group | AirTime | ArrDelay | DepDelay | Origin | Dest | Distance | | -------------- | ------- | -------- | -------- | ------ | ---- | -------- | | flights_01.csv | 54 | 1 | 7 | SMF | ONT | 389 | | flights_01.csv | 74 | 8 | 13 | SMF | PDX | 479 | | flights_01.csv | 73 | 34 | 36 | SMF | PDX | 479 | | flights_01.csv | 75 | 26 | 30 | SMF | PDX | 479 | | flights_01.csv | 74 | -3 | 1 | SMF | PDX | 479 | | flights_01.csv | 74 | 3 | 10 | SMF | PDX | 479 | | flights_01.csv | 89 | 47 | 56 | SMF | PHX | 647 | | flights_01.csv | 86 | -2 | 9 | SMF | PHX | 647 | | flights_01.csv | 90 | 44 | 47 | SMF | PHX | 647 | csvsql \u00b6 Generate SQL CREATE TABLE statements for one or more CSV files ( very useful! ) Execute these statements directly on a database Execute one or more SQL queries. OPTIONS -i {access,sybase,sqlite,informix,firebird,mysql,oracle,maxdb,postgresql,mssql}, --dialect {access,sybase,sqlite,informix,firebird,mysql,oracle,maxdb,postgresql,mssql} Dialect of SQL to generate. Only valid when --db is not specified. --db CONNECTION_STRING If present, a sqlalchemy connection string to use to directly execute generated SQL on a database. --query QUERY Execute one or more SQL queries delimited by \";\" and output the result of the last query as CSV. --insert In addition to creating the table, also insert the data into the table. Only valid when --db is specified. --tables TABLE_NAMES Specify one or more names for the tables to be created. If omitted, the filename (minus extension) or \"stdin\" will be used. --no-constraints Generate a schema without length limits or null checks. Useful when sampling big tables. --no-create Skip creating a table. Only valid when --insert is specified. --blanks Do not coerce empty strings to NULL values. --no-inference Disable type inference when parsing the input. --db-schema DB_SCHEMA Optional name of database schema to create table(s) in. ! head flights . csv | csvsql - i sqlite . /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ [Errno 21] Is a directory: '.' ! head - n 10000 flights . csv | csvsql - i postgresql /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ CREATE TABLE stdin ( \"Year\" DECIMAL NOT NULL, \"Month\" BOOLEAN NOT NULL, \"DayofMonth\" DECIMAL NOT NULL, \"DayOfWeek\" DECIMAL NOT NULL, \"DepTime\" DECIMAL, \"CRSDepTime\" DECIMAL NOT NULL, \"ArrTime\" DECIMAL, \"CRSArrTime\" DECIMAL NOT NULL, \"UniqueCarrier\" VARCHAR(2) NOT NULL, \"FlightNum\" DECIMAL NOT NULL, \"TailNum\" VARCHAR(6) NOT NULL, \"ActualElapsedTime\" DECIMAL, \"CRSElapsedTime\" DECIMAL NOT NULL, \"AirTime\" DECIMAL, \"ArrDelay\" DECIMAL, \"DepDelay\" DECIMAL, \"Origin\" VARCHAR(3) NOT NULL, \"Dest\" VARCHAR(3) NOT NULL, \"Distance\" DECIMAL NOT NULL, \"TaxiIn\" DECIMAL NOT NULL, \"TaxiOut\" DECIMAL NOT NULL, \"Cancelled\" BOOLEAN NOT NULL, \"CancellationCode\" VARCHAR(1), \"Diverted\" BOOLEAN NOT NULL, \"CarrierDelay\" DECIMAL NOT NULL, \"WeatherDelay\" DECIMAL NOT NULL, \"NASDelay\" DECIMAL NOT NULL, \"SecurityDelay\" DECIMAL NOT NULL, \"LateAircraftDelay\" DECIMAL NOT NULL ); ! head - n 1000 kddcup . data \\ | csvsql -- query \"\"\"SELECT distinct(interaction_type), count(*) \\ FROM kdd \\ WHERE src_bytes > 1000 \\ GROUP BY 1 \\ ORDER BY 2 DESC\"\"\" \\ | csvlook /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_2\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_3\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_4\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_5\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_6\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_7\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_8\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_9\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_10\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_11\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_12\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_13\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_14\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_15\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_16\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"1\" already exists in Table. Column will be renamed to \"1_2\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"1\" already exists in Table. Column will be renamed to \"1_3\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_2\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_3\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_4\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_5\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_6\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_17\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_18\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_7\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_8\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_9\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_10\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_11\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_12\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_13\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_14\". (sqlite3.OperationalError) no such table: kdd [SQL: 'SELECT distinct(interaction_type), count(*) FROM kdd WHERE src_bytes > 1000 GROUP BY 1 ORDER BY 2 DESC'] (Background on this error at: http://sqlalche.me/e/e3q8) # Create a table and import data from the CSV directly into Postgres: # to be tested ! createdb test ! csvsql -- db postgresql : /// test -- table fy09 -- insert examples / realdata / FY09_EDU_Recipients_by_State . csv # Create tables for an entire folder of CSVs and import data from those files directly into Postgres: # to be tested ! createdb test ! csvsql -- db postgresql : /// test -- insert examples /*. csv","title":"CSVKit"},{"location":"02_csvkit/#csvkit","text":"csvkit Data csvlook csvcut csvgrep csvsort csvstat csvstack csvsql","title":"csvkit"},{"location":"02_csvkit/#data","text":"import os os . chdir ( \"/home/data\" ) ls - l -- block - size = MB total 1446MB -rw------- 1 root root 703MB Mar 4 01:05 flights.csv -rw-r--r-- 1 root root 1MB Mar 4 01:26 flights_1k.csv -rw-r--r-- 1 root root 1MB Mar 4 01:01 get-csvs.sh -rw-r--r-- 1 root root 1MB Mar 4 01:13 kddcup-names -rw------- 1 root root 743MB Mar 4 01:00 kddcup.data","title":"Data"},{"location":"02_csvkit/#csvlook","text":"! head flights . csv | cut - d , - f5 - 12 | csvlook | DepTime | CRSDepTime | ArrTime | CRSArrTime | UniqueCarrier | FlightNum | TailNum | ActualElapsedTime | | ------- | ---------- | ------- | ---------- | ------------- | --------- | ------- | ----------------- | | 1,232 | 1,225 | 1,341 | 1,340 | WN | 2,891 | N351 | 69 | | 1,918 | 1,905 | 2,043 | 2,035 | WN | 462 | N370 | 85 | | 2,206 | 2,130 | 2,334 | 2,300 | WN | 1,229 | N685 | 88 | | 1,230 | 1,200 | 1,356 | 1,330 | WN | 1,355 | N364 | 86 | | 831 | 830 | 957 | 1,000 | WN | 2,278 | N480 | 86 | | 1,430 | 1,420 | 1,553 | 1,550 | WN | 2,386 | N611SW | 83 | | 1,936 | 1,840 | 2,217 | 2,130 | WN | 409 | N482 | 101 | | 944 | 935 | 1,223 | 1,225 | WN | 1,131 | N749SW | 99 | | 1,537 | 1,450 | 1,819 | 1,735 | WN | 1,212 | N451 | 102 | # Again, but with an index ! head flights . csv | cut - d , - f5 - 12 | csvlook - l | line_numbers | DepTime | CRSDepTime | ArrTime | CRSArrTime | UniqueCarrier | FlightNum | TailNum | ActualElapsedTime | | ------------ | ------- | ---------- | ------- | ---------- | ------------- | --------- | ------- | ----------------- | | 1 | 1,232 | 1,225 | 1,341 | 1,340 | WN | 2,891 | N351 | 69 | | 2 | 1,918 | 1,905 | 2,043 | 2,035 | WN | 462 | N370 | 85 | | 3 | 2,206 | 2,130 | 2,334 | 2,300 | WN | 1,229 | N685 | 88 | | 4 | 1,230 | 1,200 | 1,356 | 1,330 | WN | 1,355 | N364 | 86 | | 5 | 831 | 830 | 957 | 1,000 | WN | 2,278 | N480 | 86 | | 6 | 1,430 | 1,420 | 1,553 | 1,550 | WN | 2,386 | N611SW | 83 | | 7 | 1,936 | 1,840 | 2,217 | 2,130 | WN | 409 | N482 | 101 | | 8 | 944 | 935 | 1,223 | 1,225 | WN | 1,131 | N749SW | 99 | | 9 | 1,537 | 1,450 | 1,819 | 1,735 | WN | 1,212 | N451 | 102 |","title":"csvlook"},{"location":"02_csvkit/#csvcut","text":"For column subsetting Options -n, --names Display column names and indices from the input CSV and exit. -c COLUMNS, --columns COLUMNS A comma separated list of column indices or names to be extracted. Defaults to all columns. -C NOT_COLUMNS, --not-columns NOT_COLUMNS A comma separated list of column indices or names to be excluded. Defaults to no columns. -x, --delete-empty-rows After cutting, delete rows which are completely empty. # Get column names and positions ! csvcut - n flights . csv | head 1: Year 2: Month 3: DayofMonth 4: DayOfWeek 5: DepTime 6: CRSDepTime 7: ArrTime 8: CRSArrTime 9: UniqueCarrier 10: FlightNum # subset columns by name with -c ! head flights . csv | csvcut - c FlightNum , UniqueCarrier | csvlook | FlightNum | UniqueCarrier | | --------- | ------------- | | 2,891 | WN | | 462 | WN | | 1,229 | WN | | 1,355 | WN | | 2,278 | WN | | 2,386 | WN | | 409 | WN | | 1,131 | WN | | 1,212 | WN | # or by positions ! head flights . csv | csvcut - c 1 - 2 , 6 , 8 | csvlook | Year | Month | CRSDepTime | CRSArrTime | | ----- | ----- | ---------- | ---------- | | 2,007 | True | 1,225 | 1,340 | | 2,007 | True | 1,905 | 2,035 | | 2,007 | True | 2,130 | 2,300 | | 2,007 | True | 1,200 | 1,330 | | 2,007 | True | 830 | 1,000 | | 2,007 | True | 1,420 | 1,550 | | 2,007 | True | 1,840 | 2,130 | | 2,007 | True | 935 | 1,225 | | 2,007 | True | 1,450 | 1,735 | # drop columns with -C, by name or position ! head flights . csv \\ | csvcut - c 10 - 15 \\ | csvcut - C TailNum , CRSElapsedTime \\ | csvlook | FlightNum | ActualElapsedTime | AirTime | ArrDelay | | --------- | ----------------- | ------- | -------- | | 2,891 | 69 | 54 | 1 | | 462 | 85 | 74 | 8 | | 1,229 | 88 | 73 | 34 | | 1,355 | 86 | 75 | 26 | | 2,278 | 86 | 74 | -3 | | 2,386 | 83 | 74 | 3 | | 409 | 101 | 89 | 47 | | 1,131 | 99 | 86 | -2 | | 1,212 | 102 | 90 | 44 | # Combinations work too ! head flights . csv \\ | csvcut - c 1 - 2 , FlightNum , ArrDelay \\ | csvlook | Year | Month | FlightNum | ArrDelay | | ----- | ----- | --------- | -------- | | 2,007 | True | 2,891 | 1 | | 2,007 | True | 462 | 8 | | 2,007 | True | 1,229 | 34 | | 2,007 | True | 1,355 | 26 | | 2,007 | True | 2,278 | -3 | | 2,007 | True | 2,386 | 3 | | 2,007 | True | 409 | 47 | | 2,007 | True | 1,131 | -2 | | 2,007 | True | 1,212 | 44 |","title":"csvcut"},{"location":"02_csvkit/#csvgrep","text":"** Subset Rows ** Options -c COLUMNS, --columns COLUMNS A comma separated list of column indices or names to be searched. -m PATTERN, --match PATTERN The string to search for. -r REGEX, --regex REGEX If specified, must be followed by a regular expression which will be tested against the specified columns. -f MATCHFILE, --file MATCHFILE If specified, must be the path to a file. For each tested row, if any line in the file (stripped of line separators) is an exact match for the cell value, the row will pass. -i, --invert-match If specified, select non-matching instead of matching rows. ! csvcut - c Origin flights . csv | head Origin SMF SMF SMF SMF SMF SMF SMF SMF SMF # Filter for rows where origin is San Francisco ! csvgrep - c Origin - m SFO flights . csv \\ | head - n 5 \\ | csvcut - c 17 - 22 \\ | csvlook | Origin | Dest | Distance | TaxiIn | TaxiOut | Cancelled | | ------ | ---- | -------- | ------ | ------- | --------- | | SFO | PHX | 651 | 4 | 4 | False | | SFO | PHX | 651 | 7 | 21 | False | | SFO | PHX | 651 | 7 | 13 | False | | SFO | PHX | 651 | 5 | 16 | False | # Filter for rows where Origin is not SFO ! csvgrep - ic Origin - m SFO flights . csv \\ | head - n 5 \\ | csvcut - c 17 - 22 \\ | csvlook | Origin | Dest | Distance | TaxiIn | TaxiOut | Cancelled | | ------ | ---- | -------- | ------ | ------- | --------- | | SMF | ONT | 389 | 4 | 11 | False | | SMF | PDX | 479 | 5 | 6 | False | | SMF | PDX | 479 | 6 | 9 | False | | SMF | PDX | 479 | 3 | 8 | False |","title":"csvgrep"},{"location":"02_csvkit/#csvsort","text":"Options -c COLUMNS, --columns COLUMNS A comma separated list of column indices or names to sort by. Defaults to all columns. -r, --reverse Sort in descending order. --no-inference Disable type inference when parsing the input. # ascending ! head - n 1000 flights . csv \\ | csvsort - c ArrDelay \\ | csvcut - c 14 - 20 \\ | head \\ | csvlook | AirTime | ArrDelay | DepDelay | Origin | Dest | Distance | TaxiIn | | ------- | -------- | -------- | ------ | ---- | -------- | ------ | | 230 | -51 | -6 | CLE | LAS | 1,825 | 4 | | 202 | -39 | 0 | BNA | LAS | 1,588 | 4 | | 290 | -39 | 5 | BWI | SAN | 2,295 | 3 | | 208 | -37 | -3 | BNA | LAS | 1,588 | 3 | | 227 | -37 | 2 | BNA | LAX | 1,797 | 8 | | 219 | -37 | 3 | BNA | SAN | 1,751 | 2 | | 251 | -37 | -1 | BUF | LAS | 1,987 | 5 | | 224 | -36 | -4 | BNA | ONT | 1,751 | 5 | | 301 | -36 | -1 | BWI | SAN | 2,295 | 3 | # descending ! head - n 1000 flights . csv \\ | csvsort - rc ArrDelay \\ | csvcut - c 14 - 20 \\ | head \\ | csvlook | AirTime | ArrDelay | DepDelay | Origin | Dest | Distance | TaxiIn | | ------- | -------- | -------- | ------ | ---- | -------- | ------ | | | | | SNA | LAS | 226 | 0 | | | | | AUS | DAL | 189 | 0 | | | | | DAL | AUS | 189 | 0 | | | | | DAL | HOU | 239 | 0 | | | | | DAL | HOU | 239 | 0 | | | | | DAL | HOU | 239 | 0 | | 51 | 219 | 229 | CMH | MDW | 284 | 4 | | 50 | 165 | 162 | SNA | LAS | 226 | 4 | | 49 | 133 | 138 | BWI | ALB | 288 | 3 |","title":"csvsort"},{"location":"02_csvkit/#csvstat","text":"Outputs the statistical summary of all/particular columns Default action is to provide a lot of information (you can choose either all or one. Tells you if there are nulls ina a column (will hog the memory - maybe run inside Docker?) Gives you info on the dtypes Frequency table for free! !!!---painfully slow---!!! (I would crontab this to run overnight) Options --max Only output max. --min Only output min. --sum Only output sum. --mean Only output mean. --median Only output median. --stdev Only output standard deviation. --nulls Only output whether column contains nulls. --unique Only output unique values. --freq Only output frequent values. --len Only output max value length. --count Only output row count ! csvstat - c src_bytes , dst_bytes kddcup . data Killed # Returns top 5 by default ! csvstat - c Origin -- freq flights . csv Killed","title":"csvstat"},{"location":"02_csvkit/#csvstack","text":"Options -g GROUPS, --groups GROUPS A comma-seperated list of values to add as \"grouping factors\", one for each CSV being stacked. These will be added to the stacked CSV as a new column. You may specify a name for the grouping column using the -n flag. -n GROUP_NAME, --group-name GROUP_NAME A name for the grouping column, e.g. \"year\". Only used when also specifying -g. --filenames Use the filename of each input file as its grouping value. When specified, -g will be ignored. # Create files (only one should have the header) ! head flights . csv > flights_01 . csv ! tail flights . csv > flights_02 . csv ! csvstack flights_01 . csv flights_02 . csv | csvcut - c 15 - 20 | csvlook | ArrDelay | DepDelay | Origin | Dest | Distance | TaxiIn | | -------- | -------- | ------ | ---- | -------- | ------ | | 1 | 7 | SMF | ONT | 389 | 4 | | 8 | 13 | SMF | PDX | 479 | 5 | | 34 | 36 | SMF | PDX | 479 | 6 | | 26 | 30 | SMF | PDX | 479 | 3 | | -3 | 1 | SMF | PDX | 479 | 3 | | 3 | 10 | SMF | PDX | 479 | 2 | | 47 | 56 | SMF | PHX | 647 | 5 | | -2 | 9 | SMF | PHX | 647 | 4 | | 44 | 47 | SMF | PHX | 647 | 5 | | 24 | 45 | ATL | SFO | 2,139 | 10 | | 1 | 6 | SLC | CVG | 1,449 | 8 | | -9 | 1 | CVG | SLC | 1,449 | 6 | | 9 | -1 | CVG | ATL | 373 | 13 | | 13 | -7 | MCO | ATL | 403 | 14 | | 5 | 11 | ATL | SLC | 1,589 | 6 | | 15 | -1 | LAX | ATL | 1,946 | 14 | | 36 | 38 | DFW | ATL | 732 | 11 | | 33 | 24 | ATL | MCO | 403 | 10 | # globbing allowed ! csvstack flights_ *. csv | csvcut - c 15 - 20 | head | csvlook | ArrDelay | DepDelay | Origin | Dest | Distance | TaxiIn | | -------- | -------- | ------ | ---- | -------- | ------ | | 1 | 7 | SMF | ONT | 389 | 4 | | 8 | 13 | SMF | PDX | 479 | 5 | | 34 | 36 | SMF | PDX | 479 | 6 | | 26 | 30 | SMF | PDX | 479 | 3 | | -3 | 1 | SMF | PDX | 479 | 3 | | 3 | 10 | SMF | PDX | 479 | 2 | | 47 | 56 | SMF | PHX | 647 | 5 | | -2 | 9 | SMF | PHX | 647 | 4 | | 44 | 47 | SMF | PHX | 647 | 5 | # Keep track of things ! csvstack - g head , tail - n source_dataframe flights_0 * | csvcut - c 1 , 15 - 20 | head | csvlook | source_dataframe | AirTime | ArrDelay | DepDelay | Origin | Dest | Distance | | ---------------- | ------- | -------- | -------- | ------ | ---- | -------- | | head | 54 | 1 | 7 | SMF | ONT | 389 | | head | 74 | 8 | 13 | SMF | PDX | 479 | | head | 73 | 34 | 36 | SMF | PDX | 479 | | head | 75 | 26 | 30 | SMF | PDX | 479 | | head | 74 | -3 | 1 | SMF | PDX | 479 | | head | 74 | 3 | 10 | SMF | PDX | 479 | | head | 89 | 47 | 56 | SMF | PHX | 647 | | head | 86 | -2 | 9 | SMF | PHX | 647 | | head | 90 | 44 | 47 | SMF | PHX | 647 | ! csvstack -- filenames flights_0 * | csvcut - c 1 , 15 - 20 | head | csvlook | group | AirTime | ArrDelay | DepDelay | Origin | Dest | Distance | | -------------- | ------- | -------- | -------- | ------ | ---- | -------- | | flights_01.csv | 54 | 1 | 7 | SMF | ONT | 389 | | flights_01.csv | 74 | 8 | 13 | SMF | PDX | 479 | | flights_01.csv | 73 | 34 | 36 | SMF | PDX | 479 | | flights_01.csv | 75 | 26 | 30 | SMF | PDX | 479 | | flights_01.csv | 74 | -3 | 1 | SMF | PDX | 479 | | flights_01.csv | 74 | 3 | 10 | SMF | PDX | 479 | | flights_01.csv | 89 | 47 | 56 | SMF | PHX | 647 | | flights_01.csv | 86 | -2 | 9 | SMF | PHX | 647 | | flights_01.csv | 90 | 44 | 47 | SMF | PHX | 647 |","title":"csvstack"},{"location":"02_csvkit/#csvsql","text":"Generate SQL CREATE TABLE statements for one or more CSV files ( very useful! ) Execute these statements directly on a database Execute one or more SQL queries. OPTIONS -i {access,sybase,sqlite,informix,firebird,mysql,oracle,maxdb,postgresql,mssql}, --dialect {access,sybase,sqlite,informix,firebird,mysql,oracle,maxdb,postgresql,mssql} Dialect of SQL to generate. Only valid when --db is not specified. --db CONNECTION_STRING If present, a sqlalchemy connection string to use to directly execute generated SQL on a database. --query QUERY Execute one or more SQL queries delimited by \";\" and output the result of the last query as CSV. --insert In addition to creating the table, also insert the data into the table. Only valid when --db is specified. --tables TABLE_NAMES Specify one or more names for the tables to be created. If omitted, the filename (minus extension) or \"stdin\" will be used. --no-constraints Generate a schema without length limits or null checks. Useful when sampling big tables. --no-create Skip creating a table. Only valid when --insert is specified. --blanks Do not coerce empty strings to NULL values. --no-inference Disable type inference when parsing the input. --db-schema DB_SCHEMA Optional name of database schema to create table(s) in. ! head flights . csv | csvsql - i sqlite . /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ [Errno 21] Is a directory: '.' ! head - n 10000 flights . csv | csvsql - i postgresql /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ CREATE TABLE stdin ( \"Year\" DECIMAL NOT NULL, \"Month\" BOOLEAN NOT NULL, \"DayofMonth\" DECIMAL NOT NULL, \"DayOfWeek\" DECIMAL NOT NULL, \"DepTime\" DECIMAL, \"CRSDepTime\" DECIMAL NOT NULL, \"ArrTime\" DECIMAL, \"CRSArrTime\" DECIMAL NOT NULL, \"UniqueCarrier\" VARCHAR(2) NOT NULL, \"FlightNum\" DECIMAL NOT NULL, \"TailNum\" VARCHAR(6) NOT NULL, \"ActualElapsedTime\" DECIMAL, \"CRSElapsedTime\" DECIMAL NOT NULL, \"AirTime\" DECIMAL, \"ArrDelay\" DECIMAL, \"DepDelay\" DECIMAL, \"Origin\" VARCHAR(3) NOT NULL, \"Dest\" VARCHAR(3) NOT NULL, \"Distance\" DECIMAL NOT NULL, \"TaxiIn\" DECIMAL NOT NULL, \"TaxiOut\" DECIMAL NOT NULL, \"Cancelled\" BOOLEAN NOT NULL, \"CancellationCode\" VARCHAR(1), \"Diverted\" BOOLEAN NOT NULL, \"CarrierDelay\" DECIMAL NOT NULL, \"WeatherDelay\" DECIMAL NOT NULL, \"NASDelay\" DECIMAL NOT NULL, \"SecurityDelay\" DECIMAL NOT NULL, \"LateAircraftDelay\" DECIMAL NOT NULL ); ! head - n 1000 kddcup . data \\ | csvsql -- query \"\"\"SELECT distinct(interaction_type), count(*) \\ FROM kdd \\ WHERE src_bytes > 1000 \\ GROUP BY 1 \\ ORDER BY 2 DESC\"\"\" \\ | csvlook /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /miniconda/envs/ds-py3/lib/python3.6/site-packages/mpl_toolkits: missing __init__ /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_2\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_3\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_4\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_5\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_6\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_7\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_8\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_9\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_10\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_11\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_12\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_13\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_14\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_15\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_16\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"1\" already exists in Table. Column will be renamed to \"1_2\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"1\" already exists in Table. Column will be renamed to \"1_3\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_2\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_3\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_4\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_5\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_6\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_17\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0\" already exists in Table. Column will be renamed to \"0_18\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_7\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_8\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_9\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_10\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_11\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_12\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_13\". /miniconda/envs/ds-py3/lib/python3.6/site-packages/agate/utils.py:291: DuplicateColumnWarning: Column name \"0.00\" already exists in Table. Column will be renamed to \"0.00_14\". (sqlite3.OperationalError) no such table: kdd [SQL: 'SELECT distinct(interaction_type), count(*) FROM kdd WHERE src_bytes > 1000 GROUP BY 1 ORDER BY 2 DESC'] (Background on this error at: http://sqlalche.me/e/e3q8) # Create a table and import data from the CSV directly into Postgres: # to be tested ! createdb test ! csvsql -- db postgresql : /// test -- table fy09 -- insert examples / realdata / FY09_EDU_Recipients_by_State . csv # Create tables for an entire folder of CSVs and import data from those files directly into Postgres: # to be tested ! createdb test ! csvsql -- db postgresql : /// test -- insert examples /*. csv","title":"csvsql"},{"location":"03_csvtk/","text":"csvtk \u00b6 A cross-platform, efficient, practical and pretty CSV/TSV toolkit in Golang docs Usage Tutorial csvtk Introduction Data See column names See first few rows pretty sample stats cut uniq freq plot Histogram Boxplots ScatterPlot grep filter filter2 rename stats2 mutate sort Introduction \u00b6 ! csvtk A cross-platform, efficient and practical CSV/TSV toolkit Version: 0.13.0 Author: Wei Shen <shenwei356@gmail.com> Documents : http://shenwei356.github.io/csvtk Source code: https://github.com/shenwei356/csvtk Attention: 1. The CSV parser requires all the lines have same number of fields/columns. Even lines with spaces will cause error. 2. By default, csvtk thinks your files have header row, if not, switch flag \"-H\" on. 3. Column names better be unique. 4. By default, lines starting with \"#\" will be ignored, if the header row starts with \"#\", please assign flag \"-C\" another rare symbol, e.g. '$'. 5. By default, csvtk handles CSV files, use flag \"-t\" for tab-delimited files. 6. If \" exists in tab-delimited files, use flag \"-l\". Environment variables for frequently used global flags - \"CSVTK_T\" for flag \"-t/--tabs\" - \"CSVTK_H\" for flag \"-H/--no-header-row\" Usage: csvtk [command] Available Commands: collapse collapse one field with selected fields as keys concat concatenate CSV/TSV files by rows csv2md convert CSV to markdown format csv2tab convert CSV to tabular format cut select parts of fields filter filter rows by values of selected fields with artithmetic expression filter2 filter rows by awk-like artithmetic/string expressions freq frequencies of selected fields gather gather columns into key-value pairs genautocomplete generate shell autocompletion script grep grep data by selected fields with patterns/regular expressions head print first N records headers print headers help Help about any command inter intersection of multiple files join join multiple CSV files by selected fields mutate create new column from selected fields by regular expression mutate2 create new column from selected fields by awk-like artithmetic/string expressions plot plot common figures pretty convert CSV to readable aligned table rename rename column names rename2 rename column names by regular expression replace replace data of selected fields by regular expression sample sampling by proportion sort sort by selected fields space2tab convert space delimited format to CSV split split CSV/TSV into multiple files according to column values splitxlsx split XLSX sheet into multiple sheets according to column values stats summary of CSV file stats2 summary of selected digital fields tab2csv convert tabular format to CSV transpose transpose CSV data uniq unique data without sorting version print version information and check for update xlsx2csv convert XLSX to CSV format Flags: -c, --chunk-size int chunk size of CSV reader (default 50) -C, --comment-char string lines starting with commment-character will be ignored. if your header row starts with '#', please assign \"-C\" another rare symbol, e.g. '$' (default \"#\") -d, --delimiter string delimiting character of the input CSV file (default \",\") -h, --help help for csvtk -l, --lazy-quotes if given, a quote may appear in an unquoted field and a non-doubled quote may appear in a quoted field -H, --no-header-row specifies that the input CSV file does not have header row -j, --num-cpus int number of CPUs to use (default value depends on your computer) (default 3) -D, --out-delimiter string delimiting character of the output CSV file (default \",\") -o, --out-file string out file (\"-\" for stdout, suffix .gz for gzipped out) (default \"-\") -T, --out-tabs specifies that the output is delimited with tabs. Overrides \"-D\" -t, --tabs specifies that the input CSV file is delimited with tabs. Overrides \"-d\" and \"-D\" Use \"csvtk [command] --help\" for more information about a command. Data \u00b6 import os os . chdir ( \"/home/data\" ) ls | grep csv | xargs wc - l 7453216 flights.csv 10 flights_01.csv 10 flights_02.csv 1000 flights_1k.csv 10000001 fromPandas.csv 27 get-csvs.sh 17454264 total See column names \u00b6 ! csvtk headers fromPandas . csv # fromPandas.csv 1 C00 2 A01 3 A02 4 A03 5 C04 6 D05 7 B06 8 C07 9 C08 10 A09 See first few rows \u00b6 ! csvtk head - n 5 fromPandas . csv C00,A01,A02,A03,C04,D05,B06,C07,C08,A09 PO,Critical,0.31,-0.02,0.15,0.52,-1.24,-1.12,-1.68,-0.7 AR,Critical,1.33,-0.65,0.29,-1.31,0.32,-1.61,1.27,0.34 AR,Critical,-2.4,-0.23,0.28,0.95,0.82,-0.18,-1.73,-1.44 PO,Critical,0.16,-0.01,0.46,0.09,-0.43,-0.79,-1.5,0.87 PO,Alert,-0.34,-0.37,0.17,0.62,-1.19,1.81,0.66,0.1 pretty \u00b6 ! csvtk pretty - h ! csvtk head fromPandas . csv | csvtk pretty - r sample \u00b6 the -H switch removes the header, -p specifies proportion ! csvtk sample - H - p 0.01 fromPandas . csv | wc - l ! csvtk sample - p 0.001 fromPandas . csv | head stats \u00b6 ! csvtk stats fromPandas . csv cut \u00b6 ! csvtk cut - h # by position, ranges ! head fromPandas . csv | csvtk cut - f 2 , 3 , 5 - 7 | csvtk pretty - r # by exact name ! head fromPandas . csv | csvtk cut - f A05 , B07 , C00 , D04 | csvtk pretty - r # by fuzzy matching ! head fromPandas . csv | csvtk cut - F - f \"A0*,D01\" | csvtk pretty - r # ignoring columns by position, ranges # csvtk cut -f -3--1 for discarding column 1,2,3 ! head fromPandas . csv | csvtk cut - f - 5 -- 2 , - 10 -- 9 | csvtk pretty - r uniq \u00b6 ! csvtk uniq - h # will retain the rows corresponding to the first occurence of each value in column ! cat fromPandas . csv | csvtk uniq - f C00 ! cat fromPandas . csv | csvtk cut - f D01 | csvtk uniq - f 1 freq \u00b6 ! csvtk freq - h ! cat fromPandas . csv | csvtk freq - f C00 # sort by key ! cat fromPandas . csv | csvtk freq - f C00 - k | csvtk pretty # sort in descending order of count ! cat fromPandas . csv | csvtk freq - f C00 - n - r | csvtk pretty # combination of two variables ! cat fromPandas . csv | csvtk freq - f C00 , D01 - n - r | csvtk pretty plot \u00b6 ! csvtk plot - h Histogram \u00b6 ! cat fromPandas . csv \\ | csvtk sample - p 0.01 \\ | csvtk plot hist - f \"D04\" - o hist . png Image ( 'hist.png' , width = 400 ) Boxplots \u00b6 ! cat fromPandas . csv \\ | csvtk sample - p 0.01 \\ | csvtk plot box - g \"C00\" - f \"D04\" - o box . png Image ( 'box.png' , width = 400 ) ! cat fromPandas . csv \\ | csvtk sample - p 0.01 \\ | csvtk plot box - g \"C00\" - f \"D04\" -- horiz - o box2 . png Image ( 'box2.png' , width = 400 ) import pandas as pd import numpy as np % pylab inline df = ( pd . DataFrame ({ 'x' : range ( 5000 )}) . assign ( Y = lambda df : np . random . randn ( 5000 ) . round ( 2 )) . assign ( Z = lambda df : 2 * df [ 'x' ] + 5 ) . assign ( Grp = pd . Series ( list ( 'ABCD' )) . sample ( 5000 , replace = True ) . values )) df . to_csv ( 'line.csv' , index = False ) ! csvtk plot line line . csv - x x - y Y - o lineplot . png Image ( 'lineplot.png' ) ScatterPlot \u00b6 ! cat line . csv \\ | csvtk plot line - x x - y Y - g Grp -- scatter - o scatter . png Image ( 'scatter.png' , width = 400 ) grep \u00b6 ! csvtk grep - h ! cat fromPandas . csv | csvtk grep - f C00 - p EN | csvtk head | csvtk pretty - r # Remore rows containing missing data ! csvtk grep - F - f \"*\" - r - p \"^$\" - v filter \u00b6 ! csvtk filter - h ! cat fromPandas . csv | csvtk filter - f \"D04>3.00\" | wc - l ! cat fromPandas . csv | csvtk filter - F - f \"A*>1\" | csvtk head | csvtk pretty filter2 \u00b6 ! csvtk filter2 - h ! cat fromPandas . csv | csvtk filter2 - f '$A05>1 && $C00==\"ES\"' | csvtk head | csvtk pretty - r rename \u00b6 ! csvtk rename - h ! cat fromPandas . csv | csvtk rename - f 1 , 2 - n Lang , Msg | csvtk head | csvtk pretty - r ! csvtk rename2 - h ! head - 5 fromPandas . csv \\ | csvtk cut - f - 2 -- 1 \\ | csvtk rename2 - F - f \"*\" - p \"(.*)\" - r 'Num_$ {1} ' \\ | csvtk pretty - r stats2 \u00b6 ! csvtk stats2 - h ! cat fromPandas . csv | csvtk stats2 - F - f 'A*' mutate \u00b6 ! csvtk mutate - h ! head fromPandas . csv | csvtk mutate - f C00 - n C00_copy sort \u00b6 ! csvtk sort - h","title":"CSVTK"},{"location":"03_csvtk/#csvtk","text":"A cross-platform, efficient, practical and pretty CSV/TSV toolkit in Golang docs Usage Tutorial csvtk Introduction Data See column names See first few rows pretty sample stats cut uniq freq plot Histogram Boxplots ScatterPlot grep filter filter2 rename stats2 mutate sort","title":"csvtk"},{"location":"03_csvtk/#introduction","text":"! csvtk A cross-platform, efficient and practical CSV/TSV toolkit Version: 0.13.0 Author: Wei Shen <shenwei356@gmail.com> Documents : http://shenwei356.github.io/csvtk Source code: https://github.com/shenwei356/csvtk Attention: 1. The CSV parser requires all the lines have same number of fields/columns. Even lines with spaces will cause error. 2. By default, csvtk thinks your files have header row, if not, switch flag \"-H\" on. 3. Column names better be unique. 4. By default, lines starting with \"#\" will be ignored, if the header row starts with \"#\", please assign flag \"-C\" another rare symbol, e.g. '$'. 5. By default, csvtk handles CSV files, use flag \"-t\" for tab-delimited files. 6. If \" exists in tab-delimited files, use flag \"-l\". Environment variables for frequently used global flags - \"CSVTK_T\" for flag \"-t/--tabs\" - \"CSVTK_H\" for flag \"-H/--no-header-row\" Usage: csvtk [command] Available Commands: collapse collapse one field with selected fields as keys concat concatenate CSV/TSV files by rows csv2md convert CSV to markdown format csv2tab convert CSV to tabular format cut select parts of fields filter filter rows by values of selected fields with artithmetic expression filter2 filter rows by awk-like artithmetic/string expressions freq frequencies of selected fields gather gather columns into key-value pairs genautocomplete generate shell autocompletion script grep grep data by selected fields with patterns/regular expressions head print first N records headers print headers help Help about any command inter intersection of multiple files join join multiple CSV files by selected fields mutate create new column from selected fields by regular expression mutate2 create new column from selected fields by awk-like artithmetic/string expressions plot plot common figures pretty convert CSV to readable aligned table rename rename column names rename2 rename column names by regular expression replace replace data of selected fields by regular expression sample sampling by proportion sort sort by selected fields space2tab convert space delimited format to CSV split split CSV/TSV into multiple files according to column values splitxlsx split XLSX sheet into multiple sheets according to column values stats summary of CSV file stats2 summary of selected digital fields tab2csv convert tabular format to CSV transpose transpose CSV data uniq unique data without sorting version print version information and check for update xlsx2csv convert XLSX to CSV format Flags: -c, --chunk-size int chunk size of CSV reader (default 50) -C, --comment-char string lines starting with commment-character will be ignored. if your header row starts with '#', please assign \"-C\" another rare symbol, e.g. '$' (default \"#\") -d, --delimiter string delimiting character of the input CSV file (default \",\") -h, --help help for csvtk -l, --lazy-quotes if given, a quote may appear in an unquoted field and a non-doubled quote may appear in a quoted field -H, --no-header-row specifies that the input CSV file does not have header row -j, --num-cpus int number of CPUs to use (default value depends on your computer) (default 3) -D, --out-delimiter string delimiting character of the output CSV file (default \",\") -o, --out-file string out file (\"-\" for stdout, suffix .gz for gzipped out) (default \"-\") -T, --out-tabs specifies that the output is delimited with tabs. Overrides \"-D\" -t, --tabs specifies that the input CSV file is delimited with tabs. Overrides \"-d\" and \"-D\" Use \"csvtk [command] --help\" for more information about a command.","title":"Introduction"},{"location":"03_csvtk/#data","text":"import os os . chdir ( \"/home/data\" ) ls | grep csv | xargs wc - l 7453216 flights.csv 10 flights_01.csv 10 flights_02.csv 1000 flights_1k.csv 10000001 fromPandas.csv 27 get-csvs.sh 17454264 total","title":"Data"},{"location":"03_csvtk/#see-column-names","text":"! csvtk headers fromPandas . csv # fromPandas.csv 1 C00 2 A01 3 A02 4 A03 5 C04 6 D05 7 B06 8 C07 9 C08 10 A09","title":"See column names"},{"location":"03_csvtk/#see-first-few-rows","text":"! csvtk head - n 5 fromPandas . csv C00,A01,A02,A03,C04,D05,B06,C07,C08,A09 PO,Critical,0.31,-0.02,0.15,0.52,-1.24,-1.12,-1.68,-0.7 AR,Critical,1.33,-0.65,0.29,-1.31,0.32,-1.61,1.27,0.34 AR,Critical,-2.4,-0.23,0.28,0.95,0.82,-0.18,-1.73,-1.44 PO,Critical,0.16,-0.01,0.46,0.09,-0.43,-0.79,-1.5,0.87 PO,Alert,-0.34,-0.37,0.17,0.62,-1.19,1.81,0.66,0.1","title":"See first few rows"},{"location":"03_csvtk/#pretty","text":"! csvtk pretty - h ! csvtk head fromPandas . csv | csvtk pretty - r","title":"pretty"},{"location":"03_csvtk/#sample","text":"the -H switch removes the header, -p specifies proportion ! csvtk sample - H - p 0.01 fromPandas . csv | wc - l ! csvtk sample - p 0.001 fromPandas . csv | head","title":"sample"},{"location":"03_csvtk/#stats","text":"! csvtk stats fromPandas . csv","title":"stats"},{"location":"03_csvtk/#cut","text":"! csvtk cut - h # by position, ranges ! head fromPandas . csv | csvtk cut - f 2 , 3 , 5 - 7 | csvtk pretty - r # by exact name ! head fromPandas . csv | csvtk cut - f A05 , B07 , C00 , D04 | csvtk pretty - r # by fuzzy matching ! head fromPandas . csv | csvtk cut - F - f \"A0*,D01\" | csvtk pretty - r # ignoring columns by position, ranges # csvtk cut -f -3--1 for discarding column 1,2,3 ! head fromPandas . csv | csvtk cut - f - 5 -- 2 , - 10 -- 9 | csvtk pretty - r","title":"cut"},{"location":"03_csvtk/#uniq","text":"! csvtk uniq - h # will retain the rows corresponding to the first occurence of each value in column ! cat fromPandas . csv | csvtk uniq - f C00 ! cat fromPandas . csv | csvtk cut - f D01 | csvtk uniq - f 1","title":"uniq"},{"location":"03_csvtk/#freq","text":"! csvtk freq - h ! cat fromPandas . csv | csvtk freq - f C00 # sort by key ! cat fromPandas . csv | csvtk freq - f C00 - k | csvtk pretty # sort in descending order of count ! cat fromPandas . csv | csvtk freq - f C00 - n - r | csvtk pretty # combination of two variables ! cat fromPandas . csv | csvtk freq - f C00 , D01 - n - r | csvtk pretty","title":"freq"},{"location":"03_csvtk/#plot","text":"! csvtk plot - h","title":"plot"},{"location":"03_csvtk/#histogram","text":"! cat fromPandas . csv \\ | csvtk sample - p 0.01 \\ | csvtk plot hist - f \"D04\" - o hist . png Image ( 'hist.png' , width = 400 )","title":"Histogram"},{"location":"03_csvtk/#boxplots","text":"! cat fromPandas . csv \\ | csvtk sample - p 0.01 \\ | csvtk plot box - g \"C00\" - f \"D04\" - o box . png Image ( 'box.png' , width = 400 ) ! cat fromPandas . csv \\ | csvtk sample - p 0.01 \\ | csvtk plot box - g \"C00\" - f \"D04\" -- horiz - o box2 . png Image ( 'box2.png' , width = 400 ) import pandas as pd import numpy as np % pylab inline df = ( pd . DataFrame ({ 'x' : range ( 5000 )}) . assign ( Y = lambda df : np . random . randn ( 5000 ) . round ( 2 )) . assign ( Z = lambda df : 2 * df [ 'x' ] + 5 ) . assign ( Grp = pd . Series ( list ( 'ABCD' )) . sample ( 5000 , replace = True ) . values )) df . to_csv ( 'line.csv' , index = False ) ! csvtk plot line line . csv - x x - y Y - o lineplot . png Image ( 'lineplot.png' )","title":"Boxplots"},{"location":"03_csvtk/#scatterplot","text":"! cat line . csv \\ | csvtk plot line - x x - y Y - g Grp -- scatter - o scatter . png Image ( 'scatter.png' , width = 400 )","title":"ScatterPlot"},{"location":"03_csvtk/#grep","text":"! csvtk grep - h ! cat fromPandas . csv | csvtk grep - f C00 - p EN | csvtk head | csvtk pretty - r # Remore rows containing missing data ! csvtk grep - F - f \"*\" - r - p \"^$\" - v","title":"grep"},{"location":"03_csvtk/#filter","text":"! csvtk filter - h ! cat fromPandas . csv | csvtk filter - f \"D04>3.00\" | wc - l ! cat fromPandas . csv | csvtk filter - F - f \"A*>1\" | csvtk head | csvtk pretty","title":"filter"},{"location":"03_csvtk/#filter2","text":"! csvtk filter2 - h ! cat fromPandas . csv | csvtk filter2 - f '$A05>1 && $C00==\"ES\"' | csvtk head | csvtk pretty - r","title":"filter2"},{"location":"03_csvtk/#rename","text":"! csvtk rename - h ! cat fromPandas . csv | csvtk rename - f 1 , 2 - n Lang , Msg | csvtk head | csvtk pretty - r ! csvtk rename2 - h ! head - 5 fromPandas . csv \\ | csvtk cut - f - 2 -- 1 \\ | csvtk rename2 - F - f \"*\" - p \"(.*)\" - r 'Num_$ {1} ' \\ | csvtk pretty - r","title":"rename"},{"location":"03_csvtk/#stats2","text":"! csvtk stats2 - h ! cat fromPandas . csv | csvtk stats2 - F - f 'A*'","title":"stats2"},{"location":"03_csvtk/#mutate","text":"! csvtk mutate - h ! head fromPandas . csv | csvtk mutate - f C00 - n C00_copy","title":"mutate"},{"location":"03_csvtk/#sort","text":"! csvtk sort - h","title":"sort"},{"location":"04_xsv/","text":"xsv \u00b6 A fast CSV toolkit written in Rust. repo Commands \u00b6 cat Concatenate by row or column count Count records fixlengths Makes all records have same length flatten Show one field per line fmt Format CSV output (change field delimiter) frequency Show frequency tables headers Show header names help Show this usage message. index Create CSV index for faster access input Read CSV data with special quoting rules join Join CSV files sample Randomly sample CSV data search Search CSV data with regexes select Select columns from CSV slice Slice records from CSV sort Sort CSV data split Split CSV data into many files stats Compute basic statistics table Align CSV data into columns Install \u00b6 # run wget https://goo.gl/iZMmzX # or download the tar file from 'https://github.com/BurntSushi/xsv/releases/latest' # untar tar -xzvf xsv-0.11.0-x86_64-unknown-linux-musl.tar.gz # move the executable to the binaries folder mv xsv /usr/bin/xsv # run from anywhere ### 0. View formatted ! head - n 10 flights . csv | cut - d , - f1 - 10 | xsv table Year Month DayofMonth DayOfWeek DepTime CRSDepTime ArrTime CRSArrTime UniqueCarrier FlightNum 2007 1 1 1 1232 1225 1341 1340 WN 2891 2007 1 1 1 1918 1905 2043 2035 WN 462 2007 1 1 1 2206 2130 2334 2300 WN 1229 2007 1 1 1 1230 1200 1356 1330 WN 1355 2007 1 1 1 831 830 957 1000 WN 2278 2007 1 1 1 1430 1420 1553 1550 WN 2386 2007 1 1 1 1936 1840 2217 2130 WN 409 2007 1 1 1 944 935 1223 1225 WN 1131 2007 1 1 1 1537 1450 1819 1735 WN 1212 ### 1. Count Rows (MUCH faster than `wc -l`, uses more CPU, excludes header) ! xsv count flights . csv 7453215 ### 2. Get Column names cols = ! xsv headers flights . csv [ x . split ( ' ' )[ - 1 ] for x in cols ] ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'TailNum', 'ActualElapsedTime', 'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', 'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'CancellationCode', 'Diverted', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay'] ### 5. Subset Columns ! xsv select Month , DayofMonth , UniqueCarrier , FlightNum , Origin , Dest flights . csv | head | csvlook |--------+------------+---------------+-----------+--------+-------| | Month | DayofMonth | UniqueCarrier | FlightNum | Origin | Dest | |--------+------------+---------------+-----------+--------+-------| | 1 | 1 | WN | 2891 | SMF | ONT | | 1 | 1 | WN | 462 | SMF | PDX | | 1 | 1 | WN | 1229 | SMF | PDX | | 1 | 1 | WN | 1355 | SMF | PDX | | 1 | 1 | WN | 2278 | SMF | PDX | | 1 | 1 | WN | 2386 | SMF | PDX | | 1 | 1 | WN | 409 | SMF | PHX | | 1 | 1 | WN | 1131 | SMF | PHX | | 1 | 1 | WN | 1212 | SMF | PHX | |--------+------------+---------------+-----------+--------+-------| ### 3. Create an index ! xsv index flights . csv ! ls *. idx flights.csv.idx ### 4. Get Summary Stats (MUCH faster than `csvstat`) ! xsv stats flights . csv -- everything | \\ xsv select field , type , min , median , max , mean , stddev , mode | \\ csvlook |--------------------+---------+-------+--------+--------+-----------------------+---------------------+-------| | field | type | min | median | max | mean | stddev | mode | |--------------------+---------+-------+--------+--------+-----------------------+---------------------+-------| | Year | Integer | 2007 | 2007 | 2007 | 2007 | 0 | 2007 | | Month | Integer | 1 | 7 | 12 | 6.514876197721434 | 3.425117169491096 | 8 | | DayofMonth | Integer | 1 | 16 | 31 | 15.72588876075652 | 8.781153183533013 | 26 | | DayOfWeek | Integer | 1 | 4 | 7 | 3.9338042710427263 | 1.9922668348832397 | 1 | | DepTime | Unicode | 1 | 1255 | NA | | | NA | | CRSDepTime | Integer | 0 | 1322 | 2359 | 1330.5963490923104 | 464.70792314976785 | 600 | | ArrTime | Unicode | 1 | 1430 | NA | | | NA | | CRSArrTime | Integer | 0 | 1520 | 2400 | 1495.391906714091 | 481.59020392608437 | 1930 | | UniqueCarrier | Unicode | 9E | | YV | | | WN | | FlightNum | Integer | 1 | 1509 | 9602 | 2188.0992893670827 | 1971.9575313097869 | 16 | | TailNum | Unicode | 0 | | NHZOAL | | | 0 | | ActualElapsedTime | Unicode | 100 | 115 | NA | | | NA | | CRSElapsedTime | Unicode | -1240 | 100 | NA | | | 75 | | AirTime | Unicode | 0 | 94 | NA | | | NA | | ArrDelay | Unicode | -1 | -2 | NA | | | -5 | | DepDelay | Unicode | -1 | 1 | NA | | | 0 | | Origin | Unicode | ABE | | YUM | | | ATL | | Dest | Unicode | ABE | | YUM | | | ATL | | Distance | Integer | 11 | 569 | 4962 | 719.805789045388 | 562.3050870976227 | 337 | | TaxiIn | Integer | 0 | 5 | 545 | 6.6919844657641026 | 5.151350775045749 | 4 | | TaxiOut | Integer | 0 | 14 | 530 | 16.300146178528344 | 11.833958552477231 | 10 | | Cancelled | Integer | 0 | 0 | 1 | 0.021567605389083436 | 0.14526680208108242 | 0 | | CancellationCode | Unicode | A | | D | | | | | Diverted | Integer | 0 | 0 | 1 | 0.0023049113704622683 | 0.04795413177231553 | 0 | | CarrierDelay | Integer | 0 | 0 | 2580 | 3.865235874719842 | 20.842403385965035 | 0 | | WeatherDelay | Integer | 0 | 0 | 1429 | 0.7700903569801583 | 9.619546393361475 | 0 | | NASDelay | Integer | 0 | 0 | 1386 | 3.783702200996463 | 16.176702958825693 | 0 | | SecurityDelay | Integer | 0 | 0 | 382 | 0.023735528895919497 | 1.084995425472959 | 0 | | LateAircraftDelay | Integer | 0 | 0 | 1031 | 5.099133997878817 | 21.277529347496895 | 0 | |--------------------+---------+-------+--------+--------+-----------------------+---------------------+-------| Random Sampling \u00b6 Randomly samples CSV data uniformly using memory proportional to the size of the sample. When an index is present , this command will use random indexing if the sample size is less than 10% of the total number of records. This allows for efficient sampling such that the entire CSV file is not parsed. Allows a user to work with a CSV data set that is too big to fit into memory (for example, for use with commands like xsv frequency or xsv stats ). Usage: xsv sample [options] <sample-size> [<input>] xsv sample --help ! xsv select Month , DayofMonth , UniqueCarrier , FlightNum , Origin , Dest flights . csv | xsv sample 10 | csvlook |--------+------------+---------------+-----------+--------+-------| | Month | DayofMonth | UniqueCarrier | FlightNum | Origin | Dest | |--------+------------+---------------+-----------+--------+-------| | 1 | 21 | OH | 5315 | CVG | PIT | | 5 | 17 | WN | 629 | LAS | SMF | | 3 | 27 | WN | 962 | LAX | TUS | | 8 | 15 | FL | 44 | SFO | ATL | | 11 | 4 | AQ | 45 | ITO | HNL | | 1 | 31 | WN | 57 | DAL | HOU | | 9 | 4 | OO | 3790 | MSP | ATL | | 9 | 18 | AA | 1411 | BOS | ORD | | 2 | 18 | OO | 5766 | PDX | RDM | | 1 | 18 | OO | 3873 | SLC | MFR | |--------+------------+---------------+-----------+--------+-------| Frequency Tables \u00b6 Computes a frequency table on CSV data formatted as field,value,count The order and number of values can be tweaked with --asc and --limit respectively. memory proportional to the cardinality of each column is required. Usage: xsv frequency [options] [<input>] Options: -s, --select <arg> Select a subset of columns to compute frequencies -l, --limit <arg> Limit the frequency table to the N most common items. [default: 10] -a, --asc Sort the frequency tables in ascending order by count. [default: descending] --no-nulls Don't include NULLs in the frequency table. # For all columns ! xsv frequency flights . csv -- limit 5 | head - n 15 | csvlook |-------------+-------+----------| | field | value | count | |-------------+-------+----------| | Year | 2007 | 7453215 | | Month | 8 | 653279 | | Month | 7 | 648560 | | Month | 3 | 639209 | | Month | 5 | 631609 | | Month | 10 | 629992 | | DayofMonth | 26 | 250136 | | DayofMonth | 19 | 250092 | | DayofMonth | 12 | 249773 | | DayofMonth | 16 | 249034 | | DayofMonth | 9 | 248415 | | DayOfWeek | 1 | 1112474 | | DayOfWeek | 5 | 1101689 | | DayOfWeek | 4 | 1097738 | |-------------+-------+----------| # For particular column(s) ! xsv frequency -- select Origin -- limit 5 flights . csv | csvlook |---------+-------+---------| | field | value | count | |---------+-------+---------| | Origin | ATL | 413851 | | Origin | ORD | 375784 | | Origin | DFW | 297345 | | Origin | DEN | 240928 | | Origin | LAX | 237597 | |---------+-------+---------| Filter rows \u00b6 Filters CSV data by whether the given regex matches a row. The regex is applied to each field in each row, and if any field matches, then the row is written to the output. The columns to search can be limited with the '--select' flag (but the full row is still written to the output if there is a match). Usage: xsv search [options] <regex> [<input>] xsv search --help search options: -i, --ignore-case Case insensitive search. This is equivalent to prefixing the regex with '(?i)'. -s, --select <arg> Select the columns to search. See 'xsv select -h' for the full syntax. -v, --invert-match Select only rows that did not match ! xsv search - s Origin 'ATL' flights . csv \\ | xsv select Month , DayofMonth , UniqueCarrier , FlightNum , Origin , Dest \\ | xsv sample 10 \\ | csvlook |--------+------------+---------------+-----------+--------+-------| | Month | DayofMonth | UniqueCarrier | FlightNum | Origin | Dest | |--------+------------+---------------+-----------+--------+-------| | 6 | 26 | AA | 1756 | ATL | LGA | | 8 | 1 | DL | 522 | ATL | LGA | | 2 | 8 | DL | 1253 | ATL | DEN | | 2 | 17 | DL | 688 | ATL | DCA | | 8 | 22 | EV | 4417 | ATL | SDF | | 2 | 15 | EV | 4544 | ATL | VLD | | 2 | 10 | FL | 163 | ATL | TPA | | 9 | 8 | DL | 829 | ATL | JAC | | 9 | 27 | DL | 1512 | ATL | DTW | | 11 | 20 | AA | 1197 | ATL | DFW | |--------+------------+---------------+-----------+--------+-------| Joins \u00b6 Joins two sets of CSV data on the specified columns. The default join operation is an 'inner' join. Joins are always done by ignoring leading and trailing whitespace. By default, joins are done case sensitively, but this can be disabled with the --no-case flag. The columns arguments specify the columns to join for each input. Columns can be referenced by name or index, starting at 1. Specify multiple columns by separating them with a comma. Specify a range of columns with - . Both columns1 and columns2 must specify exactly the same number of columns. Usage: xsv join [options] <columns1> <input1> <columns2> <input2> xsv join --help join options: --no-case When set, joins are done case insensitively. --left Do a 'left outer' join. --right Do a 'right outer' join. --nulls When set, joins will work on empty fields. Otherwise, empty fields are completely ignored. (In fact, any row that has an empty field in the key specified is ignored.) Get data !wget http://burntsushi.net/stuff/worldcitiespop.csv !wget https://gist.githubusercontent.com/anonymous/063cb470e56e64e98cf1/raw/98e2589b801f6ca3ff900b01a87fbb7452eb35c7/countrynames.csv ! xsv join -- no - case Country worldcitiespop . csv Abbrev countrynames . csv | xsv sample 10 | csvlook |----------+-------------------+-------------------+--------+------------+-----------+------------+--------+--------------| | Country | City | AccentCity | Region | Population | Latitude | Longitude | Abbrev | Country | |----------+-------------------+-------------------+--------+------------+-----------+------------+--------+--------------| | kr | upori | Upori | 12 | | 37.239167 | 126.105833 | KR | South Korea | | id | kaninggiduku | Kaninggiduku | 18 | | -9.6191 | 119.3201 | ID | Indonesia | | ua | ivanovsk | Ivanovsk | 04 | | 48.533572 | 34.158659 | UA | Ukraine | | nl | riscado | Riscado | 00 | | 12.233333 | -68.366667 | NL | Netherlands | | gr | plazumista | Plazum\u00edsta | 09 | | 40.3 | 21.2666667 | GR | Greece | | pl | myslowka | Myslowka | 74 | | 52.028306 | 19.415481 | PL | Poland | | hu | csem | Cs\u00e9m | 12 | | 47.685903 | 18.098287 | HU | Hungary | | cn | tungshan | Tungshan | 05 | | 41.648889 | 127.275833 | CN | China | | ir | gilavard-e bozorg | Gilavard-e Bozorg | 35 | | 36.585273 | 53.588776 | IR | Iran | | be | toutefays | Toutefays | 08 | | 50.983333 | 3.666667 | BE | Belgium | |----------+-------------------+-------------------+--------+------------+-----------+------------+--------+--------------| We have two columns named Country . Use select to get rid of it. ! xsv join -- no - case Country worldcitiespop . csv Abbrev countrynames . csv \\ | xsv select 'Country[1],AccentCity,Population' | xsv search - s Population '[0-9]' | xsv sample 5 | csvlook |----------+-------------------+-------------| | Country | AccentCity | Population | |----------+-------------------+-------------| | Mexico | Cosoleacaque | 20372 | | Mexico | Acolman | 4998 | | Turkey | Amasya | 82939 | | Brazil | Cora\u00e7\u00e3o de Jesus | 12410 | | Brazil | Ribeira do Pombal | 28236 | |----------+-------------------+-------------|","title":"XSV"},{"location":"04_xsv/#xsv","text":"A fast CSV toolkit written in Rust. repo","title":"xsv"},{"location":"04_xsv/#commands","text":"cat Concatenate by row or column count Count records fixlengths Makes all records have same length flatten Show one field per line fmt Format CSV output (change field delimiter) frequency Show frequency tables headers Show header names help Show this usage message. index Create CSV index for faster access input Read CSV data with special quoting rules join Join CSV files sample Randomly sample CSV data search Search CSV data with regexes select Select columns from CSV slice Slice records from CSV sort Sort CSV data split Split CSV data into many files stats Compute basic statistics table Align CSV data into columns","title":"Commands"},{"location":"04_xsv/#install","text":"# run wget https://goo.gl/iZMmzX # or download the tar file from 'https://github.com/BurntSushi/xsv/releases/latest' # untar tar -xzvf xsv-0.11.0-x86_64-unknown-linux-musl.tar.gz # move the executable to the binaries folder mv xsv /usr/bin/xsv # run from anywhere ### 0. View formatted ! head - n 10 flights . csv | cut - d , - f1 - 10 | xsv table Year Month DayofMonth DayOfWeek DepTime CRSDepTime ArrTime CRSArrTime UniqueCarrier FlightNum 2007 1 1 1 1232 1225 1341 1340 WN 2891 2007 1 1 1 1918 1905 2043 2035 WN 462 2007 1 1 1 2206 2130 2334 2300 WN 1229 2007 1 1 1 1230 1200 1356 1330 WN 1355 2007 1 1 1 831 830 957 1000 WN 2278 2007 1 1 1 1430 1420 1553 1550 WN 2386 2007 1 1 1 1936 1840 2217 2130 WN 409 2007 1 1 1 944 935 1223 1225 WN 1131 2007 1 1 1 1537 1450 1819 1735 WN 1212 ### 1. Count Rows (MUCH faster than `wc -l`, uses more CPU, excludes header) ! xsv count flights . csv 7453215 ### 2. Get Column names cols = ! xsv headers flights . csv [ x . split ( ' ' )[ - 1 ] for x in cols ] ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'TailNum', 'ActualElapsedTime', 'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', 'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'CancellationCode', 'Diverted', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay'] ### 5. Subset Columns ! xsv select Month , DayofMonth , UniqueCarrier , FlightNum , Origin , Dest flights . csv | head | csvlook |--------+------------+---------------+-----------+--------+-------| | Month | DayofMonth | UniqueCarrier | FlightNum | Origin | Dest | |--------+------------+---------------+-----------+--------+-------| | 1 | 1 | WN | 2891 | SMF | ONT | | 1 | 1 | WN | 462 | SMF | PDX | | 1 | 1 | WN | 1229 | SMF | PDX | | 1 | 1 | WN | 1355 | SMF | PDX | | 1 | 1 | WN | 2278 | SMF | PDX | | 1 | 1 | WN | 2386 | SMF | PDX | | 1 | 1 | WN | 409 | SMF | PHX | | 1 | 1 | WN | 1131 | SMF | PHX | | 1 | 1 | WN | 1212 | SMF | PHX | |--------+------------+---------------+-----------+--------+-------| ### 3. Create an index ! xsv index flights . csv ! ls *. idx flights.csv.idx ### 4. Get Summary Stats (MUCH faster than `csvstat`) ! xsv stats flights . csv -- everything | \\ xsv select field , type , min , median , max , mean , stddev , mode | \\ csvlook |--------------------+---------+-------+--------+--------+-----------------------+---------------------+-------| | field | type | min | median | max | mean | stddev | mode | |--------------------+---------+-------+--------+--------+-----------------------+---------------------+-------| | Year | Integer | 2007 | 2007 | 2007 | 2007 | 0 | 2007 | | Month | Integer | 1 | 7 | 12 | 6.514876197721434 | 3.425117169491096 | 8 | | DayofMonth | Integer | 1 | 16 | 31 | 15.72588876075652 | 8.781153183533013 | 26 | | DayOfWeek | Integer | 1 | 4 | 7 | 3.9338042710427263 | 1.9922668348832397 | 1 | | DepTime | Unicode | 1 | 1255 | NA | | | NA | | CRSDepTime | Integer | 0 | 1322 | 2359 | 1330.5963490923104 | 464.70792314976785 | 600 | | ArrTime | Unicode | 1 | 1430 | NA | | | NA | | CRSArrTime | Integer | 0 | 1520 | 2400 | 1495.391906714091 | 481.59020392608437 | 1930 | | UniqueCarrier | Unicode | 9E | | YV | | | WN | | FlightNum | Integer | 1 | 1509 | 9602 | 2188.0992893670827 | 1971.9575313097869 | 16 | | TailNum | Unicode | 0 | | NHZOAL | | | 0 | | ActualElapsedTime | Unicode | 100 | 115 | NA | | | NA | | CRSElapsedTime | Unicode | -1240 | 100 | NA | | | 75 | | AirTime | Unicode | 0 | 94 | NA | | | NA | | ArrDelay | Unicode | -1 | -2 | NA | | | -5 | | DepDelay | Unicode | -1 | 1 | NA | | | 0 | | Origin | Unicode | ABE | | YUM | | | ATL | | Dest | Unicode | ABE | | YUM | | | ATL | | Distance | Integer | 11 | 569 | 4962 | 719.805789045388 | 562.3050870976227 | 337 | | TaxiIn | Integer | 0 | 5 | 545 | 6.6919844657641026 | 5.151350775045749 | 4 | | TaxiOut | Integer | 0 | 14 | 530 | 16.300146178528344 | 11.833958552477231 | 10 | | Cancelled | Integer | 0 | 0 | 1 | 0.021567605389083436 | 0.14526680208108242 | 0 | | CancellationCode | Unicode | A | | D | | | | | Diverted | Integer | 0 | 0 | 1 | 0.0023049113704622683 | 0.04795413177231553 | 0 | | CarrierDelay | Integer | 0 | 0 | 2580 | 3.865235874719842 | 20.842403385965035 | 0 | | WeatherDelay | Integer | 0 | 0 | 1429 | 0.7700903569801583 | 9.619546393361475 | 0 | | NASDelay | Integer | 0 | 0 | 1386 | 3.783702200996463 | 16.176702958825693 | 0 | | SecurityDelay | Integer | 0 | 0 | 382 | 0.023735528895919497 | 1.084995425472959 | 0 | | LateAircraftDelay | Integer | 0 | 0 | 1031 | 5.099133997878817 | 21.277529347496895 | 0 | |--------------------+---------+-------+--------+--------+-----------------------+---------------------+-------|","title":"Install"},{"location":"04_xsv/#random-sampling","text":"Randomly samples CSV data uniformly using memory proportional to the size of the sample. When an index is present , this command will use random indexing if the sample size is less than 10% of the total number of records. This allows for efficient sampling such that the entire CSV file is not parsed. Allows a user to work with a CSV data set that is too big to fit into memory (for example, for use with commands like xsv frequency or xsv stats ). Usage: xsv sample [options] <sample-size> [<input>] xsv sample --help ! xsv select Month , DayofMonth , UniqueCarrier , FlightNum , Origin , Dest flights . csv | xsv sample 10 | csvlook |--------+------------+---------------+-----------+--------+-------| | Month | DayofMonth | UniqueCarrier | FlightNum | Origin | Dest | |--------+------------+---------------+-----------+--------+-------| | 1 | 21 | OH | 5315 | CVG | PIT | | 5 | 17 | WN | 629 | LAS | SMF | | 3 | 27 | WN | 962 | LAX | TUS | | 8 | 15 | FL | 44 | SFO | ATL | | 11 | 4 | AQ | 45 | ITO | HNL | | 1 | 31 | WN | 57 | DAL | HOU | | 9 | 4 | OO | 3790 | MSP | ATL | | 9 | 18 | AA | 1411 | BOS | ORD | | 2 | 18 | OO | 5766 | PDX | RDM | | 1 | 18 | OO | 3873 | SLC | MFR | |--------+------------+---------------+-----------+--------+-------|","title":"Random Sampling"},{"location":"04_xsv/#frequency-tables","text":"Computes a frequency table on CSV data formatted as field,value,count The order and number of values can be tweaked with --asc and --limit respectively. memory proportional to the cardinality of each column is required. Usage: xsv frequency [options] [<input>] Options: -s, --select <arg> Select a subset of columns to compute frequencies -l, --limit <arg> Limit the frequency table to the N most common items. [default: 10] -a, --asc Sort the frequency tables in ascending order by count. [default: descending] --no-nulls Don't include NULLs in the frequency table. # For all columns ! xsv frequency flights . csv -- limit 5 | head - n 15 | csvlook |-------------+-------+----------| | field | value | count | |-------------+-------+----------| | Year | 2007 | 7453215 | | Month | 8 | 653279 | | Month | 7 | 648560 | | Month | 3 | 639209 | | Month | 5 | 631609 | | Month | 10 | 629992 | | DayofMonth | 26 | 250136 | | DayofMonth | 19 | 250092 | | DayofMonth | 12 | 249773 | | DayofMonth | 16 | 249034 | | DayofMonth | 9 | 248415 | | DayOfWeek | 1 | 1112474 | | DayOfWeek | 5 | 1101689 | | DayOfWeek | 4 | 1097738 | |-------------+-------+----------| # For particular column(s) ! xsv frequency -- select Origin -- limit 5 flights . csv | csvlook |---------+-------+---------| | field | value | count | |---------+-------+---------| | Origin | ATL | 413851 | | Origin | ORD | 375784 | | Origin | DFW | 297345 | | Origin | DEN | 240928 | | Origin | LAX | 237597 | |---------+-------+---------|","title":"Frequency Tables"},{"location":"04_xsv/#filter-rows","text":"Filters CSV data by whether the given regex matches a row. The regex is applied to each field in each row, and if any field matches, then the row is written to the output. The columns to search can be limited with the '--select' flag (but the full row is still written to the output if there is a match). Usage: xsv search [options] <regex> [<input>] xsv search --help search options: -i, --ignore-case Case insensitive search. This is equivalent to prefixing the regex with '(?i)'. -s, --select <arg> Select the columns to search. See 'xsv select -h' for the full syntax. -v, --invert-match Select only rows that did not match ! xsv search - s Origin 'ATL' flights . csv \\ | xsv select Month , DayofMonth , UniqueCarrier , FlightNum , Origin , Dest \\ | xsv sample 10 \\ | csvlook |--------+------------+---------------+-----------+--------+-------| | Month | DayofMonth | UniqueCarrier | FlightNum | Origin | Dest | |--------+------------+---------------+-----------+--------+-------| | 6 | 26 | AA | 1756 | ATL | LGA | | 8 | 1 | DL | 522 | ATL | LGA | | 2 | 8 | DL | 1253 | ATL | DEN | | 2 | 17 | DL | 688 | ATL | DCA | | 8 | 22 | EV | 4417 | ATL | SDF | | 2 | 15 | EV | 4544 | ATL | VLD | | 2 | 10 | FL | 163 | ATL | TPA | | 9 | 8 | DL | 829 | ATL | JAC | | 9 | 27 | DL | 1512 | ATL | DTW | | 11 | 20 | AA | 1197 | ATL | DFW | |--------+------------+---------------+-----------+--------+-------|","title":"Filter rows"},{"location":"04_xsv/#joins","text":"Joins two sets of CSV data on the specified columns. The default join operation is an 'inner' join. Joins are always done by ignoring leading and trailing whitespace. By default, joins are done case sensitively, but this can be disabled with the --no-case flag. The columns arguments specify the columns to join for each input. Columns can be referenced by name or index, starting at 1. Specify multiple columns by separating them with a comma. Specify a range of columns with - . Both columns1 and columns2 must specify exactly the same number of columns. Usage: xsv join [options] <columns1> <input1> <columns2> <input2> xsv join --help join options: --no-case When set, joins are done case insensitively. --left Do a 'left outer' join. --right Do a 'right outer' join. --nulls When set, joins will work on empty fields. Otherwise, empty fields are completely ignored. (In fact, any row that has an empty field in the key specified is ignored.) Get data !wget http://burntsushi.net/stuff/worldcitiespop.csv !wget https://gist.githubusercontent.com/anonymous/063cb470e56e64e98cf1/raw/98e2589b801f6ca3ff900b01a87fbb7452eb35c7/countrynames.csv ! xsv join -- no - case Country worldcitiespop . csv Abbrev countrynames . csv | xsv sample 10 | csvlook |----------+-------------------+-------------------+--------+------------+-----------+------------+--------+--------------| | Country | City | AccentCity | Region | Population | Latitude | Longitude | Abbrev | Country | |----------+-------------------+-------------------+--------+------------+-----------+------------+--------+--------------| | kr | upori | Upori | 12 | | 37.239167 | 126.105833 | KR | South Korea | | id | kaninggiduku | Kaninggiduku | 18 | | -9.6191 | 119.3201 | ID | Indonesia | | ua | ivanovsk | Ivanovsk | 04 | | 48.533572 | 34.158659 | UA | Ukraine | | nl | riscado | Riscado | 00 | | 12.233333 | -68.366667 | NL | Netherlands | | gr | plazumista | Plazum\u00edsta | 09 | | 40.3 | 21.2666667 | GR | Greece | | pl | myslowka | Myslowka | 74 | | 52.028306 | 19.415481 | PL | Poland | | hu | csem | Cs\u00e9m | 12 | | 47.685903 | 18.098287 | HU | Hungary | | cn | tungshan | Tungshan | 05 | | 41.648889 | 127.275833 | CN | China | | ir | gilavard-e bozorg | Gilavard-e Bozorg | 35 | | 36.585273 | 53.588776 | IR | Iran | | be | toutefays | Toutefays | 08 | | 50.983333 | 3.666667 | BE | Belgium | |----------+-------------------+-------------------+--------+------------+-----------+------------+--------+--------------| We have two columns named Country . Use select to get rid of it. ! xsv join -- no - case Country worldcitiespop . csv Abbrev countrynames . csv \\ | xsv select 'Country[1],AccentCity,Population' | xsv search - s Population '[0-9]' | xsv sample 5 | csvlook |----------+-------------------+-------------| | Country | AccentCity | Population | |----------+-------------------+-------------| | Mexico | Cosoleacaque | 20372 | | Mexico | Acolman | 4998 | | Turkey | Amasya | 82939 | | Brazil | Cora\u00e7\u00e3o de Jesus | 12410 | | Brazil | Ribeira do Pombal | 28236 | |----------+-------------------+-------------|","title":"Joins"},{"location":"05_miller/","text":"miller \u00b6 \"Miller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON\" Github repo , intro , Installation \u00b6 # remove preinstalled versions (usually outdated) sudo apt remove miller # download the new file wget https://github.com/johnkerl/miller/releases/download/v5.1.0/mlr-5.1.0.tar.gz # untar tar -xzf mlr-5.1.0.tar.gz # the usual cd mlr-5.1.0.tar.gz ./configure make sudo make install # check if the mlr executable is in /usr/bin (it could have been placed in /usr/local/bin) # move it if required to /usr/bin/mlr sudo cp /usr/local/bin/mlr /usr/bin/mlr Features \u00b6 useful for data cleaning, data reduction, statistical reporting, format conversion and so on. written in C is format-aware , and retains headers has high-throughput performance on par with the Unix toolkit complements dplyr and pandas by helping you clean-filter-aggregate your data for EDA in-place mutations to files But most importantly, Miller is streaming ; most operations need only a single record in memory at a time (rather needing to hold the entire file in RAM). Miller retains only as much data as needed for operations like sort and stats , so you can operate on files which are larger than RAM Miller complements data-analysis tools such as R , pandas , etc.: you can use Miller to clean and prepare your data. While you can do basic statistics entirely in Miller, its streaming-data feature and single-pass algorithms enable you to reduce very large data sets . Commands \u00b6 Syntax mlr <command> <options> Commands Description cat, cut, grep, head, join, sort, tac, tail, top, uniq Analogs of their Unix-toolkit namesakes, discussed below as well as in Miller features in the context of the Unix toolkit filter, put, sec2gmt, sec2gmtdate, step, tee awk-like functionality bar, bootstrap, decimate, histogram, least-frequent, most-frequent, sample, shuffle, stats1, stats2 Statistically oriented group-by, group-like, having-fields Particularly oriented toward Record-heterogeneity, although all Miller commands can handle heterogeneous records check, count-distinct, label, merge-fields, nest, nothing, rename, rename, reorder, reshape, seqgen These draw from other sources (see also How original is Miller?): count-distinct is SQL-ish, and rename can be done by sed (which does it faster: see Performance). All Verbs: bar bootstrap cat check count-distinct cut decimate filter grep group-by group-like having-fields head histogram join label least-frequent merge-fields most-frequent nest nothing fraction put regularize rename reorder repeat reshape sample sec2gmt sec2gmtdate seqgen shuffle sort stats1 stats2 step tac tail tee top uniq unsparsify Functions for the filter and put verbs: + + - - * / // % ** | ^ & ~ << >> == != =~ !=~ > >= < <= && || ^^ ! ? : . gsub strlen sub substr tolower toupper abs acos acosh asin asinh atan atan2 atanh cbrt ceil cos cosh erf erfc exp expm1 floor invqnorm log log10 log1p logifit madd max mexp min mmul msub pow qnorm round roundm sgn sin sinh sqrt tan tanh urand urand32 urandint dhms2fsec dhms2sec fsec2dhms fsec2hms gmt2sec hms2fsec hms2sec sec2dhms sec2gmt sec2gmt sec2gmtdate sec2hms strftime strptime systime is_absent is_bool is_boolean is_empty is_empty_map is_float is_int is_map is_nonempty_map is_not_empty is_not_map is_not_null is_null is_numeric is_present is_string asserting_absent asserting_bool asserting_boolean asserting_empty asserting_empty_map asserting_float asserting_int asserting_map asserting_nonempty_map asserting_not_empty asserting_not_map asserting_not_null asserting_null asserting_numeric asserting_present asserting_string boolean float fmtnum hexfmt int string typeof depth haskey joink joinkv joinv leafcount length mapdiff mapsum splitkv splitkvx splitnv splitnvx Options \u00b6 Use --csv, --pprint etc. when the input and output formats are the same. Use --icsv --opprint , etc. when you want format conversion Use the mlr -I flag to process files in-place , for example PLEASE USE mlr --csv --rs lf FOR NATIVE UN*X (LINEFEED-TERMINATED) CSV FILES. mlr -I --csv cut -x -f <unwanted_column_name> mydata/*.csv # will remove unwanted_column_name from all your *.csv files in your mydata/ subdirectory. Examples \u00b6 mlr --csv cut -f hostname,uptime mydata.csv # Both input and output in csv mlr --csv --rs lf --fs tab cut -f hostname,uptime file1.tsv file2.tsv # Read tsv (--fs tab) created on unix (--rs lf) and retain named columns, concat into a csv files (--csv) mlr --csv filter '$status != \"down\" && $upsec >= 10000' *.csv # Retain specific rows mlr --nidx put '$sum = $7 + 2.1*$8' *.dat # NIDX: implicitly numerically indexed (Unix-toolkit style) # create a new column from the values in the 7th and 8th columns grep -v '^#' /etc/group | mlr --ifs : --nidx --opprint label group,pass,gid,member then sort -f group # Ignore rows that begin with '#', input file is colon separated, rename columns, then sort and groupby mlr join -j account_id -f accounts.dat then group-by account_name balances.dat # mlr put '$attr = sub($attr, \"([0-9]+)_([0-9]+)_.*\", \"\\1:\\2\")' data/* # mlr stats1 -a min,mean,max,p10,p50,p90 -f flag,u,v data/* # mlr stats2 -a linreg-pca -f u,v -g shape data/* # mlr rename \u00b6 Renames specified fields. Usage: mlr rename [options] {old1,new1,old2,new2,...} use -g for global replacement, and -r for regex matching Examples: mlr rename old_name,new_name mlr rename old_name_1,new_name_1,old_name_2,new_name_2 mlr rename -r 'Date_[0-9]+,Date,' Rename all such fields to be \"Date\" mlr rename -r '\"Date_[0-9]+\",Date' Same mlr rename -r 'Date_([0-9]+).*,\\1' Rename all such fields to be of the form 20151015 mlr rename -r '\"name\"i,Name' Rename \"name\", \"Name\", \"NAME\", etc. to \"Name\" # replace spaces with underscores ! mlr -- csv -- ifs '|' -- ofs ',' rename - g - r ' ,_' ./ raw / Sales . txt > ./ cleaned / Sales . csv ! csvcut - n ./ cleaned / Sales . csv # or !xsv headers ./cleaned/Sales.csv 1: POSSales_PK 2: Date_FK 3: Date 4: Store_FK 5: Item_FK 6: Item_PK 7: Promo_FK 8: POSSales_Ticket_No 9: POSSales_GiftList_No 10: POSSales_GiftListLine_No 11: POSSales_Sales_Quantity 12: POSSales_Sales_AmountExVAT 13: POSSales_Sales_AmountInVAT 14: POSSales_Cost_Amount 15: POSSales_Margin_Amount 16: POSSales_Discount_Amount 17: Store_No_BK 18: POS_Terminal_No_BK 19: Transaction_No_BK 20: Line_No_BK mlr cat, head, tail \u00b6 mlr head and mlr tail count records rather than lines they always return the CSV header mlr head -n 5 myfile.csv will return 6 lines. ! mlr -- csv cat flights . csv | head mlr: unacceptable empty CSV key at file \"./raw/flights.csv\" line 1. This is a very common error. Caused becaused files generated on Unix-like systems have LF line terminators while RFC compliant CSVs have CRLF line terminators (default in miller) fix by including --rs lf this says that the record separator ( rs ) is lf . ! mlr -- csv -- rs lf head - n 5 flights . csv Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,SMF,ONT,389,4,11,0,,0,0,0,0,0,0 2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,SMF,PDX,479,5,6,0,,0,0,0,0,0,0 2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,SMF,PDX,479,6,9,0,,0,3,0,0,0,31 2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,SMF,PDX,479,3,8,0,,0,23,0,0,0,3 2007,1,1,1,831,830,957,1000,WN,2278,N480,86,90,74,-3,1,SMF,PDX,479,3,9,0,,0,0,0,0,0,0 read files with other delimiters by specifying ifs (or input field separator) could be useful for file format conversion ! mlr -- csv -- ifs '|' head - n 5 ./ raw / Sales . txt POSSales_PK,Date_FK,Date,Store_FK,Item_FK,Item_PK,Promo_FK,POSSales_Ticket No,POSSales_GiftList No,POSSales_GiftListLine No,POSSales_Sales Quantity,POSSales_Sales AmountExVAT,POSSales_Sales AmountInVAT,POSSales_Cost Amount,POSSales_Margin Amount,POSSales_Discount Amount,Store No_BK,POS Terminal No_BK,Transaction No_BK,Line No_BK 42169332,41639,2014-01-02 00:00:00,17,316213,20337325,806,3810195837,,0,\"1,00000000000000000000\",\"17,32231000000000000000\",\"20,96000000000000000000\",\"9,98000000000000000000\",\"7,34231000000000000000\",\"-8,99000000000000000000\",S038,P0381,226847,10000 42169333,41639,2014-01-02 00:00:00,17,274932,20194564,812,3810195837,,0,\"1,00000000000000000000\",\"43,38017000000000000000\",\"52,49000000000000000000\",\"35,86000000000000000000\",\"7,52017000000000000000\",\",00000000000000000000\",S038,P0381,226847,20000 42169334,41639,2014-01-02 00:00:00,17,326727,20347663,63,3810195838,,0,\"1,00000000000000000000\",\"6,60331000000000000000\",\"7,99000000000000000000\",\"5,21000000000000000000\",\"1,39331000000000000000\",\",00000000000000000000\",S038,P0381,226848,10000 42169335,41639,2014-01-02 00:00:00,17,311837,20332760,63,3810195838,,0,\"1,00000000000000000000\",\"24,78512000000000000000\",\"29,99000000000000000000\",\"12,23000000000000000000\",\"12,55512000000000000000\",\",00000000000000000000\",S038,P0381,226848,20000 42169336,41639,2014-01-02 00:00:00,17,262025,20181754,63,3810195838,,0,\"1,00000000000000000000\",\"12,38843000000000000000\",\"14,99000000000000000000\",\"6,34000000000000000000\",\"6,04843000000000000000\",\",00000000000000000000\",S038,P0381,226848,30000 # get the first record from every group that appears in the data ! mlr -- csv -- rs lf head - n 1 - g UniqueCarrier then cut - f Origin , Dest , UniqueCarrier , FlightNum flights . csv \\ | head | csvlook |----------------+-----------+--------+-------| | UniqueCarrier | FlightNum | Origin | Dest | |----------------+-----------+--------+-------| | WN | 2891 | SMF | ONT | | XE | 2809 | CLE | CLT | | YV | 2827 | ABQ | PHX | | OH | 5026 | SAT | CVG | | OO | 3664 | SUN | SLC | | UA | 1 | ORD | HNL | | US | 290 | ABQ | LAS | | DL | 1772 | ATL | PNS | | EV | 4083 | ATL | RDU | |----------------+-----------+--------+-------| Chaining \u00b6 Output of one verb may be chained as input to another using \"then\", e.g. mlr stats1 -a min,mean,max -f flag,u,v -g color then sort -f color mlr cat \u00b6 very useful for format conversion ( txt -> csv ) fast!! (under a minute for a 4GB txt file) and concatenating multiple same-schema CSV file mlr cat [options] Passes input records directly to output. Most useful for format conversion. Options: -n Prepend field \"n\" to each record with record-counter starting at 1 -g {comma-separated field name(s)} When used with -n/-N, writes record-counters keyed by specified field name(s). -N {name} Prepend field {name} to each record with record-counter starting at 1 import os os . chdir ( \"/home/data\" ) ! ls | grep kdd kddcup.data kddcup.names ! cat kddcup . names | sed 1 d | cut - d : - f1 duration protocol_type service flag src_bytes dst_bytes land wrong_fragment urgent hot num_failed_logins logged_in num_compromised root_shell su_attempted num_root num_file_creations num_shells num_access_files num_outbound_cmds is_host_login is_guest_login count srv_count serror_rate srv_serror_rate rerror_rate srv_rerror_rate same_srv_rate diff_srv_rate srv_diff_host_rate dst_host_count dst_host_srv_count dst_host_same_srv_rate dst_host_diff_srv_rate dst_host_same_src_port_rate dst_host_srv_diff_host_rate dst_host_serror_rate dst_host_srv_serror_rate dst_host_rerror_rate dst_host_srv_rerror_rate # converting pipe-delimited to csv ! mlr -- csv -- ifs '|' cat ./ raw / Sales . txt > ./ cleaned / Sales . csv ! mlr -- csv head - n 5 ./ cleaned / Sales . csv POSSales_PK,Date_FK,Date,Store_FK,Item_FK,Item_PK,Promo_FK,POSSales_Ticket No,POSSales_GiftList No,POSSales_GiftListLine No,POSSales_Sales Quantity,POSSales_Sales AmountExVAT,POSSales_Sales AmountInVAT,POSSales_Cost Amount,POSSales_Margin Amount,POSSales_Discount Amount,Store No_BK,POS Terminal No_BK,Transaction No_BK,Line No_BK 42169332,41639,2014-01-02 00:00:00,17,316213,20337325,806,3810195837,,0,\"1,00000000000000000000\",\"17,32231000000000000000\",\"20,96000000000000000000\",\"9,98000000000000000000\",\"7,34231000000000000000\",\"-8,99000000000000000000\",S038,P0381,226847,10000 42169333,41639,2014-01-02 00:00:00,17,274932,20194564,812,3810195837,,0,\"1,00000000000000000000\",\"43,38017000000000000000\",\"52,49000000000000000000\",\"35,86000000000000000000\",\"7,52017000000000000000\",\",00000000000000000000\",S038,P0381,226847,20000 42169334,41639,2014-01-02 00:00:00,17,326727,20347663,63,3810195838,,0,\"1,00000000000000000000\",\"6,60331000000000000000\",\"7,99000000000000000000\",\"5,21000000000000000000\",\"1,39331000000000000000\",\",00000000000000000000\",S038,P0381,226848,10000 42169335,41639,2014-01-02 00:00:00,17,311837,20332760,63,3810195838,,0,\"1,00000000000000000000\",\"24,78512000000000000000\",\"29,99000000000000000000\",\"12,23000000000000000000\",\"12,55512000000000000000\",\",00000000000000000000\",S038,P0381,226848,20000 42169336,41639,2014-01-02 00:00:00,17,262025,20181754,63,3810195838,,0,\"1,00000000000000000000\",\"12,38843000000000000000\",\"14,99000000000000000000\",\"6,34000000000000000000\",\"6,04843000000000000000\",\",00000000000000000000\",S038,P0381,226848,30000 # Create an index and extract a row from a specific index ! mlr -- csv -- rs lf head - n 10 then cat - n then filter '$n==2' flights . csv n,Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2,2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,SMF,PDX,479,6,9,0,,0,3,0,0,0,31 # Even numbered rows (creating an index isnt necessary) ! mlr -- csv -- rs lf head - n 10 then filter 'FNR%2==0' flights . csv Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,SMF,PDX,479,5,6,0,,0,0,0,0,0,0 2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,SMF,PDX,479,3,8,0,,0,23,0,0,0,3 2007,1,1,1,1430,1420,1553,1550,WN,2386,N611SW,83,90,74,3,10,SMF,PDX,479,2,7,0,,0,0,0,0,0,0 2007,1,1,1,944,935,1223,1225,WN,1131,N749SW,99,110,86,-2,9,SMF,PHX,647,4,9,0,,0,0,0,0,0,0 2007,1,1,1,1318,1315,1603,1610,WN,2456,N630WN,105,115,92,-7,3,SMF,PHX,647,5,8,0,,0,0,0,0,0,0 #provide implicit header (auto numeric) to headerless files ! cat flights . csv | sed 1 d | cut - d , - f1 - 10 | head \\ | mlr -- csv -- rs lf -- implicit - csv - header cat \\ | csvlook |-------+---+---+---+------+------+------+------+----+-------| | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | |-------+---+---+---+------+------+------+------+----+-------| | 2007 | 1 | 1 | 1 | 1232 | 1225 | 1341 | 1340 | WN | 2891 | | 2007 | 1 | 1 | 1 | 1918 | 1905 | 2043 | 2035 | WN | 462 | | 2007 | 1 | 1 | 1 | 2206 | 2130 | 2334 | 2300 | WN | 1229 | | 2007 | 1 | 1 | 1 | 1230 | 1200 | 1356 | 1330 | WN | 1355 | | 2007 | 1 | 1 | 1 | 831 | 830 | 957 | 1000 | WN | 2278 | | 2007 | 1 | 1 | 1 | 1430 | 1420 | 1553 | 1550 | WN | 2386 | | 2007 | 1 | 1 | 1 | 1936 | 1840 | 2217 | 2130 | WN | 409 | | 2007 | 1 | 1 | 1 | 944 | 935 | 1223 | 1225 | WN | 1131 | | 2007 | 1 | 1 | 1 | 1537 | 1450 | 1819 | 1735 | WN | 1212 | | 2007 | 1 | 1 | 1 | 1318 | 1315 | 1603 | 1610 | WN | 2456 | |-------+---+---+---+------+------+------+------+----+-------| cut: write error: Broken pipe # provide column names (if less than num_cols, the implicit names will be kept) ! cat flights . csv | sed 1 d | cut - d , - f1 - 10 | head \\ | mlr -- csv -- rs lf -- implicit - csv - header label a , b , c , d \\ | csvlook |-------+---+---+---+------+------+------+------+----+-------| | a | b | c | d | 5 | 6 | 7 | 8 | 9 | 10 | |-------+---+---+---+------+------+------+------+----+-------| | 2007 | 1 | 1 | 1 | 1232 | 1225 | 1341 | 1340 | WN | 2891 | | 2007 | 1 | 1 | 1 | 1918 | 1905 | 2043 | 2035 | WN | 462 | | 2007 | 1 | 1 | 1 | 2206 | 2130 | 2334 | 2300 | WN | 1229 | | 2007 | 1 | 1 | 1 | 1230 | 1200 | 1356 | 1330 | WN | 1355 | | 2007 | 1 | 1 | 1 | 831 | 830 | 957 | 1000 | WN | 2278 | | 2007 | 1 | 1 | 1 | 1430 | 1420 | 1553 | 1550 | WN | 2386 | | 2007 | 1 | 1 | 1 | 1936 | 1840 | 2217 | 2130 | WN | 409 | | 2007 | 1 | 1 | 1 | 944 | 935 | 1223 | 1225 | WN | 1131 | | 2007 | 1 | 1 | 1 | 1537 | 1450 | 1819 | 1735 | WN | 1212 | | 2007 | 1 | 1 | 1 | 1318 | 1315 | 1603 | 1610 | WN | 2456 | |-------+---+---+---+------+------+------+------+----+-------| cut: write error: Broken pipe mlr cut \u00b6 select columns by name with -f select all columns except some with -x # Print only Origin,Dest ! head - n 10 flights . csv \\ | mlr -- csv -- rs lf cut - f Origin , Dest \\ | csvlook |---------+-------| | Origin | Dest | |---------+-------| | SMF | ONT | | SMF | PDX | | SMF | PDX | | SMF | PDX | | SMF | PDX | | SMF | PDX | | SMF | PHX | | SMF | PHX | | SMF | PHX | |---------+-------| # print all except Origin, Dest ! head - n 5 flights . csv \\ | mlr -- csv -- rs lf cut - x - f Origin , Dest Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,389,4,11,0,,0,0,0,0,0,0 2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,479,5,6,0,,0,0,0,0,0,0 2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,479,6,9,0,,0,3,0,0,0,31 2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,479,3,8,0,,0,23,0,0,0,3 ! mlr -- csv head - n 10 then cut - r - f \"Amount\" ./ cleaned / Sales . csv | xsv headers 1 POSSales_Sales_AmountExVAT 2 POSSales_Sales_AmountInVAT 3 POSSales_Cost_Amount 4 POSSales_Margin_Amount 5 POSSales_Discount_Amount ! mlr -- csv head - n 10 then cut - r - f \"Quantity\" ./ cleaned / Sales . csv | xsv headers 1 POSSales_Sales_Quantity ! mlr -- csv head - n 10 then having - fields -- all - matching 'Quantity' ./ cleaned / Sales . csv mlr filter \u00b6 retain specific records Examples: mlr filter 'log10($count) > 4.0' mlr filter 'FNR == 2' mlr filter 'urand() < 0.001' mlr filter '$color != \"blue\" && $value > 4.2' mlr filter '($x<.5 && $y<.5) || ($x>.5 && $y>.5)' mlr filter '($name =~ \"^sys.*east$\") || ($name =~ \"^dev.[0-9]+\"i)' mlr filter '$ab = $a+$b; $cd = $c+$d; $ab != $cd' # single condition ! mlr -- csv -- rs lf filter '$Origin == \"SFO\"' flights . csv \\ | csvcut - c Origin , Dest | head | csvlook |---------+-------| | Origin | Dest | |---------+-------| | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | |---------+-------| # compound logic ! mlr -- csv -- rs lf filter '$Origin == \"SFO\" && $Dest == \"DFW\"' flights . csv \\ | csvcut - c UniqueCarrier , FlightNum , Origin , Dest | head | csvlook |----------------+-----------+--------+-------| | UniqueCarrier | FlightNum | Origin | Dest | |----------------+-----------+--------+-------| | UA | 136 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | |----------------+-----------+--------+-------| mlr put \u00b6 derive new columns on the fly from existing ones Adds/updates specified field(s). Expressions are semicolon-separated and must either be assignments, or evaluate to boolean. Please use a dollar sign for field names and double-quotes for string literals. Miller built-in variables are NF NR FNR FILENUM FILENAME PI E, and ENV Examples: mlr put '$y = log10($x); $z = sqrt($y)' mlr put '$x>0.0 { $y=log10($x); $z=sqrt($y) }' # does {...} only if $x > 0.0 mlr put '$x>0.0; $y=log10($x); $z=sqrt($y)' # does all three statements mlr put '$a =~ \"([a-z]+)_([0-9]+); $b = \"left_\\1\"; $c = \"right_\\2\"' mlr put '$a =~ \"([a-z]+)_([0-9]+) { $b = \"left_\\1\"; $c = \"right_\\2\" }' mlr put '$filename = FILENAME' mlr put '$colored_shape = $color . \"_\" . $shape' mlr put '$y = cos($theta); $z = atan2($y, $x)' mlr put '$name = sub($name, \"http.*com\"i, \"\")' mlr put -q '@sum += $x; end {emit @sum}' mlr put -q '@sum[$a] += $x; end {emit @sum, \"a\"}' mlr put -q '@sum[$a][$b] += $x; end {emit @sum, \"a\", \"b\"}' mlr put -q '@min=min(@min,$x);@max=max(@max,$x); end{emitf @min, @max}' mlr put -q 'is_null(@xmax) || $x > @xmax {@xmax=$x; @recmax=$*}; end {emit @recmax}' ! csvcut - c Origin , Dest , Distance flights . csv | head \\ | mlr -- csv -- rs lf put '$Distance_2 = $Distance/100;' \\ | csvlook |---------+------+----------+-------------| | Origin | Dest | Distance | Distance_2 | |---------+------+----------+-------------| | SMF | ONT | 389 | 3.890000 | | SMF | PDX | 479 | 4.790000 | | SMF | PDX | 479 | 4.790000 | | SMF | PDX | 479 | 4.790000 | | SMF | PDX | 479 | 4.790000 | | SMF | PDX | 479 | 4.790000 | | SMF | PHX | 647 | 6.470000 | | SMF | PHX | 647 | 6.470000 | | SMF | PHX | 647 | 6.470000 | |---------+------+----------+-------------| ! cat ./ raw / het - bool . csv name,reachable barney,false betty,true fred,true wilma,1 ! mlr -- icsv -- rs lf -- opprint put '$reachable = boolean($reachable)' ./ raw / het - bool . csv name reachable barney false betty true fred true wilma true ! mlr -- icsv -- rs lf -- opprint put '$reachable = float(boolean($reachable))' ./ raw / het - bool . csv name reachable barney 0.000000 betty 1.000000 fred 1.000000 wilma 1.000000 # Creating an index field ! mlr -- icsv -- rs lf -- opprint put '$index = NR' ./ raw / het - bool . csv name reachable index barney false 1 betty true 2 fred true 3 wilma 1 4 Functions to use with put, filter \u00b6 ! mlr - F | tr ' \\n ' ' \\t ' + + - - * / // % ** | ^ & ~ << >> == != =~ !=~ > >= < <= && || ^^ ! ? : . gsub strlen sub substr tolower toupper abs acos acosh asin asinh atan atan2 atanh cbrt ceil cos cosh erf erfc exp expm1 floor invqnorm log log10 log1p logifit madd max mexp min mmul msub pow qnorm round roundm sgn sin sinh sqrt tan tanh urand urand32 urandint dhms2fsec dhms2sec fsec2dhms fsec2hms gmt2sec hms2fsec hms2sec sec2dhms sec2gmt sec2gmt sec2gmtdate sec2hms strftime strptime systime is_absent is_bool is_boolean is_empty is_empty_map is_float is_int is_map is_nonempty_map is_not_empty is_not_map is_not_null is_null is_numeric is_present is_string asserting_absent asserting_bool asserting_boolean asserting_empty asserting_empty_map asserting_float asserting_int asserting_map asserting_nonempty_map asserting_not_empty asserting_not_map asserting_not_null asserting_null asserting_numeric asserting_present asserting_string boolean float fmtnum hexfmt int string typeof depth haskey joink joinkv joinv leafcount length mapdiff mapsum splitkv splitkvx splitnv splitnvx ! mlr - f + (class=arithmetic #args=2): Addition. + (class=arithmetic #args=1): Unary plus. - (class=arithmetic #args=2): Subtraction. - (class=arithmetic #args=1): Unary minus. * (class=arithmetic #args=2): Multiplication. / (class=arithmetic #args=2): Division. // (class=arithmetic #args=2): Integer division: rounds to negative (pythonic). % (class=arithmetic #args=2): Remainder; never negative-valued (pythonic). ** (class=arithmetic #args=2): Exponentiation; same as pow, but as an infix operator. | (class=arithmetic #args=2): Bitwise OR. ^ (class=arithmetic #args=2): Bitwise XOR. & (class=arithmetic #args=2): Bitwise AND. ~ (class=arithmetic #args=1): Bitwise NOT. Beware '$y=~$x' since =~ is the regex-match operator: try '$y = ~$x'. << (class=arithmetic #args=2): Bitwise left-shift. >> (class=arithmetic #args=2): Bitwise right-shift. == (class=boolean #args=2): String/numeric equality. Mixing number and string results in string compare. != (class=boolean #args=2): String/numeric inequality. Mixing number and string results in string compare. =~ (class=boolean #args=2): String (left-hand side) matches regex (right-hand side), e.g. '$name =~ \"^a.*b$\"'. !=~ (class=boolean #args=2): String (left-hand side) does not match regex (right-hand side), e.g. '$name !=~ \"^a.*b$\"'. > (class=boolean #args=2): String/numeric greater-than. Mixing number and string results in string compare. >= (class=boolean #args=2): String/numeric greater-than-or-equals. Mixing number and string results in string compare. < (class=boolean #args=2): String/numeric less-than. Mixing number and string results in string compare. <= (class=boolean #args=2): String/numeric less-than-or-equals. Mixing number and string results in string compare. && (class=boolean #args=2): Logical AND. || (class=boolean #args=2): Logical OR. ^^ (class=boolean #args=2): Logical XOR. ! (class=boolean #args=1): Logical negation. ? : (class=boolean #args=3): Ternary operator. . (class=string #args=2): String concatenation. gsub (class=string #args=3): Example: '$name=gsub($name, \"old\", \"new\")' (replace all). strlen (class=string #args=1): String length. sub (class=string #args=3): Example: '$name=sub($name, \"old\", \"new\")' (replace once). substr (class=string #args=3): substr(s,m,n) gives substring of s from 0-up position m to n inclusive. Negative indices -len .. -1 alias to 0 .. len-1. tolower (class=string #args=1): Convert string to lowercase. toupper (class=string #args=1): Convert string to uppercase. abs (class=math #args=1): Absolute value. acos (class=math #args=1): Inverse trigonometric cosine. acosh (class=math #args=1): Inverse hyperbolic cosine. asin (class=math #args=1): Inverse trigonometric sine. asinh (class=math #args=1): Inverse hyperbolic sine. atan (class=math #args=1): One-argument arctangent. atan2 (class=math #args=2): Two-argument arctangent. atanh (class=math #args=1): Inverse hyperbolic tangent. cbrt (class=math #args=1): Cube root. ceil (class=math #args=1): Ceiling: nearest integer at or above. cos (class=math #args=1): Trigonometric cosine. cosh (class=math #args=1): Hyperbolic cosine. erf (class=math #args=1): Error function. erfc (class=math #args=1): Complementary error function. exp (class=math #args=1): Exponential function e**x. expm1 (class=math #args=1): e**x - 1. floor (class=math #args=1): Floor: nearest integer at or below. invqnorm (class=math #args=1): Inverse of normal cumulative distribution function. Note that invqorm(urand()) is normally distributed. log (class=math #args=1): Natural (base-e) logarithm. log10 (class=math #args=1): Base-10 logarithm. log1p (class=math #args=1): log(1-x). logifit (class=math #args=3): Given m and b from logistic regression, compute fit: $yhat=logifit($x,$m,$b). madd (class=math #args=3): a + b mod m (integers) max (class=math variadic): max of n numbers; null loses mexp (class=math #args=3): a ** b mod m (integers) min (class=math variadic): Min of n numbers; null loses mmul (class=math #args=3): a * b mod m (integers) msub (class=math #args=3): a - b mod m (integers) pow (class=math #args=2): Exponentiation; same as **. qnorm (class=math #args=1): Normal cumulative distribution function. round (class=math #args=1): Round to nearest integer. roundm (class=math #args=2): Round to nearest multiple of m: roundm($x,$m) is the same as round($x/$m)*$m sgn (class=math #args=1): +1 for positive input, 0 for zero input, -1 for negative input. sin (class=math #args=1): Trigonometric sine. sinh (class=math #args=1): Hyperbolic sine. sqrt (class=math #args=1): Square root. tan (class=math #args=1): Trigonometric tangent. tanh (class=math #args=1): Hyperbolic tangent. urand (class=math #args=0): Floating-point numbers on the unit interval. Int-valued example: '$n=floor(20+urand()*11)'. urand32 (class=math #args=0): Integer uniformly distributed 0 and 2**32-1 inclusive. urandint (class=math #args=2): Integer uniformly distributed between inclusive integer endpoints. dhms2fsec (class=time #args=1): Recovers floating-point seconds as in dhms2fsec(\"5d18h53m20.250000s\") = 500000.250000 dhms2sec (class=time #args=1): Recovers integer seconds as in dhms2sec(\"5d18h53m20s\") = 500000 fsec2dhms (class=time #args=1): Formats floating-point seconds as in fsec2dhms(500000.25) = \"5d18h53m20.250000s\" fsec2hms (class=time #args=1): Formats floating-point seconds as in fsec2hms(5000.25) = \"01:23:20.250000\" gmt2sec (class=time #args=1): Parses GMT timestamp as integer seconds since the epoch. hms2fsec (class=time #args=1): Recovers floating-point seconds as in hms2fsec(\"01:23:20.250000\") = 5000.250000 hms2sec (class=time #args=1): Recovers integer seconds as in hms2sec(\"01:23:20\") = 5000 sec2dhms (class=time #args=1): Formats integer seconds as in sec2dhms(500000) = \"5d18h53m20s\" sec2gmt (class=time #args=1): Formats seconds since epoch (integer part) as GMT timestamp, e.g. sec2gmt(1440768801.7) = \"2015-08-28T13:33:21Z\". Leaves non-numbers as-is. sec2gmt (class=time #args=2): Formats seconds since epoch as GMT timestamp with n decimal places for seconds, e.g. sec2gmt(1440768801.7,1) = \"2015-08-28T13:33:21.7Z\". Leaves non-numbers as-is. sec2gmtdate (class=time #args=1): Formats seconds since epoch (integer part) as GMT timestamp with year-month-date, e.g. sec2gmtdate(1440768801.7) = \"2015-08-28\". Leaves non-numbers as-is. sec2hms (class=time #args=1): Formats integer seconds as in sec2hms(5000) = \"01:23:20\" strftime (class=time #args=2): Formats seconds since the epoch as timestamp, e.g. strftime(1440768801.7,\"%Y-%m-%dT%H:%M:%SZ\") = \"2015-08-28T13:33:21Z\", and strftime(1440768801.7,\"%Y-%m-%dT%H:%M:%3SZ\") = \"2015-08-28T13:33:21.700Z\". Format strings are as in the C library (please see \"man strftime\" on your system), with the Miller-specific addition of \"%1S\" through \"%9S\" which format the seocnds with 1 through 9 decimal places, respectively. (\"%S\" uses no decimal places.) strptime (class=time #args=2): Parses timestamp as floating-point seconds since the epoch, e.g. strptime(\"2015-08-28T13:33:21Z\",\"%Y-%m-%dT%H:%M:%SZ\") = 1440768801.000000, and strptime(\"2015-08-28T13:33:21.345Z\",\"%Y-%m-%dT%H:%M:%SZ\") = 1440768801.345000. systime (class=time #args=0): Floating-point seconds since the epoch, e.g. 1440768801.748936. is_absent (class=typing #args=1): False if field is present in input, false otherwise is_bool (class=typing #args=1): True if field is present with boolean value. Synonymous with is_boolean. is_boolean (class=typing #args=1): True if field is present with boolean value. Synonymous with is_bool. is_empty (class=typing #args=1): True if field is present in input with empty string value, false otherwise. is_empty_map (class=typing #args=1): True if argument is a map which is empty. is_float (class=typing #args=1): True if field is present with value inferred to be float is_int (class=typing #args=1): True if field is present with value inferred to be int is_map (class=typing #args=1): True if argument is a map. is_nonempty_map (class=typing #args=1): True if argument is a map which is non-empty. is_not_empty (class=typing #args=1): False if field is present in input with empty value, false otherwise is_not_map (class=typing #args=1): True if argument is not a map. is_not_null (class=typing #args=1): False if argument is null (empty or absent), true otherwise. is_null (class=typing #args=1): True if argument is null (empty or absent), false otherwise. is_numeric (class=typing #args=1): True if field is present with value inferred to be int or float is_present (class=typing #args=1): True if field is present in input, false otherwise. is_string (class=typing #args=1): True if field is present with string (including empty-string) value asserting_absent (class=typing #args=1): Returns argument if it is absent in the input data, else throws an error. asserting_bool (class=typing #args=1): Returns argument if it is present with boolean value, else throws an error. asserting_boolean (class=typing #args=1): Returns argument if it is present with boolean value, else throws an error. asserting_empty (class=typing #args=1): Returns argument if it is present in input with empty value, else throws an error. asserting_empty_map (class=typing #args=1): Returns argument if it is a map with empty value, else throws an error. asserting_float (class=typing #args=1): Returns argument if it is present with float value, else throws an error. asserting_int (class=typing #args=1): Returns argument if it is present with int value, else throws an error. asserting_map (class=typing #args=1): Returns argument if it is a map, else throws an error. asserting_nonempty_map (class=typing #args=1): Returns argument if it is a non-empty map, else throws an error. asserting_not_empty (class=typing #args=1): Returns argument if it is present in input with non-empty value, else throws an error. asserting_not_map (class=typing #args=1): Returns argument if it is not a map, else throws an error. asserting_not_null (class=typing #args=1): Returns argument if it is non-null (non-empty and non-absent), else throws an error. asserting_null (class=typing #args=1): Returns argument if it is null (empty or absent), else throws an error. asserting_numeric (class=typing #args=1): Returns argument if it is present with int or float value, else throws an error. asserting_present (class=typing #args=1): Returns argument if it is present in input, else throws an error. asserting_string (class=typing #args=1): Returns argument if it is present with string (including empty-string) value, else throws an error. boolean (class=conversion #args=1): Convert int/float/bool/string to boolean. float (class=conversion #args=1): Convert int/float/bool/string to float. fmtnum (class=conversion #args=2): Convert int/float/bool to string using printf-style format string, e.g. '$s = fmtnum($n, \"%06lld\")'. hexfmt (class=conversion #args=1): Convert int to string, e.g. 255 to \"0xff\". int (class=conversion #args=1): Convert int/float/bool/string to int. string (class=conversion #args=1): Convert int/float/bool/string to string. typeof (class=conversion #args=1): Convert argument to type of argument (e.g. MT_STRING). For debug. depth (class=maps #args=1): Prints maximum depth of hashmap: ''. Scalars have depth 0. haskey (class=maps #args=2): True/false if map has/hasn't key, e.g. 'haskey($*, \"a\")' or 'haskey(mymap, mykey)'. Error if 1st argument is not a map. joink (class=maps #args=2): Makes string from map keys. E.g. 'joink($*, \",\")'. joinkv (class=maps #args=3): Makes string from map key-value pairs. E.g. 'joinkv(@v[2], \"=\", \",\")' joinv (class=maps #args=2): Makes string from map keys. E.g. 'joinv(mymap, \",\")'. leafcount (class=maps #args=1): Counts total number of terminal values in hashmap. For single-level maps, same as length. length (class=maps #args=1): Counts number of top-level entries in hashmap. Scalars have length 1. mapdiff (class=maps variadic): With 0 args, returns empty map. With 1 arg, returns copy of arg. With 2 or more, returns copy of arg 1 with all keys from any of remaining argument maps removed. mapsum (class=maps variadic): With 0 args, returns empty map. With >= 1 arg, returns a map with key-value pairs from all arguments. Rightmost collisions win, e.g. 'mapsum({1:2,3,4},{1:5})' is '{1:5,3:4}'. splitkv (class=maps #args=3): Splits string by separators into map with type inference. E.g. 'splitkv(\"a=1,b=2,c=3\", \"=\", \",\")' gives '{\"a\" : 1, \"b\" : 2, \"c\" : 3}'. splitkvx (class=maps #args=3): Splits string by separators into map without type inference (keys and values are strings). E.g. 'splitkv(\"a=1,b=2,c=3\", \"=\", \",\")' gives '{\"a\" : \"1\", \"b\" : \"2\", \"c\" : \"3\"}'. splitnv (class=maps #args=2): Splits string by separator into integer-indexed map with type inference. E.g. 'splitnv(\"a,b,c\" , \",\")' gives '{1 : \"a\", 2 : \"b\", 3 : \"c\"}'. splitnvx (class=maps #args=2): Splits string by separator into integer-indexed map without type inference (values are strings). E.g. 'splitnv(\"4,5,6\" , \",\")' gives '{1 : \"4\", 2 : \"5\", 3 : \"6\"}'. To set the seed for urand, you may specify decimal or hexadecimal 32-bit numbers of the form \"mlr --seed 123456789\" or \"mlr --seed 0xcafefeed\". Miller's built-in variables are NF, NR, FNR, FILENUM, and FILENAME (awk-like) along with the mathematical constants PI and E. ! xsv headers ./ cleaned / Sales . csv 1 POSSales_PK 2 Date_FK 3 Date 4 Store_FK 5 Item_FK 6 Item_PK 7 Promo_FK 8 POSSales_Ticket_No 9 POSSales_GiftList_No 10 POSSales_GiftListLine_No 11 POSSales_Sales_Quantity 12 POSSales_Sales_AmountExVAT 13 POSSales_Sales_AmountInVAT 14 POSSales_Cost_Amount 15 POSSales_Margin_Amount 16 POSSales_Discount_Amount 17 Store_No_BK 18 POS_Terminal_No_BK 19 Transaction_No_BK 20 Line_No_BK ! mlr -- csv head - n 10 \\ then put '$POSSales_Sales_AmountExVAT2=float(gsub($POSSales_Sales_AmountExVAT,\",\",\".\"))' \\ then cut - r - f 'POSSales_Sales_AmountExVAT' ./ cleaned / Sales . csv POSSales_Sales_AmountExVAT,POSSales_Sales_AmountExVAT2 \"17,32231000000000000000\",17.322310 \"43,38017000000000000000\",43.380170 \"6,60331000000000000000\",6.603310 \"24,78512000000000000000\",24.785120 \"12,38843000000000000000\",12.388430 \",94340000000000000000\",0.943400 \"2,22314000000000000000\",2.223140 \"10,70248000000000000000\",10.702480 \"3,29752000000000000000\",3.297520 \"28,91736000000000000000\",28.917360 sec2gmt, sec2gmtdate for epoch timestamps \u00b6 Replaces a numeric field representing seconds since the epoch with the corresponding GMT timestamp mlr sec2gmt time1,time2 # is the same as mlr put '$time1=sec2gmt($time1);$time2=sec2gmt($time2)' Alternatively, use the following functions with put strftime: Formats seconds since epoch (integer part) as timestamp, e.g. strftime(1440768801.7,\"%Y-%m-%dT%H:%M:%SZ\") = \"2015-08-28T13:33:21Z\". strptime: Parses timestamp as integer seconds since epoch, e.g. strptime(\"2015-08-28T13:33:21Z\",\"%Y-%m-%dT%H:%M:%SZ\") = 1440768801. ! mlr -- csv head - n 10 then cut - f 'Date' ./ cleaned / Sales . csv Date 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 ! mlr -- csv head - n 10 \\ then cut - f 'Date' \\ then put '$epoch=strptime($Date, \"%Y-%m- %d %H:%M:%S\")' ./ cleaned / Sales . csv \\ | csvlook |----------------------+-------------| | Date | epoch | |----------------------+-------------| | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | |----------------------+-------------| mlr label \u00b6 use --implicit-csv-header with mlr cat to auto-create numeric column names create column labels for files using mlr <options> label <colnames> ! csvcut - c Origin , Dest , FlightNum flights . csv | head | sed 1 d \\ | mlr -- csv -- rs lf label col1 , col2 , col3 col1,col2,col3 SMF,PDX,462 SMF,PDX,1229 SMF,PDX,1355 SMF,PDX,2278 SMF,PDX,2386 SMF,PHX,409 SMF,PHX,1131 SMF,PHX,1212 mlr rename \u00b6 Renames specified fields. Usage: mlr rename [options] {old1,new1,old2,new2,...} use -g for global replacement, and -r for regex matching Examples: mlr rename old_name,new_name mlr rename old_name_1,new_name_1,old_name_2,new_name_2 mlr rename -r 'Date_[0-9]+,Date,' Rename all such fields to be \"Date\" mlr rename -r '\"Date_[0-9]+\",Date' Same mlr rename -r 'Date_([0-9]+).*,\\1' Rename all such fields to be of the form 20151015 mlr rename -r '\"name\"i,Name' Rename \"name\", \"Name\", \"NAME\", etc. to \"Name\" # replace spaces with underscores ! mlr -- csv -- ifs '|' -- ofs ',' rename - g - r ' ,_' ./ raw / Sales . txt > ./ cleaned / Sales . csv ! csvcut - n ./ cleaned / Sales . csv # or !xsv headers ./cleaned/Sales.csv 1: POSSales_PK 2: Date_FK 3: Date 4: Store_FK 5: Item_FK 6: Item_PK 7: Promo_FK 8: POSSales_Ticket_No 9: POSSales_GiftList_No 10: POSSales_GiftListLine_No 11: POSSales_Sales_Quantity 12: POSSales_Sales_AmountExVAT 13: POSSales_Sales_AmountInVAT 14: POSSales_Cost_Amount 15: POSSales_Margin_Amount 16: POSSales_Discount_Amount 17: Store_No_BK 18: POS_Terminal_No_BK 19: Transaction_No_BK 20: Line_No_BK Random Sampling with bootstrap, sample, shuffle \u00b6 bootstrap is for sampling with replacement The canonical use for bootstrap sampling is to put error bars (or, confints ) on statistical quantities, such as the mean. ! mlr -- csv -- rs lf bootstrap - n 100 then stats1 - a mean - f WeatherDelay - g UniqueCarrier flights . csv UniqueCarrier,WeatherDelay_mean NW,0.000000 OH,2.000000 DL,0.000000 9E,0.000000 AA,0.000000 WN,0.000000 UA,0.000000 FL,0.000000 OO,0.000000 EV,46.833333 US,0.000000 YV,0.000000 MQ,0.000000 XE,0.000000 AS,0.000000 CO,0.000000 AQ,0.000000 HA,0.000000 B6,0.000000 sample is for sampling -k records without replacement Stratified if groups are specified with a -g switch ! mlr -- csv -- rs lf sample - k 4 flights . csv Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,4,20,5,3,2353,221,209,US,19,N601AW,138,136,116,12,10,LAS,PDX,762,3,19,0,,0,0,0,0,0,0 2007,3,21,3,902,900,1242,1153,YV,7176,N37342,160,113,93,49,2,ORD,CAE,666,4,63,0,,0,49,0,0,0,0 2007,8,3,5,2115,2028,2248,2218,FL,59,N932AT,93,110,72,30,47,IAD,ATL,533,9,12,0,,0,0,0,0,0,30 2007,10,12,5,810,820,947,1005,AA,1895,N5FHAA,157,165,139,-18,-10,MCO,DFW,984,6,12,0,,0,0,0,0,0,0 mlr count-distinct \u00b6 ! mlr -- csv -- rs lf count - distinct - f Origin , Dest then sort - nr count then head - n 10 flights . csv | csvlook |---------+------+--------| | Origin | Dest | count | |---------+------+--------| | OGG | HNL | 16099 | | HNL | OGG | 15876 | | LAX | LAS | 14385 | | LAS | LAX | 13815 | | HNL | LIH | 13156 | | LIH | HNL | 13030 | | SAN | LAX | 12779 | | LAX | SAN | 12767 | | BOS | LGA | 12263 | | LAS | PHX | 12228 | |---------+------+--------| ! cat flights . csv | cut - d , - f17 | sed 1 d | sort | uniq | wc - l 304 mlr merge-fields \u00b6 like mlr stats1 but all accumulation is done across fields within each given record: horizontal rather than vertical statistics ! mlr -- csv -- rs lf head - n 10 then cut - r - f \"Time\" flights . csv | csvlook |----------+------------+---------+------------+-------------------+----------------+----------| | DepTime | CRSDepTime | ArrTime | CRSArrTime | ActualElapsedTime | CRSElapsedTime | AirTime | |----------+------------+---------+------------+-------------------+----------------+----------| | 1232 | 1225 | 1341 | 1340 | 69 | 75 | 54 | | 1918 | 1905 | 2043 | 2035 | 85 | 90 | 74 | | 2206 | 2130 | 2334 | 2300 | 88 | 90 | 73 | | 1230 | 1200 | 1356 | 1330 | 86 | 90 | 75 | | 831 | 830 | 957 | 1000 | 86 | 90 | 74 | | 1430 | 1420 | 1553 | 1550 | 83 | 90 | 74 | | 1936 | 1840 | 2217 | 2130 | 101 | 110 | 89 | | 944 | 935 | 1223 | 1225 | 99 | 110 | 86 | | 1537 | 1450 | 1819 | 1735 | 102 | 105 | 90 | | 1318 | 1315 | 1603 | 1610 | 105 | 115 | 92 | |----------+------------+---------+------------+-------------------+----------------+----------| ! mlr -- csv -- rs lf head - n 10 then \\ cut - r - f \"Time\" then \\ merge - fields - a sum - r 'Time' - k - o time \\ flights . csv | csvlook |----------+------------+---------+------------+-------------------+----------------+---------+-----------| | DepTime | CRSDepTime | ArrTime | CRSArrTime | ActualElapsedTime | CRSElapsedTime | AirTime | time_sum | |----------+------------+---------+------------+-------------------+----------------+---------+-----------| | 1232 | 1225 | 1341 | 1340 | 69 | 75 | 54 | 5336 | | 1918 | 1905 | 2043 | 2035 | 85 | 90 | 74 | 8150 | | 2206 | 2130 | 2334 | 2300 | 88 | 90 | 73 | 9221 | | 1230 | 1200 | 1356 | 1330 | 86 | 90 | 75 | 5367 | | 831 | 830 | 957 | 1000 | 86 | 90 | 74 | 3868 | | 1430 | 1420 | 1553 | 1550 | 83 | 90 | 74 | 6200 | | 1936 | 1840 | 2217 | 2130 | 101 | 110 | 89 | 8423 | | 944 | 935 | 1223 | 1225 | 99 | 110 | 86 | 4622 | | 1537 | 1450 | 1819 | 1735 | 102 | 105 | 90 | 6838 | | 1318 | 1315 | 1603 | 1610 | 105 | 115 | 92 | 6158 | |----------+------------+---------+------------+-------------------+----------------+---------+-----------| stats1 \u00b6 Computes univariate statistics for one or more given fields, accumulated across the input record stream. switch -f {a,b,c} Value-field names on which to compute statistics switch -g {d,e,f} Optional group-by-field names switch -a {g,h,i} with the listed stats count Count instances of fields mode Find most-frequently-occurring values for fields; first-found wins tie antimode Find least-frequently-occurring values for fields; first-found wins tie sum Compute sums of specified fields mean Compute averages (sample means) of specified fields stddev Compute sample standard deviation of specified fields var Compute sample variance of specified fields meaneb Estimate error bars for averages (assuming no sample autocorrelation) skewness Compute sample skewness of specified fields kurtosis Compute sample kurtosis of specified fields min Compute minimum values of specified fields max Compute maximum values of specified fields Examples - mlr stats1 -a min,p10,p50,p90,max -f value -g size,shape mlr stats1 -a count,mode -f size mlr stats1 -a count,mode -f size -g shape ! mlr -- csv -- rs lf filter '$ArrDelay!=\"NA\"' \\ then stats1 - a mean , stddev - f ArrDelay - g Origin \\ then sort - nr ArrDelay_mean \\ then head - n 10 \\ flights . csv | csvlook |---------+---------------+------------------| | Origin | ArrDelay_mean | ArrDelay_stddev | |---------+---------------+------------------| | ACK | 46.115523 | 82.715645 | | SOP | 40.464865 | 61.941857 | | PIR | 35.750000 | 33.260337 | | MKC | 25.000000 | | | CEC | 24.763527 | 47.427527 | | MCN | 24.087607 | 79.076628 | | SPI | 21.591925 | 62.123661 | | EWN | 21.486532 | 53.500822 | | HHH | 20.648305 | 61.881898 | | MEI | 20.529141 | 52.402560 | |---------+---------------+------------------| ! mlr -- csv -- rs lf filter '$ArrDelay!=\"NA\"' \\ then stats1 - a min , p05 , p10 , p90 , p95 , max - f ArrDelay - g Origin \\ then sort - f Origin \\ then head - n 10 \\ flights . csv | csvlook |---------+--------------+--------------+--------------+--------------+--------------+---------------| | Origin | ArrDelay_min | ArrDelay_p05 | ArrDelay_p10 | ArrDelay_p90 | ArrDelay_p95 | ArrDelay_max | |---------+--------------+--------------+--------------+--------------+--------------+---------------| | ABE | -44 | -22 | -18 | 39 | 83 | 690 | | ABI | -32 | -14 | -12 | 39 | 78 | 695 | | ABQ | -61 | -19 | -15 | 29 | 54 | 1049 | | ABY | -27 | -15 | -12 | 45 | 79 | 336 | | ACK | -51 | -37 | -31 | 163 | 234 | 386 | | ACT | -32 | -17 | -14 | 33 | 67 | 366 | | ACV | -30 | -17 | -14 | 62 | 96 | 498 | | ACY | -55 | -30 | -24 | 60 | 110 | 741 | | ADK | -83 | -73 | -65 | 56 | 78 | 160 | | ADQ | -31 | -19 | -16 | 41 | 66 | 275 | |---------+--------------+--------------+--------------+--------------+--------------+---------------| stats2 \u00b6 for bivariate statistics -a {linreg-ols,corr,...} Names of accumulators: one or more of: linreg-pca Linear regression using principal component analysis linreg-ols Linear regression using ordinary least squares r2 Quality metric for linreg-ols (linreg-pca emits its own) logireg Logistic regression corr Sample correlation cov Sample covariance covx Sample-covariance matrix -f {a,b,c,d} Value-field name-pairs on which to compute statistics. There must be an even number of names. -g {e,f,g} Optional group-by-field names. -v Print additional output for linreg-pca. -s Print iterative stats. Useful in tail -f contexts (in which case please avoid pprint-format output since end of input stream will never be seen). --fit Rather than printing regression parameters, applies them to the input data to compute new fit fields. All input records are held in memory until end of input stream. Has effect only for linreg-ols, linreg-pca, and logireg. Only one of -s or --fit may be used. Examples mlr stats2 -a linreg-pca -f x,y mlr stats2 -a linreg-ols,r2 -f x,y -g size,shape mlr stats2 -a corr -f x,y ! mlr -- csv -- rs lf filter '$ArrDelay!=\"NA\"' \\ then stats2 - a corr - f ArrDelay , AirTime steps \u00b6 Computes values dependent on the previous record, optionally grouped by category. useful for calculating lagged variables Most Miller commands are record-at-a-time , with the exception of stats1, stats2 , and histogram which compute aggregate output. The step command is intermediate: it allows the option of adding fields which are functions of fields from previous records. Rsum is short for running sum. Options: -a {delta,rsum,...} Names of steppers: comma-separated, one or more of: delta Compute differences in field(s) between successive records shift Include value(s) in field(s) from previous record, if any from-first Compute differences in field(s) from first record ratio Compute ratios in field(s) between successive records rsum Compute running sums of field(s) between successive records counter Count instances of field(s) between successive records ewma Exponentially weighted moving average over successive records -f {a,b,c} Value-field names on which to compute statistics -g {d,e,f} Optional group-by-field names -F Computes integerable things (e.g. counter) in floating point. -d {x,y,z} Weights for ewma. 1 means current sample gets all weight (no smoothing), near under under 1 is light smoothing, near over 0 is heavy smoothing. Multiple weights may be specified, e.g. \"mlr step -a ewma -f sys_load -d 0.01,0.1,0.9\". Default if omitted is \"-d 0.5\". -o {a,b,c} Custom suffixes for EWMA output fields. If omitted, these default to the -d values. If supplied, the number of -o values must be the same as the number of -d values. Examples: mlr step -a rsum -f request_size mlr step -a delta -f request_size -g hostname mlr step -a ewma -d 0.1,0.9 -f x,y mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y -g group_name Also see \u00b6 uniq for uniques and frequency tables top for nlargest some examples DSL for put , filter","title":"Miller"},{"location":"05_miller/#miller","text":"\"Miller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON\" Github repo , intro ,","title":"miller"},{"location":"05_miller/#installation","text":"# remove preinstalled versions (usually outdated) sudo apt remove miller # download the new file wget https://github.com/johnkerl/miller/releases/download/v5.1.0/mlr-5.1.0.tar.gz # untar tar -xzf mlr-5.1.0.tar.gz # the usual cd mlr-5.1.0.tar.gz ./configure make sudo make install # check if the mlr executable is in /usr/bin (it could have been placed in /usr/local/bin) # move it if required to /usr/bin/mlr sudo cp /usr/local/bin/mlr /usr/bin/mlr","title":"Installation"},{"location":"05_miller/#features","text":"useful for data cleaning, data reduction, statistical reporting, format conversion and so on. written in C is format-aware , and retains headers has high-throughput performance on par with the Unix toolkit complements dplyr and pandas by helping you clean-filter-aggregate your data for EDA in-place mutations to files But most importantly, Miller is streaming ; most operations need only a single record in memory at a time (rather needing to hold the entire file in RAM). Miller retains only as much data as needed for operations like sort and stats , so you can operate on files which are larger than RAM Miller complements data-analysis tools such as R , pandas , etc.: you can use Miller to clean and prepare your data. While you can do basic statistics entirely in Miller, its streaming-data feature and single-pass algorithms enable you to reduce very large data sets .","title":"Features"},{"location":"05_miller/#commands","text":"Syntax mlr <command> <options> Commands Description cat, cut, grep, head, join, sort, tac, tail, top, uniq Analogs of their Unix-toolkit namesakes, discussed below as well as in Miller features in the context of the Unix toolkit filter, put, sec2gmt, sec2gmtdate, step, tee awk-like functionality bar, bootstrap, decimate, histogram, least-frequent, most-frequent, sample, shuffle, stats1, stats2 Statistically oriented group-by, group-like, having-fields Particularly oriented toward Record-heterogeneity, although all Miller commands can handle heterogeneous records check, count-distinct, label, merge-fields, nest, nothing, rename, rename, reorder, reshape, seqgen These draw from other sources (see also How original is Miller?): count-distinct is SQL-ish, and rename can be done by sed (which does it faster: see Performance). All Verbs: bar bootstrap cat check count-distinct cut decimate filter grep group-by group-like having-fields head histogram join label least-frequent merge-fields most-frequent nest nothing fraction put regularize rename reorder repeat reshape sample sec2gmt sec2gmtdate seqgen shuffle sort stats1 stats2 step tac tail tee top uniq unsparsify Functions for the filter and put verbs: + + - - * / // % ** | ^ & ~ << >> == != =~ !=~ > >= < <= && || ^^ ! ? : . gsub strlen sub substr tolower toupper abs acos acosh asin asinh atan atan2 atanh cbrt ceil cos cosh erf erfc exp expm1 floor invqnorm log log10 log1p logifit madd max mexp min mmul msub pow qnorm round roundm sgn sin sinh sqrt tan tanh urand urand32 urandint dhms2fsec dhms2sec fsec2dhms fsec2hms gmt2sec hms2fsec hms2sec sec2dhms sec2gmt sec2gmt sec2gmtdate sec2hms strftime strptime systime is_absent is_bool is_boolean is_empty is_empty_map is_float is_int is_map is_nonempty_map is_not_empty is_not_map is_not_null is_null is_numeric is_present is_string asserting_absent asserting_bool asserting_boolean asserting_empty asserting_empty_map asserting_float asserting_int asserting_map asserting_nonempty_map asserting_not_empty asserting_not_map asserting_not_null asserting_null asserting_numeric asserting_present asserting_string boolean float fmtnum hexfmt int string typeof depth haskey joink joinkv joinv leafcount length mapdiff mapsum splitkv splitkvx splitnv splitnvx","title":"Commands"},{"location":"05_miller/#options","text":"Use --csv, --pprint etc. when the input and output formats are the same. Use --icsv --opprint , etc. when you want format conversion Use the mlr -I flag to process files in-place , for example PLEASE USE mlr --csv --rs lf FOR NATIVE UN*X (LINEFEED-TERMINATED) CSV FILES. mlr -I --csv cut -x -f <unwanted_column_name> mydata/*.csv # will remove unwanted_column_name from all your *.csv files in your mydata/ subdirectory.","title":"Options"},{"location":"05_miller/#examples","text":"mlr --csv cut -f hostname,uptime mydata.csv # Both input and output in csv mlr --csv --rs lf --fs tab cut -f hostname,uptime file1.tsv file2.tsv # Read tsv (--fs tab) created on unix (--rs lf) and retain named columns, concat into a csv files (--csv) mlr --csv filter '$status != \"down\" && $upsec >= 10000' *.csv # Retain specific rows mlr --nidx put '$sum = $7 + 2.1*$8' *.dat # NIDX: implicitly numerically indexed (Unix-toolkit style) # create a new column from the values in the 7th and 8th columns grep -v '^#' /etc/group | mlr --ifs : --nidx --opprint label group,pass,gid,member then sort -f group # Ignore rows that begin with '#', input file is colon separated, rename columns, then sort and groupby mlr join -j account_id -f accounts.dat then group-by account_name balances.dat # mlr put '$attr = sub($attr, \"([0-9]+)_([0-9]+)_.*\", \"\\1:\\2\")' data/* # mlr stats1 -a min,mean,max,p10,p50,p90 -f flag,u,v data/* # mlr stats2 -a linreg-pca -f u,v -g shape data/* #","title":"Examples"},{"location":"05_miller/#mlr-rename","text":"Renames specified fields. Usage: mlr rename [options] {old1,new1,old2,new2,...} use -g for global replacement, and -r for regex matching Examples: mlr rename old_name,new_name mlr rename old_name_1,new_name_1,old_name_2,new_name_2 mlr rename -r 'Date_[0-9]+,Date,' Rename all such fields to be \"Date\" mlr rename -r '\"Date_[0-9]+\",Date' Same mlr rename -r 'Date_([0-9]+).*,\\1' Rename all such fields to be of the form 20151015 mlr rename -r '\"name\"i,Name' Rename \"name\", \"Name\", \"NAME\", etc. to \"Name\" # replace spaces with underscores ! mlr -- csv -- ifs '|' -- ofs ',' rename - g - r ' ,_' ./ raw / Sales . txt > ./ cleaned / Sales . csv ! csvcut - n ./ cleaned / Sales . csv # or !xsv headers ./cleaned/Sales.csv 1: POSSales_PK 2: Date_FK 3: Date 4: Store_FK 5: Item_FK 6: Item_PK 7: Promo_FK 8: POSSales_Ticket_No 9: POSSales_GiftList_No 10: POSSales_GiftListLine_No 11: POSSales_Sales_Quantity 12: POSSales_Sales_AmountExVAT 13: POSSales_Sales_AmountInVAT 14: POSSales_Cost_Amount 15: POSSales_Margin_Amount 16: POSSales_Discount_Amount 17: Store_No_BK 18: POS_Terminal_No_BK 19: Transaction_No_BK 20: Line_No_BK","title":"mlr rename"},{"location":"05_miller/#mlr-cat-head-tail","text":"mlr head and mlr tail count records rather than lines they always return the CSV header mlr head -n 5 myfile.csv will return 6 lines. ! mlr -- csv cat flights . csv | head mlr: unacceptable empty CSV key at file \"./raw/flights.csv\" line 1. This is a very common error. Caused becaused files generated on Unix-like systems have LF line terminators while RFC compliant CSVs have CRLF line terminators (default in miller) fix by including --rs lf this says that the record separator ( rs ) is lf . ! mlr -- csv -- rs lf head - n 5 flights . csv Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,SMF,ONT,389,4,11,0,,0,0,0,0,0,0 2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,SMF,PDX,479,5,6,0,,0,0,0,0,0,0 2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,SMF,PDX,479,6,9,0,,0,3,0,0,0,31 2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,SMF,PDX,479,3,8,0,,0,23,0,0,0,3 2007,1,1,1,831,830,957,1000,WN,2278,N480,86,90,74,-3,1,SMF,PDX,479,3,9,0,,0,0,0,0,0,0 read files with other delimiters by specifying ifs (or input field separator) could be useful for file format conversion ! mlr -- csv -- ifs '|' head - n 5 ./ raw / Sales . txt POSSales_PK,Date_FK,Date,Store_FK,Item_FK,Item_PK,Promo_FK,POSSales_Ticket No,POSSales_GiftList No,POSSales_GiftListLine No,POSSales_Sales Quantity,POSSales_Sales AmountExVAT,POSSales_Sales AmountInVAT,POSSales_Cost Amount,POSSales_Margin Amount,POSSales_Discount Amount,Store No_BK,POS Terminal No_BK,Transaction No_BK,Line No_BK 42169332,41639,2014-01-02 00:00:00,17,316213,20337325,806,3810195837,,0,\"1,00000000000000000000\",\"17,32231000000000000000\",\"20,96000000000000000000\",\"9,98000000000000000000\",\"7,34231000000000000000\",\"-8,99000000000000000000\",S038,P0381,226847,10000 42169333,41639,2014-01-02 00:00:00,17,274932,20194564,812,3810195837,,0,\"1,00000000000000000000\",\"43,38017000000000000000\",\"52,49000000000000000000\",\"35,86000000000000000000\",\"7,52017000000000000000\",\",00000000000000000000\",S038,P0381,226847,20000 42169334,41639,2014-01-02 00:00:00,17,326727,20347663,63,3810195838,,0,\"1,00000000000000000000\",\"6,60331000000000000000\",\"7,99000000000000000000\",\"5,21000000000000000000\",\"1,39331000000000000000\",\",00000000000000000000\",S038,P0381,226848,10000 42169335,41639,2014-01-02 00:00:00,17,311837,20332760,63,3810195838,,0,\"1,00000000000000000000\",\"24,78512000000000000000\",\"29,99000000000000000000\",\"12,23000000000000000000\",\"12,55512000000000000000\",\",00000000000000000000\",S038,P0381,226848,20000 42169336,41639,2014-01-02 00:00:00,17,262025,20181754,63,3810195838,,0,\"1,00000000000000000000\",\"12,38843000000000000000\",\"14,99000000000000000000\",\"6,34000000000000000000\",\"6,04843000000000000000\",\",00000000000000000000\",S038,P0381,226848,30000 # get the first record from every group that appears in the data ! mlr -- csv -- rs lf head - n 1 - g UniqueCarrier then cut - f Origin , Dest , UniqueCarrier , FlightNum flights . csv \\ | head | csvlook |----------------+-----------+--------+-------| | UniqueCarrier | FlightNum | Origin | Dest | |----------------+-----------+--------+-------| | WN | 2891 | SMF | ONT | | XE | 2809 | CLE | CLT | | YV | 2827 | ABQ | PHX | | OH | 5026 | SAT | CVG | | OO | 3664 | SUN | SLC | | UA | 1 | ORD | HNL | | US | 290 | ABQ | LAS | | DL | 1772 | ATL | PNS | | EV | 4083 | ATL | RDU | |----------------+-----------+--------+-------|","title":"mlr cat, head, tail"},{"location":"05_miller/#chaining","text":"Output of one verb may be chained as input to another using \"then\", e.g. mlr stats1 -a min,mean,max -f flag,u,v -g color then sort -f color","title":"Chaining"},{"location":"05_miller/#mlr-cat","text":"very useful for format conversion ( txt -> csv ) fast!! (under a minute for a 4GB txt file) and concatenating multiple same-schema CSV file mlr cat [options] Passes input records directly to output. Most useful for format conversion. Options: -n Prepend field \"n\" to each record with record-counter starting at 1 -g {comma-separated field name(s)} When used with -n/-N, writes record-counters keyed by specified field name(s). -N {name} Prepend field {name} to each record with record-counter starting at 1 import os os . chdir ( \"/home/data\" ) ! ls | grep kdd kddcup.data kddcup.names ! cat kddcup . names | sed 1 d | cut - d : - f1 duration protocol_type service flag src_bytes dst_bytes land wrong_fragment urgent hot num_failed_logins logged_in num_compromised root_shell su_attempted num_root num_file_creations num_shells num_access_files num_outbound_cmds is_host_login is_guest_login count srv_count serror_rate srv_serror_rate rerror_rate srv_rerror_rate same_srv_rate diff_srv_rate srv_diff_host_rate dst_host_count dst_host_srv_count dst_host_same_srv_rate dst_host_diff_srv_rate dst_host_same_src_port_rate dst_host_srv_diff_host_rate dst_host_serror_rate dst_host_srv_serror_rate dst_host_rerror_rate dst_host_srv_rerror_rate # converting pipe-delimited to csv ! mlr -- csv -- ifs '|' cat ./ raw / Sales . txt > ./ cleaned / Sales . csv ! mlr -- csv head - n 5 ./ cleaned / Sales . csv POSSales_PK,Date_FK,Date,Store_FK,Item_FK,Item_PK,Promo_FK,POSSales_Ticket No,POSSales_GiftList No,POSSales_GiftListLine No,POSSales_Sales Quantity,POSSales_Sales AmountExVAT,POSSales_Sales AmountInVAT,POSSales_Cost Amount,POSSales_Margin Amount,POSSales_Discount Amount,Store No_BK,POS Terminal No_BK,Transaction No_BK,Line No_BK 42169332,41639,2014-01-02 00:00:00,17,316213,20337325,806,3810195837,,0,\"1,00000000000000000000\",\"17,32231000000000000000\",\"20,96000000000000000000\",\"9,98000000000000000000\",\"7,34231000000000000000\",\"-8,99000000000000000000\",S038,P0381,226847,10000 42169333,41639,2014-01-02 00:00:00,17,274932,20194564,812,3810195837,,0,\"1,00000000000000000000\",\"43,38017000000000000000\",\"52,49000000000000000000\",\"35,86000000000000000000\",\"7,52017000000000000000\",\",00000000000000000000\",S038,P0381,226847,20000 42169334,41639,2014-01-02 00:00:00,17,326727,20347663,63,3810195838,,0,\"1,00000000000000000000\",\"6,60331000000000000000\",\"7,99000000000000000000\",\"5,21000000000000000000\",\"1,39331000000000000000\",\",00000000000000000000\",S038,P0381,226848,10000 42169335,41639,2014-01-02 00:00:00,17,311837,20332760,63,3810195838,,0,\"1,00000000000000000000\",\"24,78512000000000000000\",\"29,99000000000000000000\",\"12,23000000000000000000\",\"12,55512000000000000000\",\",00000000000000000000\",S038,P0381,226848,20000 42169336,41639,2014-01-02 00:00:00,17,262025,20181754,63,3810195838,,0,\"1,00000000000000000000\",\"12,38843000000000000000\",\"14,99000000000000000000\",\"6,34000000000000000000\",\"6,04843000000000000000\",\",00000000000000000000\",S038,P0381,226848,30000 # Create an index and extract a row from a specific index ! mlr -- csv -- rs lf head - n 10 then cat - n then filter '$n==2' flights . csv n,Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2,2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,SMF,PDX,479,6,9,0,,0,3,0,0,0,31 # Even numbered rows (creating an index isnt necessary) ! mlr -- csv -- rs lf head - n 10 then filter 'FNR%2==0' flights . csv Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,SMF,PDX,479,5,6,0,,0,0,0,0,0,0 2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,SMF,PDX,479,3,8,0,,0,23,0,0,0,3 2007,1,1,1,1430,1420,1553,1550,WN,2386,N611SW,83,90,74,3,10,SMF,PDX,479,2,7,0,,0,0,0,0,0,0 2007,1,1,1,944,935,1223,1225,WN,1131,N749SW,99,110,86,-2,9,SMF,PHX,647,4,9,0,,0,0,0,0,0,0 2007,1,1,1,1318,1315,1603,1610,WN,2456,N630WN,105,115,92,-7,3,SMF,PHX,647,5,8,0,,0,0,0,0,0,0 #provide implicit header (auto numeric) to headerless files ! cat flights . csv | sed 1 d | cut - d , - f1 - 10 | head \\ | mlr -- csv -- rs lf -- implicit - csv - header cat \\ | csvlook |-------+---+---+---+------+------+------+------+----+-------| | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | |-------+---+---+---+------+------+------+------+----+-------| | 2007 | 1 | 1 | 1 | 1232 | 1225 | 1341 | 1340 | WN | 2891 | | 2007 | 1 | 1 | 1 | 1918 | 1905 | 2043 | 2035 | WN | 462 | | 2007 | 1 | 1 | 1 | 2206 | 2130 | 2334 | 2300 | WN | 1229 | | 2007 | 1 | 1 | 1 | 1230 | 1200 | 1356 | 1330 | WN | 1355 | | 2007 | 1 | 1 | 1 | 831 | 830 | 957 | 1000 | WN | 2278 | | 2007 | 1 | 1 | 1 | 1430 | 1420 | 1553 | 1550 | WN | 2386 | | 2007 | 1 | 1 | 1 | 1936 | 1840 | 2217 | 2130 | WN | 409 | | 2007 | 1 | 1 | 1 | 944 | 935 | 1223 | 1225 | WN | 1131 | | 2007 | 1 | 1 | 1 | 1537 | 1450 | 1819 | 1735 | WN | 1212 | | 2007 | 1 | 1 | 1 | 1318 | 1315 | 1603 | 1610 | WN | 2456 | |-------+---+---+---+------+------+------+------+----+-------| cut: write error: Broken pipe # provide column names (if less than num_cols, the implicit names will be kept) ! cat flights . csv | sed 1 d | cut - d , - f1 - 10 | head \\ | mlr -- csv -- rs lf -- implicit - csv - header label a , b , c , d \\ | csvlook |-------+---+---+---+------+------+------+------+----+-------| | a | b | c | d | 5 | 6 | 7 | 8 | 9 | 10 | |-------+---+---+---+------+------+------+------+----+-------| | 2007 | 1 | 1 | 1 | 1232 | 1225 | 1341 | 1340 | WN | 2891 | | 2007 | 1 | 1 | 1 | 1918 | 1905 | 2043 | 2035 | WN | 462 | | 2007 | 1 | 1 | 1 | 2206 | 2130 | 2334 | 2300 | WN | 1229 | | 2007 | 1 | 1 | 1 | 1230 | 1200 | 1356 | 1330 | WN | 1355 | | 2007 | 1 | 1 | 1 | 831 | 830 | 957 | 1000 | WN | 2278 | | 2007 | 1 | 1 | 1 | 1430 | 1420 | 1553 | 1550 | WN | 2386 | | 2007 | 1 | 1 | 1 | 1936 | 1840 | 2217 | 2130 | WN | 409 | | 2007 | 1 | 1 | 1 | 944 | 935 | 1223 | 1225 | WN | 1131 | | 2007 | 1 | 1 | 1 | 1537 | 1450 | 1819 | 1735 | WN | 1212 | | 2007 | 1 | 1 | 1 | 1318 | 1315 | 1603 | 1610 | WN | 2456 | |-------+---+---+---+------+------+------+------+----+-------| cut: write error: Broken pipe","title":"mlr cat"},{"location":"05_miller/#mlr-cut","text":"select columns by name with -f select all columns except some with -x # Print only Origin,Dest ! head - n 10 flights . csv \\ | mlr -- csv -- rs lf cut - f Origin , Dest \\ | csvlook |---------+-------| | Origin | Dest | |---------+-------| | SMF | ONT | | SMF | PDX | | SMF | PDX | | SMF | PDX | | SMF | PDX | | SMF | PDX | | SMF | PHX | | SMF | PHX | | SMF | PHX | |---------+-------| # print all except Origin, Dest ! head - n 5 flights . csv \\ | mlr -- csv -- rs lf cut - x - f Origin , Dest Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,389,4,11,0,,0,0,0,0,0,0 2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,479,5,6,0,,0,0,0,0,0,0 2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,479,6,9,0,,0,3,0,0,0,31 2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,479,3,8,0,,0,23,0,0,0,3 ! mlr -- csv head - n 10 then cut - r - f \"Amount\" ./ cleaned / Sales . csv | xsv headers 1 POSSales_Sales_AmountExVAT 2 POSSales_Sales_AmountInVAT 3 POSSales_Cost_Amount 4 POSSales_Margin_Amount 5 POSSales_Discount_Amount ! mlr -- csv head - n 10 then cut - r - f \"Quantity\" ./ cleaned / Sales . csv | xsv headers 1 POSSales_Sales_Quantity ! mlr -- csv head - n 10 then having - fields -- all - matching 'Quantity' ./ cleaned / Sales . csv","title":"mlr cut"},{"location":"05_miller/#mlr-filter","text":"retain specific records Examples: mlr filter 'log10($count) > 4.0' mlr filter 'FNR == 2' mlr filter 'urand() < 0.001' mlr filter '$color != \"blue\" && $value > 4.2' mlr filter '($x<.5 && $y<.5) || ($x>.5 && $y>.5)' mlr filter '($name =~ \"^sys.*east$\") || ($name =~ \"^dev.[0-9]+\"i)' mlr filter '$ab = $a+$b; $cd = $c+$d; $ab != $cd' # single condition ! mlr -- csv -- rs lf filter '$Origin == \"SFO\"' flights . csv \\ | csvcut - c Origin , Dest | head | csvlook |---------+-------| | Origin | Dest | |---------+-------| | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | | SFO | PHX | |---------+-------| # compound logic ! mlr -- csv -- rs lf filter '$Origin == \"SFO\" && $Dest == \"DFW\"' flights . csv \\ | csvcut - c UniqueCarrier , FlightNum , Origin , Dest | head | csvlook |----------------+-----------+--------+-------| | UniqueCarrier | FlightNum | Origin | Dest | |----------------+-----------+--------+-------| | UA | 136 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | | UA | 336 | SFO | DFW | |----------------+-----------+--------+-------|","title":"mlr filter"},{"location":"05_miller/#mlr-put","text":"derive new columns on the fly from existing ones Adds/updates specified field(s). Expressions are semicolon-separated and must either be assignments, or evaluate to boolean. Please use a dollar sign for field names and double-quotes for string literals. Miller built-in variables are NF NR FNR FILENUM FILENAME PI E, and ENV Examples: mlr put '$y = log10($x); $z = sqrt($y)' mlr put '$x>0.0 { $y=log10($x); $z=sqrt($y) }' # does {...} only if $x > 0.0 mlr put '$x>0.0; $y=log10($x); $z=sqrt($y)' # does all three statements mlr put '$a =~ \"([a-z]+)_([0-9]+); $b = \"left_\\1\"; $c = \"right_\\2\"' mlr put '$a =~ \"([a-z]+)_([0-9]+) { $b = \"left_\\1\"; $c = \"right_\\2\" }' mlr put '$filename = FILENAME' mlr put '$colored_shape = $color . \"_\" . $shape' mlr put '$y = cos($theta); $z = atan2($y, $x)' mlr put '$name = sub($name, \"http.*com\"i, \"\")' mlr put -q '@sum += $x; end {emit @sum}' mlr put -q '@sum[$a] += $x; end {emit @sum, \"a\"}' mlr put -q '@sum[$a][$b] += $x; end {emit @sum, \"a\", \"b\"}' mlr put -q '@min=min(@min,$x);@max=max(@max,$x); end{emitf @min, @max}' mlr put -q 'is_null(@xmax) || $x > @xmax {@xmax=$x; @recmax=$*}; end {emit @recmax}' ! csvcut - c Origin , Dest , Distance flights . csv | head \\ | mlr -- csv -- rs lf put '$Distance_2 = $Distance/100;' \\ | csvlook |---------+------+----------+-------------| | Origin | Dest | Distance | Distance_2 | |---------+------+----------+-------------| | SMF | ONT | 389 | 3.890000 | | SMF | PDX | 479 | 4.790000 | | SMF | PDX | 479 | 4.790000 | | SMF | PDX | 479 | 4.790000 | | SMF | PDX | 479 | 4.790000 | | SMF | PDX | 479 | 4.790000 | | SMF | PHX | 647 | 6.470000 | | SMF | PHX | 647 | 6.470000 | | SMF | PHX | 647 | 6.470000 | |---------+------+----------+-------------| ! cat ./ raw / het - bool . csv name,reachable barney,false betty,true fred,true wilma,1 ! mlr -- icsv -- rs lf -- opprint put '$reachable = boolean($reachable)' ./ raw / het - bool . csv name reachable barney false betty true fred true wilma true ! mlr -- icsv -- rs lf -- opprint put '$reachable = float(boolean($reachable))' ./ raw / het - bool . csv name reachable barney 0.000000 betty 1.000000 fred 1.000000 wilma 1.000000 # Creating an index field ! mlr -- icsv -- rs lf -- opprint put '$index = NR' ./ raw / het - bool . csv name reachable index barney false 1 betty true 2 fred true 3 wilma 1 4","title":"mlr put"},{"location":"05_miller/#functions-to-use-with-put-filter","text":"! mlr - F | tr ' \\n ' ' \\t ' + + - - * / // % ** | ^ & ~ << >> == != =~ !=~ > >= < <= && || ^^ ! ? : . gsub strlen sub substr tolower toupper abs acos acosh asin asinh atan atan2 atanh cbrt ceil cos cosh erf erfc exp expm1 floor invqnorm log log10 log1p logifit madd max mexp min mmul msub pow qnorm round roundm sgn sin sinh sqrt tan tanh urand urand32 urandint dhms2fsec dhms2sec fsec2dhms fsec2hms gmt2sec hms2fsec hms2sec sec2dhms sec2gmt sec2gmt sec2gmtdate sec2hms strftime strptime systime is_absent is_bool is_boolean is_empty is_empty_map is_float is_int is_map is_nonempty_map is_not_empty is_not_map is_not_null is_null is_numeric is_present is_string asserting_absent asserting_bool asserting_boolean asserting_empty asserting_empty_map asserting_float asserting_int asserting_map asserting_nonempty_map asserting_not_empty asserting_not_map asserting_not_null asserting_null asserting_numeric asserting_present asserting_string boolean float fmtnum hexfmt int string typeof depth haskey joink joinkv joinv leafcount length mapdiff mapsum splitkv splitkvx splitnv splitnvx ! mlr - f + (class=arithmetic #args=2): Addition. + (class=arithmetic #args=1): Unary plus. - (class=arithmetic #args=2): Subtraction. - (class=arithmetic #args=1): Unary minus. * (class=arithmetic #args=2): Multiplication. / (class=arithmetic #args=2): Division. // (class=arithmetic #args=2): Integer division: rounds to negative (pythonic). % (class=arithmetic #args=2): Remainder; never negative-valued (pythonic). ** (class=arithmetic #args=2): Exponentiation; same as pow, but as an infix operator. | (class=arithmetic #args=2): Bitwise OR. ^ (class=arithmetic #args=2): Bitwise XOR. & (class=arithmetic #args=2): Bitwise AND. ~ (class=arithmetic #args=1): Bitwise NOT. Beware '$y=~$x' since =~ is the regex-match operator: try '$y = ~$x'. << (class=arithmetic #args=2): Bitwise left-shift. >> (class=arithmetic #args=2): Bitwise right-shift. == (class=boolean #args=2): String/numeric equality. Mixing number and string results in string compare. != (class=boolean #args=2): String/numeric inequality. Mixing number and string results in string compare. =~ (class=boolean #args=2): String (left-hand side) matches regex (right-hand side), e.g. '$name =~ \"^a.*b$\"'. !=~ (class=boolean #args=2): String (left-hand side) does not match regex (right-hand side), e.g. '$name !=~ \"^a.*b$\"'. > (class=boolean #args=2): String/numeric greater-than. Mixing number and string results in string compare. >= (class=boolean #args=2): String/numeric greater-than-or-equals. Mixing number and string results in string compare. < (class=boolean #args=2): String/numeric less-than. Mixing number and string results in string compare. <= (class=boolean #args=2): String/numeric less-than-or-equals. Mixing number and string results in string compare. && (class=boolean #args=2): Logical AND. || (class=boolean #args=2): Logical OR. ^^ (class=boolean #args=2): Logical XOR. ! (class=boolean #args=1): Logical negation. ? : (class=boolean #args=3): Ternary operator. . (class=string #args=2): String concatenation. gsub (class=string #args=3): Example: '$name=gsub($name, \"old\", \"new\")' (replace all). strlen (class=string #args=1): String length. sub (class=string #args=3): Example: '$name=sub($name, \"old\", \"new\")' (replace once). substr (class=string #args=3): substr(s,m,n) gives substring of s from 0-up position m to n inclusive. Negative indices -len .. -1 alias to 0 .. len-1. tolower (class=string #args=1): Convert string to lowercase. toupper (class=string #args=1): Convert string to uppercase. abs (class=math #args=1): Absolute value. acos (class=math #args=1): Inverse trigonometric cosine. acosh (class=math #args=1): Inverse hyperbolic cosine. asin (class=math #args=1): Inverse trigonometric sine. asinh (class=math #args=1): Inverse hyperbolic sine. atan (class=math #args=1): One-argument arctangent. atan2 (class=math #args=2): Two-argument arctangent. atanh (class=math #args=1): Inverse hyperbolic tangent. cbrt (class=math #args=1): Cube root. ceil (class=math #args=1): Ceiling: nearest integer at or above. cos (class=math #args=1): Trigonometric cosine. cosh (class=math #args=1): Hyperbolic cosine. erf (class=math #args=1): Error function. erfc (class=math #args=1): Complementary error function. exp (class=math #args=1): Exponential function e**x. expm1 (class=math #args=1): e**x - 1. floor (class=math #args=1): Floor: nearest integer at or below. invqnorm (class=math #args=1): Inverse of normal cumulative distribution function. Note that invqorm(urand()) is normally distributed. log (class=math #args=1): Natural (base-e) logarithm. log10 (class=math #args=1): Base-10 logarithm. log1p (class=math #args=1): log(1-x). logifit (class=math #args=3): Given m and b from logistic regression, compute fit: $yhat=logifit($x,$m,$b). madd (class=math #args=3): a + b mod m (integers) max (class=math variadic): max of n numbers; null loses mexp (class=math #args=3): a ** b mod m (integers) min (class=math variadic): Min of n numbers; null loses mmul (class=math #args=3): a * b mod m (integers) msub (class=math #args=3): a - b mod m (integers) pow (class=math #args=2): Exponentiation; same as **. qnorm (class=math #args=1): Normal cumulative distribution function. round (class=math #args=1): Round to nearest integer. roundm (class=math #args=2): Round to nearest multiple of m: roundm($x,$m) is the same as round($x/$m)*$m sgn (class=math #args=1): +1 for positive input, 0 for zero input, -1 for negative input. sin (class=math #args=1): Trigonometric sine. sinh (class=math #args=1): Hyperbolic sine. sqrt (class=math #args=1): Square root. tan (class=math #args=1): Trigonometric tangent. tanh (class=math #args=1): Hyperbolic tangent. urand (class=math #args=0): Floating-point numbers on the unit interval. Int-valued example: '$n=floor(20+urand()*11)'. urand32 (class=math #args=0): Integer uniformly distributed 0 and 2**32-1 inclusive. urandint (class=math #args=2): Integer uniformly distributed between inclusive integer endpoints. dhms2fsec (class=time #args=1): Recovers floating-point seconds as in dhms2fsec(\"5d18h53m20.250000s\") = 500000.250000 dhms2sec (class=time #args=1): Recovers integer seconds as in dhms2sec(\"5d18h53m20s\") = 500000 fsec2dhms (class=time #args=1): Formats floating-point seconds as in fsec2dhms(500000.25) = \"5d18h53m20.250000s\" fsec2hms (class=time #args=1): Formats floating-point seconds as in fsec2hms(5000.25) = \"01:23:20.250000\" gmt2sec (class=time #args=1): Parses GMT timestamp as integer seconds since the epoch. hms2fsec (class=time #args=1): Recovers floating-point seconds as in hms2fsec(\"01:23:20.250000\") = 5000.250000 hms2sec (class=time #args=1): Recovers integer seconds as in hms2sec(\"01:23:20\") = 5000 sec2dhms (class=time #args=1): Formats integer seconds as in sec2dhms(500000) = \"5d18h53m20s\" sec2gmt (class=time #args=1): Formats seconds since epoch (integer part) as GMT timestamp, e.g. sec2gmt(1440768801.7) = \"2015-08-28T13:33:21Z\". Leaves non-numbers as-is. sec2gmt (class=time #args=2): Formats seconds since epoch as GMT timestamp with n decimal places for seconds, e.g. sec2gmt(1440768801.7,1) = \"2015-08-28T13:33:21.7Z\". Leaves non-numbers as-is. sec2gmtdate (class=time #args=1): Formats seconds since epoch (integer part) as GMT timestamp with year-month-date, e.g. sec2gmtdate(1440768801.7) = \"2015-08-28\". Leaves non-numbers as-is. sec2hms (class=time #args=1): Formats integer seconds as in sec2hms(5000) = \"01:23:20\" strftime (class=time #args=2): Formats seconds since the epoch as timestamp, e.g. strftime(1440768801.7,\"%Y-%m-%dT%H:%M:%SZ\") = \"2015-08-28T13:33:21Z\", and strftime(1440768801.7,\"%Y-%m-%dT%H:%M:%3SZ\") = \"2015-08-28T13:33:21.700Z\". Format strings are as in the C library (please see \"man strftime\" on your system), with the Miller-specific addition of \"%1S\" through \"%9S\" which format the seocnds with 1 through 9 decimal places, respectively. (\"%S\" uses no decimal places.) strptime (class=time #args=2): Parses timestamp as floating-point seconds since the epoch, e.g. strptime(\"2015-08-28T13:33:21Z\",\"%Y-%m-%dT%H:%M:%SZ\") = 1440768801.000000, and strptime(\"2015-08-28T13:33:21.345Z\",\"%Y-%m-%dT%H:%M:%SZ\") = 1440768801.345000. systime (class=time #args=0): Floating-point seconds since the epoch, e.g. 1440768801.748936. is_absent (class=typing #args=1): False if field is present in input, false otherwise is_bool (class=typing #args=1): True if field is present with boolean value. Synonymous with is_boolean. is_boolean (class=typing #args=1): True if field is present with boolean value. Synonymous with is_bool. is_empty (class=typing #args=1): True if field is present in input with empty string value, false otherwise. is_empty_map (class=typing #args=1): True if argument is a map which is empty. is_float (class=typing #args=1): True if field is present with value inferred to be float is_int (class=typing #args=1): True if field is present with value inferred to be int is_map (class=typing #args=1): True if argument is a map. is_nonempty_map (class=typing #args=1): True if argument is a map which is non-empty. is_not_empty (class=typing #args=1): False if field is present in input with empty value, false otherwise is_not_map (class=typing #args=1): True if argument is not a map. is_not_null (class=typing #args=1): False if argument is null (empty or absent), true otherwise. is_null (class=typing #args=1): True if argument is null (empty or absent), false otherwise. is_numeric (class=typing #args=1): True if field is present with value inferred to be int or float is_present (class=typing #args=1): True if field is present in input, false otherwise. is_string (class=typing #args=1): True if field is present with string (including empty-string) value asserting_absent (class=typing #args=1): Returns argument if it is absent in the input data, else throws an error. asserting_bool (class=typing #args=1): Returns argument if it is present with boolean value, else throws an error. asserting_boolean (class=typing #args=1): Returns argument if it is present with boolean value, else throws an error. asserting_empty (class=typing #args=1): Returns argument if it is present in input with empty value, else throws an error. asserting_empty_map (class=typing #args=1): Returns argument if it is a map with empty value, else throws an error. asserting_float (class=typing #args=1): Returns argument if it is present with float value, else throws an error. asserting_int (class=typing #args=1): Returns argument if it is present with int value, else throws an error. asserting_map (class=typing #args=1): Returns argument if it is a map, else throws an error. asserting_nonempty_map (class=typing #args=1): Returns argument if it is a non-empty map, else throws an error. asserting_not_empty (class=typing #args=1): Returns argument if it is present in input with non-empty value, else throws an error. asserting_not_map (class=typing #args=1): Returns argument if it is not a map, else throws an error. asserting_not_null (class=typing #args=1): Returns argument if it is non-null (non-empty and non-absent), else throws an error. asserting_null (class=typing #args=1): Returns argument if it is null (empty or absent), else throws an error. asserting_numeric (class=typing #args=1): Returns argument if it is present with int or float value, else throws an error. asserting_present (class=typing #args=1): Returns argument if it is present in input, else throws an error. asserting_string (class=typing #args=1): Returns argument if it is present with string (including empty-string) value, else throws an error. boolean (class=conversion #args=1): Convert int/float/bool/string to boolean. float (class=conversion #args=1): Convert int/float/bool/string to float. fmtnum (class=conversion #args=2): Convert int/float/bool to string using printf-style format string, e.g. '$s = fmtnum($n, \"%06lld\")'. hexfmt (class=conversion #args=1): Convert int to string, e.g. 255 to \"0xff\". int (class=conversion #args=1): Convert int/float/bool/string to int. string (class=conversion #args=1): Convert int/float/bool/string to string. typeof (class=conversion #args=1): Convert argument to type of argument (e.g. MT_STRING). For debug. depth (class=maps #args=1): Prints maximum depth of hashmap: ''. Scalars have depth 0. haskey (class=maps #args=2): True/false if map has/hasn't key, e.g. 'haskey($*, \"a\")' or 'haskey(mymap, mykey)'. Error if 1st argument is not a map. joink (class=maps #args=2): Makes string from map keys. E.g. 'joink($*, \",\")'. joinkv (class=maps #args=3): Makes string from map key-value pairs. E.g. 'joinkv(@v[2], \"=\", \",\")' joinv (class=maps #args=2): Makes string from map keys. E.g. 'joinv(mymap, \",\")'. leafcount (class=maps #args=1): Counts total number of terminal values in hashmap. For single-level maps, same as length. length (class=maps #args=1): Counts number of top-level entries in hashmap. Scalars have length 1. mapdiff (class=maps variadic): With 0 args, returns empty map. With 1 arg, returns copy of arg. With 2 or more, returns copy of arg 1 with all keys from any of remaining argument maps removed. mapsum (class=maps variadic): With 0 args, returns empty map. With >= 1 arg, returns a map with key-value pairs from all arguments. Rightmost collisions win, e.g. 'mapsum({1:2,3,4},{1:5})' is '{1:5,3:4}'. splitkv (class=maps #args=3): Splits string by separators into map with type inference. E.g. 'splitkv(\"a=1,b=2,c=3\", \"=\", \",\")' gives '{\"a\" : 1, \"b\" : 2, \"c\" : 3}'. splitkvx (class=maps #args=3): Splits string by separators into map without type inference (keys and values are strings). E.g. 'splitkv(\"a=1,b=2,c=3\", \"=\", \",\")' gives '{\"a\" : \"1\", \"b\" : \"2\", \"c\" : \"3\"}'. splitnv (class=maps #args=2): Splits string by separator into integer-indexed map with type inference. E.g. 'splitnv(\"a,b,c\" , \",\")' gives '{1 : \"a\", 2 : \"b\", 3 : \"c\"}'. splitnvx (class=maps #args=2): Splits string by separator into integer-indexed map without type inference (values are strings). E.g. 'splitnv(\"4,5,6\" , \",\")' gives '{1 : \"4\", 2 : \"5\", 3 : \"6\"}'. To set the seed for urand, you may specify decimal or hexadecimal 32-bit numbers of the form \"mlr --seed 123456789\" or \"mlr --seed 0xcafefeed\". Miller's built-in variables are NF, NR, FNR, FILENUM, and FILENAME (awk-like) along with the mathematical constants PI and E. ! xsv headers ./ cleaned / Sales . csv 1 POSSales_PK 2 Date_FK 3 Date 4 Store_FK 5 Item_FK 6 Item_PK 7 Promo_FK 8 POSSales_Ticket_No 9 POSSales_GiftList_No 10 POSSales_GiftListLine_No 11 POSSales_Sales_Quantity 12 POSSales_Sales_AmountExVAT 13 POSSales_Sales_AmountInVAT 14 POSSales_Cost_Amount 15 POSSales_Margin_Amount 16 POSSales_Discount_Amount 17 Store_No_BK 18 POS_Terminal_No_BK 19 Transaction_No_BK 20 Line_No_BK ! mlr -- csv head - n 10 \\ then put '$POSSales_Sales_AmountExVAT2=float(gsub($POSSales_Sales_AmountExVAT,\",\",\".\"))' \\ then cut - r - f 'POSSales_Sales_AmountExVAT' ./ cleaned / Sales . csv POSSales_Sales_AmountExVAT,POSSales_Sales_AmountExVAT2 \"17,32231000000000000000\",17.322310 \"43,38017000000000000000\",43.380170 \"6,60331000000000000000\",6.603310 \"24,78512000000000000000\",24.785120 \"12,38843000000000000000\",12.388430 \",94340000000000000000\",0.943400 \"2,22314000000000000000\",2.223140 \"10,70248000000000000000\",10.702480 \"3,29752000000000000000\",3.297520 \"28,91736000000000000000\",28.917360","title":"Functions to use with put, filter"},{"location":"05_miller/#sec2gmt-sec2gmtdate-for-epoch-timestamps","text":"Replaces a numeric field representing seconds since the epoch with the corresponding GMT timestamp mlr sec2gmt time1,time2 # is the same as mlr put '$time1=sec2gmt($time1);$time2=sec2gmt($time2)' Alternatively, use the following functions with put strftime: Formats seconds since epoch (integer part) as timestamp, e.g. strftime(1440768801.7,\"%Y-%m-%dT%H:%M:%SZ\") = \"2015-08-28T13:33:21Z\". strptime: Parses timestamp as integer seconds since epoch, e.g. strptime(\"2015-08-28T13:33:21Z\",\"%Y-%m-%dT%H:%M:%SZ\") = 1440768801. ! mlr -- csv head - n 10 then cut - f 'Date' ./ cleaned / Sales . csv Date 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 2014-01-02 00:00:00 ! mlr -- csv head - n 10 \\ then cut - f 'Date' \\ then put '$epoch=strptime($Date, \"%Y-%m- %d %H:%M:%S\")' ./ cleaned / Sales . csv \\ | csvlook |----------------------+-------------| | Date | epoch | |----------------------+-------------| | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | | 2014-01-02 00:00:00 | 1388620800 | |----------------------+-------------|","title":"sec2gmt, sec2gmtdate for epoch timestamps"},{"location":"05_miller/#mlr-label","text":"use --implicit-csv-header with mlr cat to auto-create numeric column names create column labels for files using mlr <options> label <colnames> ! csvcut - c Origin , Dest , FlightNum flights . csv | head | sed 1 d \\ | mlr -- csv -- rs lf label col1 , col2 , col3 col1,col2,col3 SMF,PDX,462 SMF,PDX,1229 SMF,PDX,1355 SMF,PDX,2278 SMF,PDX,2386 SMF,PHX,409 SMF,PHX,1131 SMF,PHX,1212","title":"mlr label"},{"location":"05_miller/#mlr-rename_1","text":"Renames specified fields. Usage: mlr rename [options] {old1,new1,old2,new2,...} use -g for global replacement, and -r for regex matching Examples: mlr rename old_name,new_name mlr rename old_name_1,new_name_1,old_name_2,new_name_2 mlr rename -r 'Date_[0-9]+,Date,' Rename all such fields to be \"Date\" mlr rename -r '\"Date_[0-9]+\",Date' Same mlr rename -r 'Date_([0-9]+).*,\\1' Rename all such fields to be of the form 20151015 mlr rename -r '\"name\"i,Name' Rename \"name\", \"Name\", \"NAME\", etc. to \"Name\" # replace spaces with underscores ! mlr -- csv -- ifs '|' -- ofs ',' rename - g - r ' ,_' ./ raw / Sales . txt > ./ cleaned / Sales . csv ! csvcut - n ./ cleaned / Sales . csv # or !xsv headers ./cleaned/Sales.csv 1: POSSales_PK 2: Date_FK 3: Date 4: Store_FK 5: Item_FK 6: Item_PK 7: Promo_FK 8: POSSales_Ticket_No 9: POSSales_GiftList_No 10: POSSales_GiftListLine_No 11: POSSales_Sales_Quantity 12: POSSales_Sales_AmountExVAT 13: POSSales_Sales_AmountInVAT 14: POSSales_Cost_Amount 15: POSSales_Margin_Amount 16: POSSales_Discount_Amount 17: Store_No_BK 18: POS_Terminal_No_BK 19: Transaction_No_BK 20: Line_No_BK","title":"mlr rename"},{"location":"05_miller/#random-sampling-with-bootstrap-sample-shuffle","text":"bootstrap is for sampling with replacement The canonical use for bootstrap sampling is to put error bars (or, confints ) on statistical quantities, such as the mean. ! mlr -- csv -- rs lf bootstrap - n 100 then stats1 - a mean - f WeatherDelay - g UniqueCarrier flights . csv UniqueCarrier,WeatherDelay_mean NW,0.000000 OH,2.000000 DL,0.000000 9E,0.000000 AA,0.000000 WN,0.000000 UA,0.000000 FL,0.000000 OO,0.000000 EV,46.833333 US,0.000000 YV,0.000000 MQ,0.000000 XE,0.000000 AS,0.000000 CO,0.000000 AQ,0.000000 HA,0.000000 B6,0.000000 sample is for sampling -k records without replacement Stratified if groups are specified with a -g switch ! mlr -- csv -- rs lf sample - k 4 flights . csv Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay 2007,4,20,5,3,2353,221,209,US,19,N601AW,138,136,116,12,10,LAS,PDX,762,3,19,0,,0,0,0,0,0,0 2007,3,21,3,902,900,1242,1153,YV,7176,N37342,160,113,93,49,2,ORD,CAE,666,4,63,0,,0,49,0,0,0,0 2007,8,3,5,2115,2028,2248,2218,FL,59,N932AT,93,110,72,30,47,IAD,ATL,533,9,12,0,,0,0,0,0,0,30 2007,10,12,5,810,820,947,1005,AA,1895,N5FHAA,157,165,139,-18,-10,MCO,DFW,984,6,12,0,,0,0,0,0,0,0","title":"Random Sampling with bootstrap, sample, shuffle"},{"location":"05_miller/#mlr-count-distinct","text":"! mlr -- csv -- rs lf count - distinct - f Origin , Dest then sort - nr count then head - n 10 flights . csv | csvlook |---------+------+--------| | Origin | Dest | count | |---------+------+--------| | OGG | HNL | 16099 | | HNL | OGG | 15876 | | LAX | LAS | 14385 | | LAS | LAX | 13815 | | HNL | LIH | 13156 | | LIH | HNL | 13030 | | SAN | LAX | 12779 | | LAX | SAN | 12767 | | BOS | LGA | 12263 | | LAS | PHX | 12228 | |---------+------+--------| ! cat flights . csv | cut - d , - f17 | sed 1 d | sort | uniq | wc - l 304","title":"mlr count-distinct"},{"location":"05_miller/#mlr-merge-fields","text":"like mlr stats1 but all accumulation is done across fields within each given record: horizontal rather than vertical statistics ! mlr -- csv -- rs lf head - n 10 then cut - r - f \"Time\" flights . csv | csvlook |----------+------------+---------+------------+-------------------+----------------+----------| | DepTime | CRSDepTime | ArrTime | CRSArrTime | ActualElapsedTime | CRSElapsedTime | AirTime | |----------+------------+---------+------------+-------------------+----------------+----------| | 1232 | 1225 | 1341 | 1340 | 69 | 75 | 54 | | 1918 | 1905 | 2043 | 2035 | 85 | 90 | 74 | | 2206 | 2130 | 2334 | 2300 | 88 | 90 | 73 | | 1230 | 1200 | 1356 | 1330 | 86 | 90 | 75 | | 831 | 830 | 957 | 1000 | 86 | 90 | 74 | | 1430 | 1420 | 1553 | 1550 | 83 | 90 | 74 | | 1936 | 1840 | 2217 | 2130 | 101 | 110 | 89 | | 944 | 935 | 1223 | 1225 | 99 | 110 | 86 | | 1537 | 1450 | 1819 | 1735 | 102 | 105 | 90 | | 1318 | 1315 | 1603 | 1610 | 105 | 115 | 92 | |----------+------------+---------+------------+-------------------+----------------+----------| ! mlr -- csv -- rs lf head - n 10 then \\ cut - r - f \"Time\" then \\ merge - fields - a sum - r 'Time' - k - o time \\ flights . csv | csvlook |----------+------------+---------+------------+-------------------+----------------+---------+-----------| | DepTime | CRSDepTime | ArrTime | CRSArrTime | ActualElapsedTime | CRSElapsedTime | AirTime | time_sum | |----------+------------+---------+------------+-------------------+----------------+---------+-----------| | 1232 | 1225 | 1341 | 1340 | 69 | 75 | 54 | 5336 | | 1918 | 1905 | 2043 | 2035 | 85 | 90 | 74 | 8150 | | 2206 | 2130 | 2334 | 2300 | 88 | 90 | 73 | 9221 | | 1230 | 1200 | 1356 | 1330 | 86 | 90 | 75 | 5367 | | 831 | 830 | 957 | 1000 | 86 | 90 | 74 | 3868 | | 1430 | 1420 | 1553 | 1550 | 83 | 90 | 74 | 6200 | | 1936 | 1840 | 2217 | 2130 | 101 | 110 | 89 | 8423 | | 944 | 935 | 1223 | 1225 | 99 | 110 | 86 | 4622 | | 1537 | 1450 | 1819 | 1735 | 102 | 105 | 90 | 6838 | | 1318 | 1315 | 1603 | 1610 | 105 | 115 | 92 | 6158 | |----------+------------+---------+------------+-------------------+----------------+---------+-----------|","title":"mlr merge-fields"},{"location":"05_miller/#stats1","text":"Computes univariate statistics for one or more given fields, accumulated across the input record stream. switch -f {a,b,c} Value-field names on which to compute statistics switch -g {d,e,f} Optional group-by-field names switch -a {g,h,i} with the listed stats count Count instances of fields mode Find most-frequently-occurring values for fields; first-found wins tie antimode Find least-frequently-occurring values for fields; first-found wins tie sum Compute sums of specified fields mean Compute averages (sample means) of specified fields stddev Compute sample standard deviation of specified fields var Compute sample variance of specified fields meaneb Estimate error bars for averages (assuming no sample autocorrelation) skewness Compute sample skewness of specified fields kurtosis Compute sample kurtosis of specified fields min Compute minimum values of specified fields max Compute maximum values of specified fields Examples - mlr stats1 -a min,p10,p50,p90,max -f value -g size,shape mlr stats1 -a count,mode -f size mlr stats1 -a count,mode -f size -g shape ! mlr -- csv -- rs lf filter '$ArrDelay!=\"NA\"' \\ then stats1 - a mean , stddev - f ArrDelay - g Origin \\ then sort - nr ArrDelay_mean \\ then head - n 10 \\ flights . csv | csvlook |---------+---------------+------------------| | Origin | ArrDelay_mean | ArrDelay_stddev | |---------+---------------+------------------| | ACK | 46.115523 | 82.715645 | | SOP | 40.464865 | 61.941857 | | PIR | 35.750000 | 33.260337 | | MKC | 25.000000 | | | CEC | 24.763527 | 47.427527 | | MCN | 24.087607 | 79.076628 | | SPI | 21.591925 | 62.123661 | | EWN | 21.486532 | 53.500822 | | HHH | 20.648305 | 61.881898 | | MEI | 20.529141 | 52.402560 | |---------+---------------+------------------| ! mlr -- csv -- rs lf filter '$ArrDelay!=\"NA\"' \\ then stats1 - a min , p05 , p10 , p90 , p95 , max - f ArrDelay - g Origin \\ then sort - f Origin \\ then head - n 10 \\ flights . csv | csvlook |---------+--------------+--------------+--------------+--------------+--------------+---------------| | Origin | ArrDelay_min | ArrDelay_p05 | ArrDelay_p10 | ArrDelay_p90 | ArrDelay_p95 | ArrDelay_max | |---------+--------------+--------------+--------------+--------------+--------------+---------------| | ABE | -44 | -22 | -18 | 39 | 83 | 690 | | ABI | -32 | -14 | -12 | 39 | 78 | 695 | | ABQ | -61 | -19 | -15 | 29 | 54 | 1049 | | ABY | -27 | -15 | -12 | 45 | 79 | 336 | | ACK | -51 | -37 | -31 | 163 | 234 | 386 | | ACT | -32 | -17 | -14 | 33 | 67 | 366 | | ACV | -30 | -17 | -14 | 62 | 96 | 498 | | ACY | -55 | -30 | -24 | 60 | 110 | 741 | | ADK | -83 | -73 | -65 | 56 | 78 | 160 | | ADQ | -31 | -19 | -16 | 41 | 66 | 275 | |---------+--------------+--------------+--------------+--------------+--------------+---------------|","title":"stats1"},{"location":"05_miller/#stats2","text":"for bivariate statistics -a {linreg-ols,corr,...} Names of accumulators: one or more of: linreg-pca Linear regression using principal component analysis linreg-ols Linear regression using ordinary least squares r2 Quality metric for linreg-ols (linreg-pca emits its own) logireg Logistic regression corr Sample correlation cov Sample covariance covx Sample-covariance matrix -f {a,b,c,d} Value-field name-pairs on which to compute statistics. There must be an even number of names. -g {e,f,g} Optional group-by-field names. -v Print additional output for linreg-pca. -s Print iterative stats. Useful in tail -f contexts (in which case please avoid pprint-format output since end of input stream will never be seen). --fit Rather than printing regression parameters, applies them to the input data to compute new fit fields. All input records are held in memory until end of input stream. Has effect only for linreg-ols, linreg-pca, and logireg. Only one of -s or --fit may be used. Examples mlr stats2 -a linreg-pca -f x,y mlr stats2 -a linreg-ols,r2 -f x,y -g size,shape mlr stats2 -a corr -f x,y ! mlr -- csv -- rs lf filter '$ArrDelay!=\"NA\"' \\ then stats2 - a corr - f ArrDelay , AirTime","title":"stats2"},{"location":"05_miller/#steps","text":"Computes values dependent on the previous record, optionally grouped by category. useful for calculating lagged variables Most Miller commands are record-at-a-time , with the exception of stats1, stats2 , and histogram which compute aggregate output. The step command is intermediate: it allows the option of adding fields which are functions of fields from previous records. Rsum is short for running sum. Options: -a {delta,rsum,...} Names of steppers: comma-separated, one or more of: delta Compute differences in field(s) between successive records shift Include value(s) in field(s) from previous record, if any from-first Compute differences in field(s) from first record ratio Compute ratios in field(s) between successive records rsum Compute running sums of field(s) between successive records counter Count instances of field(s) between successive records ewma Exponentially weighted moving average over successive records -f {a,b,c} Value-field names on which to compute statistics -g {d,e,f} Optional group-by-field names -F Computes integerable things (e.g. counter) in floating point. -d {x,y,z} Weights for ewma. 1 means current sample gets all weight (no smoothing), near under under 1 is light smoothing, near over 0 is heavy smoothing. Multiple weights may be specified, e.g. \"mlr step -a ewma -f sys_load -d 0.01,0.1,0.9\". Default if omitted is \"-d 0.5\". -o {a,b,c} Custom suffixes for EWMA output fields. If omitted, these default to the -d values. If supplied, the number of -o values must be the same as the number of -d values. Examples: mlr step -a rsum -f request_size mlr step -a delta -f request_size -g hostname mlr step -a ewma -d 0.1,0.9 -f x,y mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y -g group_name","title":"steps"},{"location":"05_miller/#also-see","text":"uniq for uniques and frequency tables top for nlargest some examples DSL for put , filter","title":"Also see"},{"location":"06_q_textql/","text":"import os os . chdir ( \"/home/data\" ) ls | grep kdd | xargs wc - l 42 kddcup-names 4898431 kddcup.data 4898473 total Basic Info on a File \u00b6 ! cat kddcup . data | csvtk stats file num_cols num_rows - 42 4,898,430 ! echo 'interaction_type' >> ../ data / raw / kdd . csv ! cat ../ data / raw / kdd . csv duration,protocol_type,service,flag,src_bytes,dst_bytes,land,wrong_fragment,urgent,hot,num_failed_logins,logged_in,num_compromised,root_shell,su_attempted,num_root,num_file_creations,num_shells,num_access_files,num_outbound_cmds,is_host_login,is_guest_login,count,srv_count,serror_rate,srv_serror_rate,rerror_rate,srv_rerror_rate,same_srv_rate,diff_srv_rate,srv_diff_host_rate,dst_host_count,dst_host_srv_count,dst_host_same_srv_rate,dst_host_diff_srv_rate,dst_host_same_src_port_rate,dst_host_srv_diff_host_rate,dst_host_serror_rate,dst_host_srv_serror_rate,dst_host_rerror_rate,dst_host_srv_rerror_rate,interaction_type ! cat kddcup . data >> ../ data / raw / kdd . csv ! head ../ data / raw / kdd . csv | cut - d , - f30 - 42 | sed '1d' 0.00,0.00,0,0,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,1,1,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,2,2,1.00,0.00,0.50,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,3,3,1.00,0.00,0.33,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,4,4,1.00,0.00,0.25,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,5,5,1.00,0.00,0.20,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,6,6,1.00,0.00,0.17,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,7,7,1.00,0.00,0.14,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,8,8,1.00,0.00,0.12,0.00,0.00,0.00,0.00,0.00,normal. q: text as data \u00b6 http://harelba.github.io/q/index.html q allows performing SQL-like statements on tabular text data. Its purpose is to bring SQL expressive power to manipulating text data using the Linux command line. Basic usage is q \"<sql like query>\" where table names are just regular file names When the input contains a header row, use -H , and column names will be set according to the header row content. If there isn't a header row, then columns will automatically be named c1..cN Column types are detected automatically. Use -A in order to see the column name/type analysis. Delimiter can be set using the -d (or -t ) option. Output delimiter can be set using -D All sqlite3 SQL constructs are supported. Please note that column names that include spaces need to be used in the query with back-ticks, as per the sqlite standard. Full type detection is implemented, so there is no need for any casting Multiple Files Multiple files can be concatenated by using one of both of the following ways: Separating the filenames with a + sign: SELECT * FROM datafile1+datafile2+datefile3. Using glob matching: SELECT * FROM mydata*.dat ! ls ../ data / raw / | grep csv countrynames.csv Crimes_Chicago.csv Flight_Delays.csv fromPandas.csv het-bool.csv kdd.csv millionSongsSample.csv NYC__311Requests.csv worldcitiespop.csv ! ls ../ data / raw / | grep txt Fun__Item.txt Fun__Sales.txt kddcup.names.txt MusicXMatch__Test.txt MusicXMatch__Train.txt Stock_Export.txt ! cat ../ data / raw / kdd . csv | q - H - d , \"\"\"SELECT distinct(interaction_type), count(*) from - group by 1 order by 2 desc\"\"\" smurf.,2807886 neptune.,1072017 normal.,972781 satan.,15892 ipsweep.,12481 portsweep.,10413 nmap.,2316 back.,2203 warezclient.,1020 teardrop.,979 pod.,264 guess_passwd.,53 buffer_overflow.,30 land.,21 warezmaster.,20 imap.,12 rootkit.,10 loadmodule.,9 ftp_write.,8 multihop.,7 phf.,4 perl.,3 spy.,2 ! head ../ data / raw / kdd . csv | q - H - d , \"\"\"SELECT interaction_type, case when interaction_type = 'normal.' then 0 else 1 end as attack \\ from -\"\"\" normal.,0 normal.,0 normal.,0 normal.,0 normal.,0 normal.,0 normal.,0 normal.,0 normal.,0","title":"Q"},{"location":"06_q_textql/#basic-info-on-a-file","text":"! cat kddcup . data | csvtk stats file num_cols num_rows - 42 4,898,430 ! echo 'interaction_type' >> ../ data / raw / kdd . csv ! cat ../ data / raw / kdd . csv duration,protocol_type,service,flag,src_bytes,dst_bytes,land,wrong_fragment,urgent,hot,num_failed_logins,logged_in,num_compromised,root_shell,su_attempted,num_root,num_file_creations,num_shells,num_access_files,num_outbound_cmds,is_host_login,is_guest_login,count,srv_count,serror_rate,srv_serror_rate,rerror_rate,srv_rerror_rate,same_srv_rate,diff_srv_rate,srv_diff_host_rate,dst_host_count,dst_host_srv_count,dst_host_same_srv_rate,dst_host_diff_srv_rate,dst_host_same_src_port_rate,dst_host_srv_diff_host_rate,dst_host_serror_rate,dst_host_srv_serror_rate,dst_host_rerror_rate,dst_host_srv_rerror_rate,interaction_type ! cat kddcup . data >> ../ data / raw / kdd . csv ! head ../ data / raw / kdd . csv | cut - d , - f30 - 42 | sed '1d' 0.00,0.00,0,0,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,1,1,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,2,2,1.00,0.00,0.50,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,3,3,1.00,0.00,0.33,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,4,4,1.00,0.00,0.25,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,5,5,1.00,0.00,0.20,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,6,6,1.00,0.00,0.17,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,7,7,1.00,0.00,0.14,0.00,0.00,0.00,0.00,0.00,normal. 0.00,0.00,8,8,1.00,0.00,0.12,0.00,0.00,0.00,0.00,0.00,normal.","title":"Basic Info on a File"},{"location":"06_q_textql/#q-text-as-data","text":"http://harelba.github.io/q/index.html q allows performing SQL-like statements on tabular text data. Its purpose is to bring SQL expressive power to manipulating text data using the Linux command line. Basic usage is q \"<sql like query>\" where table names are just regular file names When the input contains a header row, use -H , and column names will be set according to the header row content. If there isn't a header row, then columns will automatically be named c1..cN Column types are detected automatically. Use -A in order to see the column name/type analysis. Delimiter can be set using the -d (or -t ) option. Output delimiter can be set using -D All sqlite3 SQL constructs are supported. Please note that column names that include spaces need to be used in the query with back-ticks, as per the sqlite standard. Full type detection is implemented, so there is no need for any casting Multiple Files Multiple files can be concatenated by using one of both of the following ways: Separating the filenames with a + sign: SELECT * FROM datafile1+datafile2+datefile3. Using glob matching: SELECT * FROM mydata*.dat ! ls ../ data / raw / | grep csv countrynames.csv Crimes_Chicago.csv Flight_Delays.csv fromPandas.csv het-bool.csv kdd.csv millionSongsSample.csv NYC__311Requests.csv worldcitiespop.csv ! ls ../ data / raw / | grep txt Fun__Item.txt Fun__Sales.txt kddcup.names.txt MusicXMatch__Test.txt MusicXMatch__Train.txt Stock_Export.txt ! cat ../ data / raw / kdd . csv | q - H - d , \"\"\"SELECT distinct(interaction_type), count(*) from - group by 1 order by 2 desc\"\"\" smurf.,2807886 neptune.,1072017 normal.,972781 satan.,15892 ipsweep.,12481 portsweep.,10413 nmap.,2316 back.,2203 warezclient.,1020 teardrop.,979 pod.,264 guess_passwd.,53 buffer_overflow.,30 land.,21 warezmaster.,20 imap.,12 rootkit.,10 loadmodule.,9 ftp_write.,8 multihop.,7 phf.,4 perl.,3 spy.,2 ! head ../ data / raw / kdd . csv | q - H - d , \"\"\"SELECT interaction_type, case when interaction_type = 'normal.' then 0 else 1 end as attack \\ from -\"\"\" normal.,0 normal.,0 normal.,0 normal.,0 normal.,0 normal.,0 normal.,0 normal.,0 normal.,0","title":"q: text as data"},{"location":"07_datamash/","text":"datamash \u00b6 https://www.gnu.org/software/datamash/manual/datamash.html https://www.gnu.org/software/datamash/manual/html_node/Available-Operations.html Installation Notes \u00b6 If you use sudo apt install datamash , check the version with datamash --version We need version 1.1.0 and above (for functions like crosstab ) If not, follow these steps # download the installer wget -P /tmp/ ftp://ftp.gnu.org/gnu/datamash/datamash-1.1.1.tar.gz # untar the file mkdir /tmp/datamash tar -xzf /tmp/datamash-1.1.1.tar.gz -C /tmp/datamash/ # build cd /tmp/datamash ./configure make sudo make install # if `datamash` doesn't run, copy the executable to the /usr/bin/ folder sudo cp ./datamash /usr/bin/datamash # run this to check datamash --version # read the documentation datamash --help man datamash info datamash Usage \u00b6 Where op1 is the operation to perform on the values in column1. datamash reads input from stdin and performs one or more operations on the input data. If --group is used, each operation is performed on every group. If --group is not used, each operation is performed on all the values in the input file. Syntax datamash [option]\u2026 op1 column1 [op2 column2 \u2026] Highlights \u00b6 Title Features Summary Statistics count,min,max,mean,stdev,median,quartiles Header Lines and Column Names Using files with header lines Field Delimiters Tabs, Whitespace, other delimiters Column Ranges Operating on multiple columns Groupby Groupby , count, collapse Check Validate tabular structure Crosstab Cross-tabulation ( pivot-tables ) Rounding numbers round, ceil, floor, trunc, frac Binning numbers assigning numbers into fixed number of buckets Binning strings assigning strings into fixed number of buckets File operations transpose, reverse Line-Filtering operations rmdup Per-Line operations base64, debase64, md5, sha1, sha256, sha512 Numeric Grouping operations sum, min, max, absmin, absmax Textual/Numeric Grouping operations count, first, last, rand, unique, collapse, countunique Statistical Grouping operations 1 mean, median, q1, q3, iqr, mode, antimode, pstdev, sstdev Statistical Grouping Operations 2 , pvar, svar, mad, madraw, pskew, sskew, pkurt, skurt, dpo, jarque Also \u00b6 fills NAs! remove duplicates! MOAR Options \u00b6 Grouping Options: -f, --full print entire input line before op results (default: print only the grouped keys) -g, --group=X[,Y,Z] group via fields X,[Y,Z] -H, --headers first input line is column headers, print column headers as first line -i, --ignore-case ignore upper/lower case when comparing text; this affects grouping, and string operations -s, --sort sort the input before grouping; this removes the need to manually pipe the input through 'sort' File Operation Options: --no-strict allow lines with varying number of fields --filler=X fill missing values with X (default %s) General Options: -t, --field-separator=X use X instead of TAB as field delimiter --narm skip NA/NaN values -W, --whitespace use whitespace (one or more spaces and/or tabs) for field delimiters ! ls - l ../ data / raw | grep csv -rw-rw-r-- 1 dk dk 3619 May 16 10:51 countrynames.csv -rw-rw-r-- 1 dk dk 1489911208 May 16 10:51 Crimes_Chicago.csv -rw------- 1 dk dk 702878193 May 16 10:51 Flight_Delays.csv -rw------- 1 dk dk 702878193 May 16 23:10 flights.csv -rw-rw-r-- 1 dk dk 523 May 16 23:45 fromPandas_01.csv -rw-rw-r-- 1 dk dk 532 May 16 23:45 fromPandas_02.csv -rw-rw-r-- 1 dk dk 535743168 May 16 15:42 fromPandas.csv -rw-rw-r-- 1 dk dk 57 May 16 10:51 het-bool.csv -rw-rw-r-- 1 dk dk 742580451 May 16 18:59 kdd.csv -rw-rw-r-- 1 dk dk 28512235 May 16 18:49 millionSongsSample.csv -rw-rw-r-- 1 dk dk 10136806470 May 16 10:52 NYC__311Requests.csv -rw-rw-r-- 1 dk dk 151488712 May 16 10:52 worldcitiespop.csv basic stat s \u00b6 ! head ../ data / raw / fromPandas . csv | csvlook |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | C00 | D01 | C02 | D03 | D04 | A05 | A06 | B07 | A08 | C09 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | PO | Alert | 0.08 | -0.29 | 1.04 | 0.5 | -0.19 | 0.92 | -1.18 | 0.46 | | PO | Alert | 0.86 | 1.81 | 2.28 | 1.58 | 0.79 | 1.19 | 0.99 | -1.17 | | PO | Critical | -0.04 | 0.52 | -0.52 | 0.34 | 2.09 | -0.6 | 0.85 | -1.14 | | AR | Critical | -0.28 | 0.56 | 0.69 | 0.62 | -0.28 | -0.25 | 0.64 | -2.03 | | AR | Critical | 0.04 | -0.38 | -0.66 | -1.76 | -0.13 | 0.6 | -2.06 | 0.96 | | PO | Critical | 0.77 | 1.48 | 0.16 | -0.59 | 0.94 | 0.48 | -0.32 | -0.55 | | AR | Alert | -0.15 | 0.03 | -2.05 | -0.56 | -1.04 | 1.29 | 0.51 | -0.01 | | PO | Alert | -0.17 | -0.4 | -0.16 | 2.65 | -0.48 | 0.25 | -1.1 | 0.77 | | AR | Alert | -0.57 | -0.31 | -0.14 | -2.89 | 0.52 | 0.18 | -0.03 | 0.47 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| ! datamash - sHi - t , sum 3 - 6 < ../ data / raw / fromPandas . csv | csvlook |-----------+----------+----------+-----------| | sum(C02) | sum(D03) | sum(D04) | sum(A05) | |-----------+----------+----------+-----------| | 4083.25 | 3448.1 | -2271.68 | -236.88 | |-----------+----------+----------+-----------| ! datamash - t , - H min 3 q1 3 median 3 q3 3 max 3 < ../ data / raw / fromPandas . csv | csvlook |-----------+---------+-------------+---------+-----------| | min(C02) | q1(C02) | median(C02) | q3(C02) | max(C02) | |-----------+---------+-------------+---------+-----------| | -5.21 | -0.67 | -0 | 0.68 | 5.19 | |-----------+---------+-------------+---------+-----------| ! datamash - t , - H mean A06 sstdev A06 < ../ data / raw / fromPandas . csv | csvlook |--------------+------------------| | mean(A06) | sstdev(A06) | |--------------+------------------| | 0.000461629 | 1.0001281946602 | |--------------+------------------| groupby operations \u00b6 ! csvcut - c interaction_type , src_bytes , dst_bytes ../ data / raw / kdd . csv | head | csvlook |-------------------+-----------+------------| | interaction_type | src_bytes | dst_bytes | |-------------------+-----------+------------| | normal. | 215 | 45076 | | normal. | 162 | 4528 | | normal. | 236 | 1228 | | normal. | 233 | 2032 | | normal. | 239 | 486 | | normal. | 238 | 1282 | | normal. | 235 | 1337 | | normal. | 234 | 1364 | | normal. | 239 | 1295 | |-------------------+-----------+------------| ! datamash - sHi - t , \\ -- group interaction_type \\ mean src_bytes , dst_bytes \\ sstdev src_bytes , dst_bytes < ../ data / raw / kdd . csv | csvlook # -t, because our file is comma delimited # --sort because groupby needs it # -H because the input has headers and we want the output to have them too # --group for group-by # -i to ignore case # count is the aggregation function used (Remember the syntax <op1> <col1>) |----------------------------+--------------------+---------------------+-------------------+--------------------| | GroupBy(interaction_type) | mean(src_bytes) | mean(dst_bytes) | sstdev(src_bytes) | sstdev(dst_bytes) | |----------------------------+--------------------+---------------------+-------------------+--------------------| | back. | 54156.355878348 | 8232.6495687699 | 3159.3602320464 | 616.23179457446 | | buffer_overflow. | 1400.4333333333 | 6339.8333333333 | 1337.1326162103 | 12440.664773466 | | ftp_write. | 220.75 | 5382.25 | 267.74761570234 | 13793.737530903 | | guess_passwd. | 125.33962264151 | 216.18867924528 | 3.037859799687 | 257.50228203647 | | imap. | 347.58333333333 | 54948.666666667 | 629.92603582172 | 187158.40119454 | | ipsweep. | 10.436583607083 | 4.394359426328 | 37.09492550297 | 462.19579938396 | | land. | 0 | 0 | 0 | 0 | | loadmodule. | 151.88888888889 | 3009.8888888889 | 127.74529780431 | 2907.4277052252 | | multihop. | 435.14285714286 | 213016.28571429 | 540.96038936057 | 382586.00499962 | | neptune. | 0.0099942444942571 | 0.00082088250466177 | 10.347866229879 | 0.84992741107836 | | nmap. | 24.424006908463 | 0.13255613126079 | 60.37052484509 | 4.7761242620571 | | normal. | 1477.846250081 | 3234.6501113817 | 110500.41940105 | 34231.680610548 | | perl. | 265.66666666667 | 2444 | 4.9328828623162 | 166.13548687743 | | phf. | 51 | 8127 | 0 | 0 | | pod. | 1462.6515151515 | 0 | 125.0980442777 | 0 | | portsweep. | 431708.31182176 | 202681.31643138 | 20383536.155508 | 13983601.671997 | | rootkit. | 294.7 | 4276.6 | 538.57817961328 | 7558.5449540386 | | satan. | 0.99874150515983 | 2.1274855273093 | 35.927415459528 | 145.10315741745 | | smurf. | 935.7730962012 | 0 | 200.02142879934 | 0 | | spy. | 174.5 | 1193.5 | 88.388347648318 | 490.02499936228 | | teardrop. | 28 | 0.057201225740552 | 0 | 1.2649097429583 | | warezclient. | 300219.5627451 | 719.31764705882 | 1200905.2431303 | 1104.4083239047 | | warezmaster. | 49.3 | 3922087.7 | 212.15513192002 | 2197498.9594262 | |----------------------------+--------------------+---------------------+-------------------+--------------------| ! csvcut - c 9 , 10 , 15 - 19 ../ data / raw / flights . csv | head | csvlook |----------------+-----------+----------+----------+--------+------+-----------| | UniqueCarrier | FlightNum | ArrDelay | DepDelay | Origin | Dest | Distance | |----------------+-----------+----------+----------+--------+------+-----------| | WN | 2891 | 1 | 7 | SMF | ONT | 389 | | WN | 462 | 8 | 13 | SMF | PDX | 479 | | WN | 1229 | 34 | 36 | SMF | PDX | 479 | | WN | 1355 | 26 | 30 | SMF | PDX | 479 | | WN | 2278 | -3 | 1 | SMF | PDX | 479 | | WN | 2386 | 3 | 10 | SMF | PDX | 479 | | WN | 409 | 47 | 56 | SMF | PHX | 647 | | WN | 1131 | -2 | 9 | SMF | PHX | 647 | | WN | 1212 | 44 | 47 | SMF | PHX | 647 | |----------------+-----------+----------+----------+--------+------+-----------| # exclude missing data manually ! csvtk filter - f \"15-16>0\" ../ data / raw / flights . csv \\ | datamash - t , - sHi - g UniqueCarrier , Origin , Dest \\ mean ArrDelay , DepDelay \\ | csvsort - r - c 'mean(ArrDelay)' \\ | head | csvlook |-------------------------+-----------------+---------------+----------------+-----------------| | GroupBy(UniqueCarrier) | GroupBy(Origin) | GroupBy(Dest) | mean(ArrDelay) | mean(DepDelay) | |-------------------------+-----------------+---------------+----------------+-----------------| | B6 | ONT | IAD | 370.0 | 386.0 | | XE | ELP | MFE | 316.0 | 307.0 | | DL | SLC | KOA | 308.0 | 317.5 | | OH | ACY | MYR | 252.0 | 222.0 | | EV | RIC | CVG | 243.0 | 262.0 | | UA | OAK | LAX | 231.0 | 248.0 | | OH | MDT | CLE | 219.0 | 209.0 | | OH | JAX | CMH | 217.0 | 165.0 | | NW | DCA | PLN | 210.0 | 168.0 | |-------------------------+-----------------+---------------+----------------+-----------------| ! csvtk filter - f \"15-16=' '\" ../ data / raw / flights . csv | head \u001b[31m[ERRO]\u001b[0m invalid filter: 15-16=' ' ! mlr ! datamash - t , - sHi - g UniqueCarrier , Origin , Dest \\ mean ArrDelay , DepDelay < ../ data / raw / flights . csv | head | csvlook datamash: invalid numeric value in line 2 field 15: 'NA' sort: write failed: 'standard output': Broken pipe sort: write error |-------------------------+-----------------+---------------+----------------+-----------------| | GroupBy(UniqueCarrier) | GroupBy(Origin) | GroupBy(Dest) | mean(ArrDelay) | mean(DepDelay) | |-------------------------+-----------------+---------------+----------------+-----------------| dealing with missing data \u00b6 check with csvstat --nulls remove with awk fill with datamash ! csvstat - c 15 - 16 -- nulls ../ data / raw / flights . csv 15. ArrDelay: True 16. DepDelay: True Drop rows with missing data in a particular column # ignore rows with null values in column 5 $ awk -F, '!length($5)' file # 1,abc,543,87,,fsg; # 1,abc,543,88,,fsg; import pandas as pd import numpy as np df = pd . DataFrame ( data = np . random . randn ( 10 , 10 ) . round ( 2 )) df . iloc [:: 2 , :: 2 ] = np . nan df . columns = [ 'C' + str ( i ) . zfill ( 2 ) for i in range ( 10 )] df . to_csv ( 'fp2.csv' , index = False ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } C00 C01 C02 C03 C04 C05 C06 C07 C08 C09 0 NaN -0.65 NaN -0.51 NaN 2.10 NaN -0.01 NaN -0.11 1 0.14 0.24 -0.91 -0.54 0.72 1.14 -0.07 0.02 -0.05 1.07 2 NaN 0.81 NaN -1.10 NaN 1.01 NaN -1.88 NaN -3.18 3 1.14 -0.25 -2.19 0.40 1.34 -0.96 0.27 0.26 -1.02 -0.37 4 NaN 0.34 NaN -0.81 NaN 0.80 NaN 1.13 NaN -0.15 5 -2.23 -0.90 -0.52 0.54 -1.07 1.47 -0.58 -0.46 0.53 -1.79 6 NaN -0.44 NaN -0.48 NaN -0.73 NaN -0.28 NaN -1.94 7 -2.73 -0.22 1.87 0.92 -0.02 -0.44 0.48 1.48 -1.62 0.92 8 NaN -0.31 NaN -0.58 NaN 0.46 NaN 0.10 NaN 0.81 9 -1.36 0.29 -0.73 0.34 -1.85 -1.64 1.15 -0.23 -1.35 1.11 Checking for nulls \u00b6 Checking for missing or extra fields \u00b6 ! datamash - t , check < fp2 . csv 11 lines, 10 fields %% writefile fp3 . csv C01 , C02 , C03 0 , 10 , 100 0000 , 0001 , 0010 0011 , 0100 , 0101 , 0110 0111 , 1000 , 1001 Overwriting fp3.csv ! datamash - t , check < fp3 . csv line 3 (3 fields): 0000,0001,0010 line 4 (4 fields): 0011,0100,0101,0110 datamash: check failed: line 4 has 4 fields (previous line had 3) Checking for nulls \u00b6 ! csvstat -- nulls fp2 . csv 1. C00: True 2. C01: False 3. C02: True 4. C03: False 5. C04: True 6. C05: False 7. C06: True 8. C07: False 9. C08: True 10. C09: False Filtering for rows with nulls \u00b6 ! csvsql -- query \"SELECT * \\ FROM fp2 \\ WHERE C00 IS NULL\" fp2 . csv | csvlook |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| | C00 | C01 | C02 | C03 | C04 | C05 | C06 | C07 | C08 | C09 | |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| | | -0.04 | | 2.58 | | 0.65 | | 0.45 | | -0.47 | | | 1.19 | | 1.35 | | -0.04 | | 0.06 | | -0.08 | | | -1.7 | | -0.38 | | -0.93 | | -1.17 | | -0.22 | | | 0.05 | | 0.94 | | -0.44 | | 0.56 | | -0.59 | | | -1.7 | | -2.03 | | -1.15 | | 1.84 | | 0.11 | |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| 194.78 . 85.8 # select rows where C00 isnull ! mlr -- csv -- rs lf cat \\ then filter 'is_null($C00)' fp2 . csv | csvlook |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| | C00 | C01 | C02 | C03 | C04 | C05 | C06 | C07 | C08 | C09 | |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| | | -0.04 | | 2.58 | | 0.65 | | 0.45 | | -0.47 | | | 1.19 | | 1.35 | | -0.04 | | 0.06 | | -0.08 | | | -1.7 | | -0.38 | | -0.93 | | -1.17 | | -0.22 | | | 0.05 | | 0.94 | | -0.44 | | 0.56 | | -0.59 | | | -1.7 | | -2.03 | | -1.15 | | 1.84 | | 0.11 | |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| ! mlr -- csv -- rs lf cat \\ then filter '$ArrDelay == \"NA\"' ../ data / raw / flights . csv | head | csvcut - c ArrDelay , Origin , Dest | csvlook |-----------+--------+-------| | ArrDelay | Origin | Dest | |-----------+--------+-------| | NA | SNA | LAS | | NA | AUS | DAL | | NA | DAL | AUS | | NA | DAL | HOU | | NA | DAL | HOU | | NA | DAL | HOU | | NA | HOU | DAL | | NA | HOU | DAL | | NA | HOU | DAL | |-----------+--------+-------| ! csvgrep - c ArrDelay - r NA ../ data / raw / flights . csv | head | cut - d , - f5 , 7 , 12 , 15 - 18 | csvlook |----------+---------+-------------------+----------+----------+--------+-------| | DepTime | ArrTime | ActualElapsedTime | ArrDelay | DepDelay | Origin | Dest | |----------+---------+-------------------+----------+----------+--------+-------| | NA | NA | NA | NA | NA | SNA | LAS | | NA | NA | NA | NA | NA | AUS | DAL | | NA | NA | NA | NA | NA | DAL | AUS | | NA | NA | NA | NA | NA | DAL | HOU | | NA | NA | NA | NA | NA | DAL | HOU | | NA | NA | NA | NA | NA | DAL | HOU | | NA | NA | NA | NA | NA | HOU | DAL | | NA | NA | NA | NA | NA | HOU | DAL | | NA | NA | NA | NA | NA | HOU | DAL | |----------+---------+-------------------+----------+----------+--------+-------| ! csvgrep - c ArrDelay , DepDelay , DepTime , ArrTime - r NA ../ data / raw / flights . csv | wc - l 160749 ! csvgrep - c ArrDelay , DepDelay , DepTime , ArrTime - r '^NA$' ../ data / raw / flights . csv | wc - l 160749 Marking/Isolating rows with Nulls \u00b6 ! mlr -- csv -- rs lf cat \\ then put '$C00_isnull = is_null($C00)' fp2 . csv | csvlook |--------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------------| | C00 | C01 | C02 | C03 | C04 | C05 | C06 | C07 | C08 | C09 | C00_isnull | |--------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------------| | | -0.04 | | 2.58 | | 0.65 | | 0.45 | | -0.47 | true | | 0.66 | -0.75 | 1.26 | 0.07 | 0.18 | -1.09 | 0.9 | -1.92 | 0.04 | -1.02 | false | | | 1.19 | | 1.35 | | -0.04 | | 0.06 | | -0.08 | true | | 0.37 | 0.79 | -0.63 | -0.18 | 1.51 | -1.55 | -0.82 | 0.46 | 0.47 | 1.02 | false | | | -1.7 | | -0.38 | | -0.93 | | -1.17 | | -0.22 | true | | 0.89 | 1.46 | -2.24 | -1.29 | -0.79 | 0.51 | 1.28 | 1.75 | 0.64 | -1.8 | false | | | 0.05 | | 0.94 | | -0.44 | | 0.56 | | -0.59 | true | | -1.04 | 0.4 | 1.67 | -1.06 | 0.81 | 0.19 | 0.3 | 0.18 | -0.37 | -1.79 | false | | | -1.7 | | -2.03 | | -1.15 | | 1.84 | | 0.11 | true | | 1.24 | 0.29 | 0.72 | -1.67 | -0.16 | 1.7 | 0.5 | 0.09 | -0.77 | -0.49 | false | |--------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------------| crosstabs, pivot tables \u00b6 ! datamash crosstab -- help Usage: datamash [OPTION] op [fld] [op fld ...] Performs numeric/string operations on input from stdin. 'op' is the operation to perform. If a primary operation is used, it must be listed first, optionally followed by other operations. 'fld' is the input field to use. 'fld' can be a number (1=first field), or a field name when using the -H or --header-in options. Multiple fields can be listed with a comma (e.g. 1,6,8). A range of fields can be listed with a dash (e.g. 2-8). Use colons for operations which require a pair of fields (e.g. 'pcov 2:6'). Primary operations: groupby, crosstab, transpose, reverse, check Line-Filtering operations: rmdup Per-Line operations: base64, debase64, md5, sha1, sha256, sha512, bin, strbin, round, floor, ceil, trunc, frac Numeric Grouping operations: sum, min, max, absmin, absmax Textual/Numeric Grouping operations: count, first, last, rand, unique, collapse, countunique Statistical Grouping operations: mean, median, q1, q3, iqr, mode, antimode, pstdev, sstdev, pvar, svar, mad, madraw, pskew, sskew, pkurt, skurt, dpo, jarque, scov, pcov, spearson, ppearson Grouping Options: -f, --full print entire input line before op results (default: print only the grouped keys) -g, --group=X[,Y,Z] group via fields X,[Y,Z]; equivalent to primary operation 'groupby' --header-in first input line is column headers --header-out print column headers as first line -H, --headers same as '--header-in --header-out' -i, --ignore-case ignore upper/lower case when comparing text; this affects grouping, and string operations -s, --sort sort the input before grouping; this removes the need to manually pipe the input through 'sort' File Operation Options: --no-strict allow lines with varying number of fields --filler=X fill missing values with X (default %s) General Options: -t, --field-separator=X use X instead of TAB as field delimiter --narm skip NA/NaN values -W, --whitespace use whitespace (one or more spaces and/or tabs) for field delimiters -z, --zero-terminated end lines with 0 byte, not newline --help display this help and exit --version output version information and exit Examples: Print the sum and the mean of values from column 1: $ seq 10 | datamash sum 1 mean 1 55 5.5 Transpose input: $ seq 10 | paste - - | datamash transpose 1 3 5 7 9 2 4 6 8 10 For detailed usage information and examples, see man GNU datamash The manual and more examples are available at http://www.gnu.org/software/datamash ! man datamash | grep crosstab <standard input>:161: a space character is not allowed in an escape name groupby, crosstab, transpose, reverse, check numbers (groupby, crosstab) while others do not (reverse,check,trans\u2010 crosstab X,Y [op fld ...] $ datamash -s crosstab 1,2 < input.txt $ datamash -s crosstab 1,2 sum 3 < input.txt $ datamash -s crosstab 1,2 unique 3 < input.txt ! cat ../ data / raw / fromPandas . csv | datamash - t , - s - H crosstab 1 , 2 | sed '1d' | csvlook |-----+---------+----------+--------+-----------| | | Alert | Critical | Ignore | Shutdown | |-----+---------+----------+--------+-----------| | AR | 2700486 | 2274768 | 29093 | 15405 | | EN | 775 | 682 | 7 | 4 | | ES | 2171 | 1762 | 17 | 17 | | FR | 291865 | 246750 | 3147 | 1652 | | PO | 2225076 | 1874976 | 24232 | 12440 | | RU | 158732 | 133364 | 1661 | 918 | |-----+---------+----------+--------+-----------| Checking for nulls \u00b6 ! cat ../ data / raw / fromPandas . csv | datamash - t , - s - H crosstab C00 , D01 # default aggfunc is `count` datamash: src/column-headers.c:63: get_input_field_name: Assertion `field_num > 0 && field_num <= num_input_column_headers' failed. GroupBy(C00),GroupBy(D01),Aborted (core dumped) ! cat ../ data / raw / fromPandas . csv | datamash - t , - s - H crosstab 1 , 2 mean 3 | sed '1d' | csvlook |-----+---------------------+----------------------+---------------------+---------------------| | | Alert | Critical | Ignore | Shutdown | |-----+---------------------+----------------------+---------------------+---------------------| | AR | 0.00096527810179353 | 0.00045535193039466 | -0.0031327123363008 | 0.0037702044790652 | | EN | -0.016554838709677 | 0.049560117302053 | 0.24714285714286 | -0.0575 | | ES | 0.011879318286504 | 0.07194665153235 | -0.028823529411765 | 0.16117647058824 | | FR | 0.00023079163311805 | -0.0020542249240122 | 0.0017572291070861 | -0.018807506053269 | | PO | 2.670470581679e-05 | 0.00029508644377315 | -0.012478128095081 | 0.011842443729904 | | RU | 0.0020377743618174 | -6.1860772022435e-05 | 9.59528713759e-21 | -0.012320261437908 | |-----+---------------------+----------------------+---------------------+---------------------| ! mlr cut - h Usage: mlr cut [options] Passes through input records with specified fields included/excluded. -f {a,b,c} Field names to include for cut. -o Retain fields in the order specified here in the argument list. Default is to retain them in the order found in the input data. -x|--complement Exclude, rather than include, field names specified by -f. -r Treat field names as regular expressions. \"ab\", \"a.*b\" will match any field name containing the substring \"ab\" or matching \"a.*b\", respectively; anchors of the form \"^ab$\", \"^a.*b$\" may be used. The -o flag is ignored when -r is present. Examples: mlr cut -f hostname,status mlr cut -x -f hostname,status mlr cut -r -f '^status$,sda[0-9]' mlr cut -r -f '^status$,\"sda[0-9]\"' mlr cut -r -f '^status$,\"sda[0-9]\"i' (this is case-insensitive) ! csvcut - c 1 - 6 , 42 ../ data / raw / kdd . csv | csvstat 1. duration <type 'int'> Nulls: False Min: 0 Max: 58329 Sum: 236802060 Mean: 48.342430464 Median: 0 Standard Deviation: 723.329737422 Unique values: 9883 5 most frequent values: 0: 4779492 1: 23886 2: 8139 3: 6016 5: 5576 2. protocol_type <type 'unicode'> Nulls: False Values: udp, icmp, tcp 3. service <type 'unicode'> Nulls: False Unique values: 70 5 most frequent values: ecr_i: 2811660 private: 1100831 http: 623091 smtp: 96554 other: 72653 Max length: 11 4. flag <type 'unicode'> Nulls: False Unique values: 11 5 most frequent values: SF: 3744328 S0: 869829 REJ: 268874 RSTR: 8094 RSTO: 5344 Max length: 6 5. src_bytes <type 'int'> Nulls: False Min: 0 Max: 1379963888 Sum: 8986765238 Mean: 1834.62117523 Median: 520 Standard Deviation: 941430.978396 Unique values: 7195 5 most frequent values: 1032: 2280245 0: 1152546 520: 527731 105: 73899 147: 27324 6. dst_bytes <type 'int'> Nulls: False Min: 0 Max: 1309937401 Sum: 5357035893 Mean: 1093.62281371 Median: 0 Standard Deviation: 645012.267904 Unique values: 21493 5 most frequent values: 0: 4064854 105: 44713 147: 24910 146: 22536 145: 9500 7. interaction_type <type 'unicode'> Nulls: False Unique values: 23 5 most frequent values: smurf.: 2807886 neptune.: 1072017 normal.: 972781 satan.: 15892 ipsweep.: 12481 Max length: 16 Row count: 4898431 ! csvcut - c 1 - 6 , 42 ../ data / raw / kdd . csv \\ | datamash - t , - sHi crosstab 4 , 2 mean 1 \\ | sed '1d' | csvlook |---------+------+--------------------+------------------| | | icmp | tcp | udp | |---------+------+--------------------+------------------| | OTH | N/A | 0 | N/A | | REJ | N/A | 0.0012384983300728 | N/A | | RSTO | N/A | 56.963323353293 | N/A | | RSTOS0 | N/A | 2876.7049180328 | N/A | | RSTR | N/A | 3323.1266370151 | N/A | | S0 | N/A | 0 | N/A | | S1 | N/A | 0 | N/A | | S2 | N/A | 6.2670807453416 | N/A | | S3 | N/A | 429.98 | N/A | | SF | 0 | 8.5918701456395 | 1045.2031520217 | | SH | N/A | 0 | N/A | |---------+------+--------------------+------------------| ! csvcut - n ../ data / raw / kdd . csv 1: duration 2: protocol_type 3: service 4: flag 5: src_bytes 6: dst_bytes 7: land 8: wrong_fragment 9: urgent 10: hot 11: num_failed_logins 12: logged_in 13: num_compromised 14: root_shell 15: su_attempted 16: num_root 17: num_file_creations 18: num_shells 19: num_access_files 20: num_outbound_cmds 21: is_host_login 22: is_guest_login 23: count 24: srv_count 25: serror_rate 26: srv_serror_rate 27: rerror_rate 28: srv_rerror_rate 29: same_srv_rate 30: diff_srv_rate 31: srv_diff_host_rate 32: dst_host_count 33: dst_host_srv_count 34: dst_host_same_srv_rate 35: dst_host_diff_srv_rate 36: dst_host_same_src_port_rate 37: dst_host_srv_diff_host_rate 38: dst_host_serror_rate 39: dst_host_srv_serror_rate 40: dst_host_rerror_rate 41: dst_host_srv_rerror_rate 42: interaction_type # crosstabs with columns created on-the-fly ! cat ../ data / raw / kdd . csv \\ | mlr -- csv -- rs lf put '$attack = $interaction_type!=\"normal.\"' \\ | datamash - t , - sHi crosstab 2 , 43 mean 1 | sed '1d' | csvlook |-------+-----------------+---------------------| | | false | true | |-------+-----------------+---------------------| | icmp | 0 | 0 | | tcp | 11.481294964029 | 22.602477657342 | | udp | 1061.2623387754 | 0.0013605442176871 | |-------+-----------------+---------------------| ! cat ../ data / raw / kdd . csv \\ | mlr -- csv -- rs lf put '$attack = $interaction_type!=\"normal.\"' \\ | datamash - t , - sHi crosstab 2 , 43 mean 1 sstdev 1 | csvlook # only one aggfunc allowed at a time datamash: crosstab supports one operation, found 2 transpose , reverse (columns/rows) \u00b6 ! cat fp2 . csv C00,C01,C02,C03,C04,C05,C06,C07,C08,C09 ,-0.65,,-0.51,,2.1,,-0.01,,-0.11 0.14,0.24,-0.91,-0.54,0.72,1.14,-0.07,0.02,-0.05,1.07 ,0.81,,-1.1,,1.01,,-1.88,,-3.18 1.14,-0.25,-2.19,0.4,1.34,-0.96,0.27,0.26,-1.02,-0.37 ,0.34,,-0.81,,0.8,,1.13,,-0.15 -2.23,-0.9,-0.52,0.54,-1.07,1.47,-0.58,-0.46,0.53,-1.79 ,-0.44,,-0.48,,-0.73,,-0.28,,-1.94 -2.73,-0.22,1.87,0.92,-0.02,-0.44,0.48,1.48,-1.62,0.92 ,-0.31,,-0.58,,0.46,,0.1,,0.81 -1.36,0.29,-0.73,0.34,-1.85,-1.64,1.15,-0.23,-1.35,1.11 ! datamash - t , transpose < fp2 . csv | csvlook |------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------| | C00 | | 0.14 | | 1.14 | | -2.23 | | -2.73 | | -1.36 | |------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------| | C01 | -0.65 | 0.24 | 0.81 | -0.25 | 0.34 | -0.9 | -0.44 | -0.22 | -0.31 | 0.29 | | C02 | | -0.91 | | -2.19 | | -0.52 | | 1.87 | | -0.73 | | C03 | -0.51 | -0.54 | -1.1 | 0.4 | -0.81 | 0.54 | -0.48 | 0.92 | -0.58 | 0.34 | | C04 | | 0.72 | | 1.34 | | -1.07 | | -0.02 | | -1.85 | | C05 | 2.1 | 1.14 | 1.01 | -0.96 | 0.8 | 1.47 | -0.73 | -0.44 | 0.46 | -1.64 | | C06 | | -0.07 | | 0.27 | | -0.58 | | 0.48 | | 1.15 | | C07 | -0.01 | 0.02 | -1.88 | 0.26 | 1.13 | -0.46 | -0.28 | 1.48 | 0.1 | -0.23 | | C08 | | -0.05 | | -1.02 | | 0.53 | | -1.62 | | -1.35 | | C09 | -0.11 | 1.07 | -3.18 | -0.37 | -0.15 | -1.79 | -1.94 | 0.92 | 0.81 | 1.11 | |------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------| ! head ../ data / raw / fromPandas . csv | csvlook |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | C00 | D01 | C02 | D03 | D04 | A05 | A06 | B07 | A08 | C09 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | PO | Alert | 0.08 | -0.29 | 1.04 | 0.5 | -0.19 | 0.92 | -1.18 | 0.46 | | PO | Alert | 0.86 | 1.81 | 2.28 | 1.58 | 0.79 | 1.19 | 0.99 | -1.17 | | PO | Critical | -0.04 | 0.52 | -0.52 | 0.34 | 2.09 | -0.6 | 0.85 | -1.14 | | AR | Critical | -0.28 | 0.56 | 0.69 | 0.62 | -0.28 | -0.25 | 0.64 | -2.03 | | AR | Critical | 0.04 | -0.38 | -0.66 | -1.76 | -0.13 | 0.6 | -2.06 | 0.96 | | PO | Critical | 0.77 | 1.48 | 0.16 | -0.59 | 0.94 | 0.48 | -0.32 | -0.55 | | AR | Alert | -0.15 | 0.03 | -2.05 | -0.56 | -1.04 | 1.29 | 0.51 | -0.01 | | PO | Alert | -0.17 | -0.4 | -0.16 | 2.65 | -0.48 | 0.25 | -1.1 | 0.77 | | AR | Alert | -0.57 | -0.31 | -0.14 | -2.89 | 0.52 | 0.18 | -0.03 | 0.47 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| # transpose ! head ../ data / raw / fromPandas . csv \\ | datamash - t , transpose | csvlook |------+-------+-------+----------+----------+----------+----------+-------+-------+--------| | C00 | PO | PO | PO | AR | AR | PO | AR | PO | AR | |------+-------+-------+----------+----------+----------+----------+-------+-------+--------| | D01 | Alert | Alert | Critical | Critical | Critical | Critical | Alert | Alert | Alert | | C02 | 0.08 | 0.86 | -0.04 | -0.28 | 0.04 | 0.77 | -0.15 | -0.17 | -0.57 | | D03 | -0.29 | 1.81 | 0.52 | 0.56 | -0.38 | 1.48 | 0.03 | -0.4 | -0.31 | | D04 | 1.04 | 2.28 | -0.52 | 0.69 | -0.66 | 0.16 | -2.05 | -0.16 | -0.14 | | A05 | 0.5 | 1.58 | 0.34 | 0.62 | -1.76 | -0.59 | -0.56 | 2.65 | -2.89 | | A06 | -0.19 | 0.79 | 2.09 | -0.28 | -0.13 | 0.94 | -1.04 | -0.48 | 0.52 | | B07 | 0.92 | 1.19 | -0.6 | -0.25 | 0.6 | 0.48 | 1.29 | 0.25 | 0.18 | | A08 | -1.18 | 0.99 | 0.85 | 0.64 | -2.06 | -0.32 | 0.51 | -1.1 | -0.03 | | C09 | 0.46 | -1.17 | -1.14 | -2.03 | 0.96 | -0.55 | -0.01 | 0.77 | 0.47 | |------+-------+-------+----------+----------+----------+----------+-------+-------+--------| By default, transpose verifies the input has the same number of fields in each line, and fails with an error otherwise Use --no-strict to allow missing values Use --filler to set the missing-field filler value # to reverse the order of columns ! head ../ data / raw / fromPandas . csv \\ | datamash - t , reverse | csvlook |--------+-------+-------+-------+-------+-------+-------+-------+----------+------| | C09 | A08 | B07 | A06 | A05 | D04 | D03 | C02 | D01 | C00 | |--------+-------+-------+-------+-------+-------+-------+-------+----------+------| | 0.46 | -1.18 | 0.92 | -0.19 | 0.5 | 1.04 | -0.29 | 0.08 | Alert | PO | | -1.17 | 0.99 | 1.19 | 0.79 | 1.58 | 2.28 | 1.81 | 0.86 | Alert | PO | | -1.14 | 0.85 | -0.6 | 2.09 | 0.34 | -0.52 | 0.52 | -0.04 | Critical | PO | | -2.03 | 0.64 | -0.25 | -0.28 | 0.62 | 0.69 | 0.56 | -0.28 | Critical | AR | | 0.96 | -2.06 | 0.6 | -0.13 | -1.76 | -0.66 | -0.38 | 0.04 | Critical | AR | | -0.55 | -0.32 | 0.48 | 0.94 | -0.59 | 0.16 | 1.48 | 0.77 | Critical | PO | | -0.01 | 0.51 | 1.29 | -1.04 | -0.56 | -2.05 | 0.03 | -0.15 | Alert | AR | | 0.77 | -1.1 | 0.25 | -0.48 | 2.65 | -0.16 | -0.4 | -0.17 | Alert | PO | | 0.47 | -0.03 | 0.18 | 0.52 | -2.89 | -0.14 | -0.31 | -0.57 | Alert | AR | |--------+-------+-------+-------+-------+-------+-------+-------+----------+------| # Remember that `tac` reverses the rows ! head ../ data / raw / fromPandas . csv \\ | tac | csvlook |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | AR | Alert | -0.57 | -0.31 | -0.14 | -2.89 | 0.52 | 0.18 | -0.03 | 0.47 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | PO | Alert | -0.17 | -0.4 | -0.16 | 2.65 | -0.48 | 0.25 | -1.1 | 0.77 | | AR | Alert | -0.15 | 0.03 | -2.05 | -0.56 | -1.04 | 1.29 | 0.51 | -0.01 | | PO | Critical | 0.77 | 1.48 | 0.16 | -0.59 | 0.94 | 0.48 | -0.32 | -0.55 | | AR | Critical | 0.04 | -0.38 | -0.66 | -1.76 | -0.13 | 0.6 | -2.06 | 0.96 | | AR | Critical | -0.28 | 0.56 | 0.69 | 0.62 | -0.28 | -0.25 | 0.64 | -2.03 | | PO | Critical | -0.04 | 0.52 | -0.52 | 0.34 | 2.09 | -0.6 | 0.85 | -1.14 | | PO | Alert | 0.86 | 1.81 | 2.28 | 1.58 | 0.79 | 1.19 | 0.99 | -1.17 | | PO | Alert | 0.08 | -0.29 | 1.04 | 0.5 | -0.19 | 0.92 | -1.18 | 0.46 | | C00 | D01 | C02 | D03 | D04 | A05 | A06 | B07 | A08 | C09 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| binning (or discretization ) \u00b6 ! head ../ data / raw / fromPandas . csv | csvlook |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | C00 | D01 | C02 | D03 | D04 | A05 | A06 | B07 | A08 | C09 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | PO | Alert | 0.08 | -0.29 | 1.04 | 0.5 | -0.19 | 0.92 | -1.18 | 0.46 | | PO | Alert | 0.86 | 1.81 | 2.28 | 1.58 | 0.79 | 1.19 | 0.99 | -1.17 | | PO | Critical | -0.04 | 0.52 | -0.52 | 0.34 | 2.09 | -0.6 | 0.85 | -1.14 | | AR | Critical | -0.28 | 0.56 | 0.69 | 0.62 | -0.28 | -0.25 | 0.64 | -2.03 | | AR | Critical | 0.04 | -0.38 | -0.66 | -1.76 | -0.13 | 0.6 | -2.06 | 0.96 | | PO | Critical | 0.77 | 1.48 | 0.16 | -0.59 | 0.94 | 0.48 | -0.32 | -0.55 | | AR | Alert | -0.15 | 0.03 | -2.05 | -0.56 | -1.04 | 1.29 | 0.51 | -0.01 | | PO | Alert | -0.17 | -0.4 | -0.16 | 2.65 | -0.48 | 0.25 | -1.1 | 0.77 | | AR | Alert | -0.57 | -0.31 | -0.14 | -2.89 | 0.52 | 0.18 | -0.03 | 0.47 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| # returns the lower edge of the window ! head ../ data / raw / fromPandas . csv | csvcut - c C02 \\ | datamash - H -- full bin : 0.25 1 \\ | mlr -- csv -- ifs tab -- ofs ',' cat \\ | csvlook |--------+-----------| | C02 | bin(C02) | |--------+-----------| | 0.08 | 0 | | 0.86 | 0.75 | | -0.04 | -0.25 | | -0.28 | -0.5 | | 0.04 | 0 | | 0.77 | 0.75 | | -0.15 | -0.25 | | -0.17 | -0.25 | | -0.57 | -0.75 | |--------+-----------| round, trunc, ceil, floor, frac \u00b6 ! head ../ data / raw / fromPandas . csv | csvcut - c C02 \\ | datamash - H -- full round 1 trunc 1 ceil 1 floor 1 frac 1 \\ | mlr -- csv -- ifs tab -- ofs ',' cat \\ | csvlook |--------+------------+------------+-----------+------------+------------| | C02 | round(C02) | trunc(C02) | ceil(C02) | floor(C02) | frac(C02) | |--------+------------+------------+-----------+------------+------------| | 0.08 | 0 | 0 | 1 | 0 | 0.08 | | 0.86 | 1 | 0 | 1 | 0 | 0.86 | | -0.04 | 0 | 0 | 0 | -1 | -0.04 | | -0.28 | 0 | 0 | 0 | -1 | -0.28 | | 0.04 | 0 | 0 | 1 | 0 | 0.04 | | 0.77 | 1 | 0 | 1 | 0 | 0.77 | | -0.15 | 0 | 0 | 0 | -1 | -0.15 | | -0.17 | 0 | 0 | 0 | -1 | -0.17 | | -0.57 | -1 | 0 | 0 | -1 | -0.57 | |--------+------------+------------+-----------+------------+------------| datamash Analysis Examples \u00b6 ! head ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 | csvlook |-----------+---------------+---------+------+-----------+-----------+-------------------| | duration | protocol_type | service | flag | src_bytes | dst_bytes | interaction_type | |-----------+---------------+---------+------+-----------+-----------+-------------------| | 0 | tcp | http | SF | 215 | 45076 | normal. | | 0 | tcp | http | SF | 162 | 4528 | normal. | | 0 | tcp | http | SF | 236 | 1228 | normal. | | 0 | tcp | http | SF | 233 | 2032 | normal. | | 0 | tcp | http | SF | 239 | 486 | normal. | | 0 | tcp | http | SF | 238 | 1282 | normal. | | 0 | tcp | http | SF | 235 | 1337 | normal. | | 0 | tcp | http | SF | 234 | 1364 | normal. | | 0 | tcp | http | SF | 239 | 1295 | normal. | |-----------+---------------+---------+------+-----------+-----------+-------------------| # frequency tables ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | datamash - sHi - t , -- group 7 count 7 | csvlook |----------------------------+--------------------------| | GroupBy(interaction_type) | count(interaction_type) | |----------------------------+--------------------------| | back. | 2203 | | buffer_overflow. | 30 | | ftp_write. | 8 | | guess_passwd. | 53 | | imap. | 12 | | ipsweep. | 12481 | | land. | 21 | | loadmodule. | 9 | | multihop. | 7 | | neptune. | 1072017 | | nmap. | 2316 | | normal. | 972781 | | perl. | 3 | | phf. | 4 | | pod. | 264 | | portsweep. | 10413 | | rootkit. | 10 | | satan. | 15892 | | smurf. | 2807886 | | spy. | 2 | | teardrop. | 979 | | warezclient. | 1020 | | warezmaster. | 20 | |----------------------------+--------------------------| # countunique ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | datamash - sHi - t , -- group 2 countunique 7 | csvlook |-------------------------+--------------------------------| | GroupBy(protocol_type) | countunique(interaction_type) | |-------------------------+--------------------------------| | icmp | 7 | | tcp | 20 | | udp | 5 | |-------------------------+--------------------------------| # groupby 1 ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | datamash - sHi - t , -- group 2 mean 5 sstdev 5 \\ | csvlook |-------------------------+-----------------+--------------------| | GroupBy(protocol_type) | mean(src_bytes) | sstdev(src_bytes) | |-------------------------+-----------------+--------------------| | icmp | 927.89168938556 | 216.02705205438 | | tcp | 3388.5699653266 | 1523443.9737618 | | udp | 97.227728938483 | 47.667234078966 | |-------------------------+-----------------+--------------------| # groupby 2 ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | mlr -- csv -- rs lf put '$attack = $interaction_type!=\"normal.\"' \\ | datamash - sHi - t , -- group 2 , 8 mean 1 sstdev 1 2 | csvlook |-------------------------+-----------------+--------------------+--------------------| | GroupBy(protocol_type) | GroupBy(attack) | mean(duration) | sstdev(duration) | |-------------------------+-----------------+--------------------+--------------------| | icmp | false | 0 | 0 | | icmp | true | 0 | 0 | | tcp | false | 11.481294964029 | 376.54007768793 | | tcp | true | 22.602477657342 | 824.78914040215 | | udp | false | 1061.2623387754 | 2799.4403718827 | | udp | true | 0.0013605442176871 | 0.073771111356332 | |-------------------------+-----------------+--------------------+--------------------| # groupby 2 ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | mlr -- csv -- rs lf put '$attack = $interaction_type!=\"normal.\"' \\ | datamash - sHi - t , -- group 2 , 8 mean 1 sstdev 1 \\ | csvlook |-------------------------+-----------------+--------------------+--------------------| | GroupBy(protocol_type) | GroupBy(attack) | mean(duration) | sstdev(duration) | |-------------------------+-----------------+--------------------+--------------------| | icmp | false | 0 | 0 | | icmp | true | 0 | 0 | | tcp | false | 11.481294964029 | 376.54007768793 | | tcp | true | 22.602477657342 | 824.78914040215 | | udp | false | 1061.2623387754 | 2799.4403718827 | | udp | true | 0.0013605442176871 | 0.073771111356332 | |-------------------------+-----------------+--------------------+--------------------|","title":"Datamash"},{"location":"07_datamash/#datamash","text":"https://www.gnu.org/software/datamash/manual/datamash.html https://www.gnu.org/software/datamash/manual/html_node/Available-Operations.html","title":"datamash"},{"location":"07_datamash/#installation-notes","text":"If you use sudo apt install datamash , check the version with datamash --version We need version 1.1.0 and above (for functions like crosstab ) If not, follow these steps # download the installer wget -P /tmp/ ftp://ftp.gnu.org/gnu/datamash/datamash-1.1.1.tar.gz # untar the file mkdir /tmp/datamash tar -xzf /tmp/datamash-1.1.1.tar.gz -C /tmp/datamash/ # build cd /tmp/datamash ./configure make sudo make install # if `datamash` doesn't run, copy the executable to the /usr/bin/ folder sudo cp ./datamash /usr/bin/datamash # run this to check datamash --version # read the documentation datamash --help man datamash info datamash","title":"Installation Notes"},{"location":"07_datamash/#usage","text":"Where op1 is the operation to perform on the values in column1. datamash reads input from stdin and performs one or more operations on the input data. If --group is used, each operation is performed on every group. If --group is not used, each operation is performed on all the values in the input file. Syntax datamash [option]\u2026 op1 column1 [op2 column2 \u2026]","title":"Usage"},{"location":"07_datamash/#highlights","text":"Title Features Summary Statistics count,min,max,mean,stdev,median,quartiles Header Lines and Column Names Using files with header lines Field Delimiters Tabs, Whitespace, other delimiters Column Ranges Operating on multiple columns Groupby Groupby , count, collapse Check Validate tabular structure Crosstab Cross-tabulation ( pivot-tables ) Rounding numbers round, ceil, floor, trunc, frac Binning numbers assigning numbers into fixed number of buckets Binning strings assigning strings into fixed number of buckets File operations transpose, reverse Line-Filtering operations rmdup Per-Line operations base64, debase64, md5, sha1, sha256, sha512 Numeric Grouping operations sum, min, max, absmin, absmax Textual/Numeric Grouping operations count, first, last, rand, unique, collapse, countunique Statistical Grouping operations 1 mean, median, q1, q3, iqr, mode, antimode, pstdev, sstdev Statistical Grouping Operations 2 , pvar, svar, mad, madraw, pskew, sskew, pkurt, skurt, dpo, jarque","title":"Highlights"},{"location":"07_datamash/#also","text":"fills NAs! remove duplicates!","title":"Also"},{"location":"07_datamash/#moar-options","text":"Grouping Options: -f, --full print entire input line before op results (default: print only the grouped keys) -g, --group=X[,Y,Z] group via fields X,[Y,Z] -H, --headers first input line is column headers, print column headers as first line -i, --ignore-case ignore upper/lower case when comparing text; this affects grouping, and string operations -s, --sort sort the input before grouping; this removes the need to manually pipe the input through 'sort' File Operation Options: --no-strict allow lines with varying number of fields --filler=X fill missing values with X (default %s) General Options: -t, --field-separator=X use X instead of TAB as field delimiter --narm skip NA/NaN values -W, --whitespace use whitespace (one or more spaces and/or tabs) for field delimiters ! ls - l ../ data / raw | grep csv -rw-rw-r-- 1 dk dk 3619 May 16 10:51 countrynames.csv -rw-rw-r-- 1 dk dk 1489911208 May 16 10:51 Crimes_Chicago.csv -rw------- 1 dk dk 702878193 May 16 10:51 Flight_Delays.csv -rw------- 1 dk dk 702878193 May 16 23:10 flights.csv -rw-rw-r-- 1 dk dk 523 May 16 23:45 fromPandas_01.csv -rw-rw-r-- 1 dk dk 532 May 16 23:45 fromPandas_02.csv -rw-rw-r-- 1 dk dk 535743168 May 16 15:42 fromPandas.csv -rw-rw-r-- 1 dk dk 57 May 16 10:51 het-bool.csv -rw-rw-r-- 1 dk dk 742580451 May 16 18:59 kdd.csv -rw-rw-r-- 1 dk dk 28512235 May 16 18:49 millionSongsSample.csv -rw-rw-r-- 1 dk dk 10136806470 May 16 10:52 NYC__311Requests.csv -rw-rw-r-- 1 dk dk 151488712 May 16 10:52 worldcitiespop.csv","title":"MOAR Options"},{"location":"07_datamash/#basic-stats","text":"! head ../ data / raw / fromPandas . csv | csvlook |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | C00 | D01 | C02 | D03 | D04 | A05 | A06 | B07 | A08 | C09 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | PO | Alert | 0.08 | -0.29 | 1.04 | 0.5 | -0.19 | 0.92 | -1.18 | 0.46 | | PO | Alert | 0.86 | 1.81 | 2.28 | 1.58 | 0.79 | 1.19 | 0.99 | -1.17 | | PO | Critical | -0.04 | 0.52 | -0.52 | 0.34 | 2.09 | -0.6 | 0.85 | -1.14 | | AR | Critical | -0.28 | 0.56 | 0.69 | 0.62 | -0.28 | -0.25 | 0.64 | -2.03 | | AR | Critical | 0.04 | -0.38 | -0.66 | -1.76 | -0.13 | 0.6 | -2.06 | 0.96 | | PO | Critical | 0.77 | 1.48 | 0.16 | -0.59 | 0.94 | 0.48 | -0.32 | -0.55 | | AR | Alert | -0.15 | 0.03 | -2.05 | -0.56 | -1.04 | 1.29 | 0.51 | -0.01 | | PO | Alert | -0.17 | -0.4 | -0.16 | 2.65 | -0.48 | 0.25 | -1.1 | 0.77 | | AR | Alert | -0.57 | -0.31 | -0.14 | -2.89 | 0.52 | 0.18 | -0.03 | 0.47 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| ! datamash - sHi - t , sum 3 - 6 < ../ data / raw / fromPandas . csv | csvlook |-----------+----------+----------+-----------| | sum(C02) | sum(D03) | sum(D04) | sum(A05) | |-----------+----------+----------+-----------| | 4083.25 | 3448.1 | -2271.68 | -236.88 | |-----------+----------+----------+-----------| ! datamash - t , - H min 3 q1 3 median 3 q3 3 max 3 < ../ data / raw / fromPandas . csv | csvlook |-----------+---------+-------------+---------+-----------| | min(C02) | q1(C02) | median(C02) | q3(C02) | max(C02) | |-----------+---------+-------------+---------+-----------| | -5.21 | -0.67 | -0 | 0.68 | 5.19 | |-----------+---------+-------------+---------+-----------| ! datamash - t , - H mean A06 sstdev A06 < ../ data / raw / fromPandas . csv | csvlook |--------------+------------------| | mean(A06) | sstdev(A06) | |--------------+------------------| | 0.000461629 | 1.0001281946602 | |--------------+------------------|","title":"basic stats"},{"location":"07_datamash/#groupby-operations","text":"! csvcut - c interaction_type , src_bytes , dst_bytes ../ data / raw / kdd . csv | head | csvlook |-------------------+-----------+------------| | interaction_type | src_bytes | dst_bytes | |-------------------+-----------+------------| | normal. | 215 | 45076 | | normal. | 162 | 4528 | | normal. | 236 | 1228 | | normal. | 233 | 2032 | | normal. | 239 | 486 | | normal. | 238 | 1282 | | normal. | 235 | 1337 | | normal. | 234 | 1364 | | normal. | 239 | 1295 | |-------------------+-----------+------------| ! datamash - sHi - t , \\ -- group interaction_type \\ mean src_bytes , dst_bytes \\ sstdev src_bytes , dst_bytes < ../ data / raw / kdd . csv | csvlook # -t, because our file is comma delimited # --sort because groupby needs it # -H because the input has headers and we want the output to have them too # --group for group-by # -i to ignore case # count is the aggregation function used (Remember the syntax <op1> <col1>) |----------------------------+--------------------+---------------------+-------------------+--------------------| | GroupBy(interaction_type) | mean(src_bytes) | mean(dst_bytes) | sstdev(src_bytes) | sstdev(dst_bytes) | |----------------------------+--------------------+---------------------+-------------------+--------------------| | back. | 54156.355878348 | 8232.6495687699 | 3159.3602320464 | 616.23179457446 | | buffer_overflow. | 1400.4333333333 | 6339.8333333333 | 1337.1326162103 | 12440.664773466 | | ftp_write. | 220.75 | 5382.25 | 267.74761570234 | 13793.737530903 | | guess_passwd. | 125.33962264151 | 216.18867924528 | 3.037859799687 | 257.50228203647 | | imap. | 347.58333333333 | 54948.666666667 | 629.92603582172 | 187158.40119454 | | ipsweep. | 10.436583607083 | 4.394359426328 | 37.09492550297 | 462.19579938396 | | land. | 0 | 0 | 0 | 0 | | loadmodule. | 151.88888888889 | 3009.8888888889 | 127.74529780431 | 2907.4277052252 | | multihop. | 435.14285714286 | 213016.28571429 | 540.96038936057 | 382586.00499962 | | neptune. | 0.0099942444942571 | 0.00082088250466177 | 10.347866229879 | 0.84992741107836 | | nmap. | 24.424006908463 | 0.13255613126079 | 60.37052484509 | 4.7761242620571 | | normal. | 1477.846250081 | 3234.6501113817 | 110500.41940105 | 34231.680610548 | | perl. | 265.66666666667 | 2444 | 4.9328828623162 | 166.13548687743 | | phf. | 51 | 8127 | 0 | 0 | | pod. | 1462.6515151515 | 0 | 125.0980442777 | 0 | | portsweep. | 431708.31182176 | 202681.31643138 | 20383536.155508 | 13983601.671997 | | rootkit. | 294.7 | 4276.6 | 538.57817961328 | 7558.5449540386 | | satan. | 0.99874150515983 | 2.1274855273093 | 35.927415459528 | 145.10315741745 | | smurf. | 935.7730962012 | 0 | 200.02142879934 | 0 | | spy. | 174.5 | 1193.5 | 88.388347648318 | 490.02499936228 | | teardrop. | 28 | 0.057201225740552 | 0 | 1.2649097429583 | | warezclient. | 300219.5627451 | 719.31764705882 | 1200905.2431303 | 1104.4083239047 | | warezmaster. | 49.3 | 3922087.7 | 212.15513192002 | 2197498.9594262 | |----------------------------+--------------------+---------------------+-------------------+--------------------| ! csvcut - c 9 , 10 , 15 - 19 ../ data / raw / flights . csv | head | csvlook |----------------+-----------+----------+----------+--------+------+-----------| | UniqueCarrier | FlightNum | ArrDelay | DepDelay | Origin | Dest | Distance | |----------------+-----------+----------+----------+--------+------+-----------| | WN | 2891 | 1 | 7 | SMF | ONT | 389 | | WN | 462 | 8 | 13 | SMF | PDX | 479 | | WN | 1229 | 34 | 36 | SMF | PDX | 479 | | WN | 1355 | 26 | 30 | SMF | PDX | 479 | | WN | 2278 | -3 | 1 | SMF | PDX | 479 | | WN | 2386 | 3 | 10 | SMF | PDX | 479 | | WN | 409 | 47 | 56 | SMF | PHX | 647 | | WN | 1131 | -2 | 9 | SMF | PHX | 647 | | WN | 1212 | 44 | 47 | SMF | PHX | 647 | |----------------+-----------+----------+----------+--------+------+-----------| # exclude missing data manually ! csvtk filter - f \"15-16>0\" ../ data / raw / flights . csv \\ | datamash - t , - sHi - g UniqueCarrier , Origin , Dest \\ mean ArrDelay , DepDelay \\ | csvsort - r - c 'mean(ArrDelay)' \\ | head | csvlook |-------------------------+-----------------+---------------+----------------+-----------------| | GroupBy(UniqueCarrier) | GroupBy(Origin) | GroupBy(Dest) | mean(ArrDelay) | mean(DepDelay) | |-------------------------+-----------------+---------------+----------------+-----------------| | B6 | ONT | IAD | 370.0 | 386.0 | | XE | ELP | MFE | 316.0 | 307.0 | | DL | SLC | KOA | 308.0 | 317.5 | | OH | ACY | MYR | 252.0 | 222.0 | | EV | RIC | CVG | 243.0 | 262.0 | | UA | OAK | LAX | 231.0 | 248.0 | | OH | MDT | CLE | 219.0 | 209.0 | | OH | JAX | CMH | 217.0 | 165.0 | | NW | DCA | PLN | 210.0 | 168.0 | |-------------------------+-----------------+---------------+----------------+-----------------| ! csvtk filter - f \"15-16=' '\" ../ data / raw / flights . csv | head \u001b[31m[ERRO]\u001b[0m invalid filter: 15-16=' ' ! mlr ! datamash - t , - sHi - g UniqueCarrier , Origin , Dest \\ mean ArrDelay , DepDelay < ../ data / raw / flights . csv | head | csvlook datamash: invalid numeric value in line 2 field 15: 'NA' sort: write failed: 'standard output': Broken pipe sort: write error |-------------------------+-----------------+---------------+----------------+-----------------| | GroupBy(UniqueCarrier) | GroupBy(Origin) | GroupBy(Dest) | mean(ArrDelay) | mean(DepDelay) | |-------------------------+-----------------+---------------+----------------+-----------------|","title":"groupby operations"},{"location":"07_datamash/#dealing-with-missing-data","text":"check with csvstat --nulls remove with awk fill with datamash ! csvstat - c 15 - 16 -- nulls ../ data / raw / flights . csv 15. ArrDelay: True 16. DepDelay: True Drop rows with missing data in a particular column # ignore rows with null values in column 5 $ awk -F, '!length($5)' file # 1,abc,543,87,,fsg; # 1,abc,543,88,,fsg; import pandas as pd import numpy as np df = pd . DataFrame ( data = np . random . randn ( 10 , 10 ) . round ( 2 )) df . iloc [:: 2 , :: 2 ] = np . nan df . columns = [ 'C' + str ( i ) . zfill ( 2 ) for i in range ( 10 )] df . to_csv ( 'fp2.csv' , index = False ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } C00 C01 C02 C03 C04 C05 C06 C07 C08 C09 0 NaN -0.65 NaN -0.51 NaN 2.10 NaN -0.01 NaN -0.11 1 0.14 0.24 -0.91 -0.54 0.72 1.14 -0.07 0.02 -0.05 1.07 2 NaN 0.81 NaN -1.10 NaN 1.01 NaN -1.88 NaN -3.18 3 1.14 -0.25 -2.19 0.40 1.34 -0.96 0.27 0.26 -1.02 -0.37 4 NaN 0.34 NaN -0.81 NaN 0.80 NaN 1.13 NaN -0.15 5 -2.23 -0.90 -0.52 0.54 -1.07 1.47 -0.58 -0.46 0.53 -1.79 6 NaN -0.44 NaN -0.48 NaN -0.73 NaN -0.28 NaN -1.94 7 -2.73 -0.22 1.87 0.92 -0.02 -0.44 0.48 1.48 -1.62 0.92 8 NaN -0.31 NaN -0.58 NaN 0.46 NaN 0.10 NaN 0.81 9 -1.36 0.29 -0.73 0.34 -1.85 -1.64 1.15 -0.23 -1.35 1.11","title":"dealing with missing data"},{"location":"07_datamash/#checking-for-nulls","text":"","title":"Checking for nulls"},{"location":"07_datamash/#checking-for-missing-or-extra-fields","text":"! datamash - t , check < fp2 . csv 11 lines, 10 fields %% writefile fp3 . csv C01 , C02 , C03 0 , 10 , 100 0000 , 0001 , 0010 0011 , 0100 , 0101 , 0110 0111 , 1000 , 1001 Overwriting fp3.csv ! datamash - t , check < fp3 . csv line 3 (3 fields): 0000,0001,0010 line 4 (4 fields): 0011,0100,0101,0110 datamash: check failed: line 4 has 4 fields (previous line had 3)","title":"Checking for missing or extra fields"},{"location":"07_datamash/#checking-for-nulls_1","text":"! csvstat -- nulls fp2 . csv 1. C00: True 2. C01: False 3. C02: True 4. C03: False 5. C04: True 6. C05: False 7. C06: True 8. C07: False 9. C08: True 10. C09: False","title":"Checking for nulls"},{"location":"07_datamash/#filtering-for-rows-with-nulls","text":"! csvsql -- query \"SELECT * \\ FROM fp2 \\ WHERE C00 IS NULL\" fp2 . csv | csvlook |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| | C00 | C01 | C02 | C03 | C04 | C05 | C06 | C07 | C08 | C09 | |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| | | -0.04 | | 2.58 | | 0.65 | | 0.45 | | -0.47 | | | 1.19 | | 1.35 | | -0.04 | | 0.06 | | -0.08 | | | -1.7 | | -0.38 | | -0.93 | | -1.17 | | -0.22 | | | 0.05 | | 0.94 | | -0.44 | | 0.56 | | -0.59 | | | -1.7 | | -2.03 | | -1.15 | | 1.84 | | 0.11 | |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| 194.78 . 85.8 # select rows where C00 isnull ! mlr -- csv -- rs lf cat \\ then filter 'is_null($C00)' fp2 . csv | csvlook |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| | C00 | C01 | C02 | C03 | C04 | C05 | C06 | C07 | C08 | C09 | |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| | | -0.04 | | 2.58 | | 0.65 | | 0.45 | | -0.47 | | | 1.19 | | 1.35 | | -0.04 | | 0.06 | | -0.08 | | | -1.7 | | -0.38 | | -0.93 | | -1.17 | | -0.22 | | | 0.05 | | 0.94 | | -0.44 | | 0.56 | | -0.59 | | | -1.7 | | -2.03 | | -1.15 | | 1.84 | | 0.11 | |------+-------+-----+-------+-----+-------+-----+-------+-----+--------| ! mlr -- csv -- rs lf cat \\ then filter '$ArrDelay == \"NA\"' ../ data / raw / flights . csv | head | csvcut - c ArrDelay , Origin , Dest | csvlook |-----------+--------+-------| | ArrDelay | Origin | Dest | |-----------+--------+-------| | NA | SNA | LAS | | NA | AUS | DAL | | NA | DAL | AUS | | NA | DAL | HOU | | NA | DAL | HOU | | NA | DAL | HOU | | NA | HOU | DAL | | NA | HOU | DAL | | NA | HOU | DAL | |-----------+--------+-------| ! csvgrep - c ArrDelay - r NA ../ data / raw / flights . csv | head | cut - d , - f5 , 7 , 12 , 15 - 18 | csvlook |----------+---------+-------------------+----------+----------+--------+-------| | DepTime | ArrTime | ActualElapsedTime | ArrDelay | DepDelay | Origin | Dest | |----------+---------+-------------------+----------+----------+--------+-------| | NA | NA | NA | NA | NA | SNA | LAS | | NA | NA | NA | NA | NA | AUS | DAL | | NA | NA | NA | NA | NA | DAL | AUS | | NA | NA | NA | NA | NA | DAL | HOU | | NA | NA | NA | NA | NA | DAL | HOU | | NA | NA | NA | NA | NA | DAL | HOU | | NA | NA | NA | NA | NA | HOU | DAL | | NA | NA | NA | NA | NA | HOU | DAL | | NA | NA | NA | NA | NA | HOU | DAL | |----------+---------+-------------------+----------+----------+--------+-------| ! csvgrep - c ArrDelay , DepDelay , DepTime , ArrTime - r NA ../ data / raw / flights . csv | wc - l 160749 ! csvgrep - c ArrDelay , DepDelay , DepTime , ArrTime - r '^NA$' ../ data / raw / flights . csv | wc - l 160749","title":"Filtering for rows with nulls"},{"location":"07_datamash/#markingisolating-rows-with-nulls","text":"! mlr -- csv -- rs lf cat \\ then put '$C00_isnull = is_null($C00)' fp2 . csv | csvlook |--------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------------| | C00 | C01 | C02 | C03 | C04 | C05 | C06 | C07 | C08 | C09 | C00_isnull | |--------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------------| | | -0.04 | | 2.58 | | 0.65 | | 0.45 | | -0.47 | true | | 0.66 | -0.75 | 1.26 | 0.07 | 0.18 | -1.09 | 0.9 | -1.92 | 0.04 | -1.02 | false | | | 1.19 | | 1.35 | | -0.04 | | 0.06 | | -0.08 | true | | 0.37 | 0.79 | -0.63 | -0.18 | 1.51 | -1.55 | -0.82 | 0.46 | 0.47 | 1.02 | false | | | -1.7 | | -0.38 | | -0.93 | | -1.17 | | -0.22 | true | | 0.89 | 1.46 | -2.24 | -1.29 | -0.79 | 0.51 | 1.28 | 1.75 | 0.64 | -1.8 | false | | | 0.05 | | 0.94 | | -0.44 | | 0.56 | | -0.59 | true | | -1.04 | 0.4 | 1.67 | -1.06 | 0.81 | 0.19 | 0.3 | 0.18 | -0.37 | -1.79 | false | | | -1.7 | | -2.03 | | -1.15 | | 1.84 | | 0.11 | true | | 1.24 | 0.29 | 0.72 | -1.67 | -0.16 | 1.7 | 0.5 | 0.09 | -0.77 | -0.49 | false | |--------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------------|","title":"Marking/Isolating rows with Nulls"},{"location":"07_datamash/#crosstabs-pivot-tables","text":"! datamash crosstab -- help Usage: datamash [OPTION] op [fld] [op fld ...] Performs numeric/string operations on input from stdin. 'op' is the operation to perform. If a primary operation is used, it must be listed first, optionally followed by other operations. 'fld' is the input field to use. 'fld' can be a number (1=first field), or a field name when using the -H or --header-in options. Multiple fields can be listed with a comma (e.g. 1,6,8). A range of fields can be listed with a dash (e.g. 2-8). Use colons for operations which require a pair of fields (e.g. 'pcov 2:6'). Primary operations: groupby, crosstab, transpose, reverse, check Line-Filtering operations: rmdup Per-Line operations: base64, debase64, md5, sha1, sha256, sha512, bin, strbin, round, floor, ceil, trunc, frac Numeric Grouping operations: sum, min, max, absmin, absmax Textual/Numeric Grouping operations: count, first, last, rand, unique, collapse, countunique Statistical Grouping operations: mean, median, q1, q3, iqr, mode, antimode, pstdev, sstdev, pvar, svar, mad, madraw, pskew, sskew, pkurt, skurt, dpo, jarque, scov, pcov, spearson, ppearson Grouping Options: -f, --full print entire input line before op results (default: print only the grouped keys) -g, --group=X[,Y,Z] group via fields X,[Y,Z]; equivalent to primary operation 'groupby' --header-in first input line is column headers --header-out print column headers as first line -H, --headers same as '--header-in --header-out' -i, --ignore-case ignore upper/lower case when comparing text; this affects grouping, and string operations -s, --sort sort the input before grouping; this removes the need to manually pipe the input through 'sort' File Operation Options: --no-strict allow lines with varying number of fields --filler=X fill missing values with X (default %s) General Options: -t, --field-separator=X use X instead of TAB as field delimiter --narm skip NA/NaN values -W, --whitespace use whitespace (one or more spaces and/or tabs) for field delimiters -z, --zero-terminated end lines with 0 byte, not newline --help display this help and exit --version output version information and exit Examples: Print the sum and the mean of values from column 1: $ seq 10 | datamash sum 1 mean 1 55 5.5 Transpose input: $ seq 10 | paste - - | datamash transpose 1 3 5 7 9 2 4 6 8 10 For detailed usage information and examples, see man GNU datamash The manual and more examples are available at http://www.gnu.org/software/datamash ! man datamash | grep crosstab <standard input>:161: a space character is not allowed in an escape name groupby, crosstab, transpose, reverse, check numbers (groupby, crosstab) while others do not (reverse,check,trans\u2010 crosstab X,Y [op fld ...] $ datamash -s crosstab 1,2 < input.txt $ datamash -s crosstab 1,2 sum 3 < input.txt $ datamash -s crosstab 1,2 unique 3 < input.txt ! cat ../ data / raw / fromPandas . csv | datamash - t , - s - H crosstab 1 , 2 | sed '1d' | csvlook |-----+---------+----------+--------+-----------| | | Alert | Critical | Ignore | Shutdown | |-----+---------+----------+--------+-----------| | AR | 2700486 | 2274768 | 29093 | 15405 | | EN | 775 | 682 | 7 | 4 | | ES | 2171 | 1762 | 17 | 17 | | FR | 291865 | 246750 | 3147 | 1652 | | PO | 2225076 | 1874976 | 24232 | 12440 | | RU | 158732 | 133364 | 1661 | 918 | |-----+---------+----------+--------+-----------|","title":"crosstabs, pivot tables"},{"location":"07_datamash/#checking-for-nulls_2","text":"! cat ../ data / raw / fromPandas . csv | datamash - t , - s - H crosstab C00 , D01 # default aggfunc is `count` datamash: src/column-headers.c:63: get_input_field_name: Assertion `field_num > 0 && field_num <= num_input_column_headers' failed. GroupBy(C00),GroupBy(D01),Aborted (core dumped) ! cat ../ data / raw / fromPandas . csv | datamash - t , - s - H crosstab 1 , 2 mean 3 | sed '1d' | csvlook |-----+---------------------+----------------------+---------------------+---------------------| | | Alert | Critical | Ignore | Shutdown | |-----+---------------------+----------------------+---------------------+---------------------| | AR | 0.00096527810179353 | 0.00045535193039466 | -0.0031327123363008 | 0.0037702044790652 | | EN | -0.016554838709677 | 0.049560117302053 | 0.24714285714286 | -0.0575 | | ES | 0.011879318286504 | 0.07194665153235 | -0.028823529411765 | 0.16117647058824 | | FR | 0.00023079163311805 | -0.0020542249240122 | 0.0017572291070861 | -0.018807506053269 | | PO | 2.670470581679e-05 | 0.00029508644377315 | -0.012478128095081 | 0.011842443729904 | | RU | 0.0020377743618174 | -6.1860772022435e-05 | 9.59528713759e-21 | -0.012320261437908 | |-----+---------------------+----------------------+---------------------+---------------------| ! mlr cut - h Usage: mlr cut [options] Passes through input records with specified fields included/excluded. -f {a,b,c} Field names to include for cut. -o Retain fields in the order specified here in the argument list. Default is to retain them in the order found in the input data. -x|--complement Exclude, rather than include, field names specified by -f. -r Treat field names as regular expressions. \"ab\", \"a.*b\" will match any field name containing the substring \"ab\" or matching \"a.*b\", respectively; anchors of the form \"^ab$\", \"^a.*b$\" may be used. The -o flag is ignored when -r is present. Examples: mlr cut -f hostname,status mlr cut -x -f hostname,status mlr cut -r -f '^status$,sda[0-9]' mlr cut -r -f '^status$,\"sda[0-9]\"' mlr cut -r -f '^status$,\"sda[0-9]\"i' (this is case-insensitive) ! csvcut - c 1 - 6 , 42 ../ data / raw / kdd . csv | csvstat 1. duration <type 'int'> Nulls: False Min: 0 Max: 58329 Sum: 236802060 Mean: 48.342430464 Median: 0 Standard Deviation: 723.329737422 Unique values: 9883 5 most frequent values: 0: 4779492 1: 23886 2: 8139 3: 6016 5: 5576 2. protocol_type <type 'unicode'> Nulls: False Values: udp, icmp, tcp 3. service <type 'unicode'> Nulls: False Unique values: 70 5 most frequent values: ecr_i: 2811660 private: 1100831 http: 623091 smtp: 96554 other: 72653 Max length: 11 4. flag <type 'unicode'> Nulls: False Unique values: 11 5 most frequent values: SF: 3744328 S0: 869829 REJ: 268874 RSTR: 8094 RSTO: 5344 Max length: 6 5. src_bytes <type 'int'> Nulls: False Min: 0 Max: 1379963888 Sum: 8986765238 Mean: 1834.62117523 Median: 520 Standard Deviation: 941430.978396 Unique values: 7195 5 most frequent values: 1032: 2280245 0: 1152546 520: 527731 105: 73899 147: 27324 6. dst_bytes <type 'int'> Nulls: False Min: 0 Max: 1309937401 Sum: 5357035893 Mean: 1093.62281371 Median: 0 Standard Deviation: 645012.267904 Unique values: 21493 5 most frequent values: 0: 4064854 105: 44713 147: 24910 146: 22536 145: 9500 7. interaction_type <type 'unicode'> Nulls: False Unique values: 23 5 most frequent values: smurf.: 2807886 neptune.: 1072017 normal.: 972781 satan.: 15892 ipsweep.: 12481 Max length: 16 Row count: 4898431 ! csvcut - c 1 - 6 , 42 ../ data / raw / kdd . csv \\ | datamash - t , - sHi crosstab 4 , 2 mean 1 \\ | sed '1d' | csvlook |---------+------+--------------------+------------------| | | icmp | tcp | udp | |---------+------+--------------------+------------------| | OTH | N/A | 0 | N/A | | REJ | N/A | 0.0012384983300728 | N/A | | RSTO | N/A | 56.963323353293 | N/A | | RSTOS0 | N/A | 2876.7049180328 | N/A | | RSTR | N/A | 3323.1266370151 | N/A | | S0 | N/A | 0 | N/A | | S1 | N/A | 0 | N/A | | S2 | N/A | 6.2670807453416 | N/A | | S3 | N/A | 429.98 | N/A | | SF | 0 | 8.5918701456395 | 1045.2031520217 | | SH | N/A | 0 | N/A | |---------+------+--------------------+------------------| ! csvcut - n ../ data / raw / kdd . csv 1: duration 2: protocol_type 3: service 4: flag 5: src_bytes 6: dst_bytes 7: land 8: wrong_fragment 9: urgent 10: hot 11: num_failed_logins 12: logged_in 13: num_compromised 14: root_shell 15: su_attempted 16: num_root 17: num_file_creations 18: num_shells 19: num_access_files 20: num_outbound_cmds 21: is_host_login 22: is_guest_login 23: count 24: srv_count 25: serror_rate 26: srv_serror_rate 27: rerror_rate 28: srv_rerror_rate 29: same_srv_rate 30: diff_srv_rate 31: srv_diff_host_rate 32: dst_host_count 33: dst_host_srv_count 34: dst_host_same_srv_rate 35: dst_host_diff_srv_rate 36: dst_host_same_src_port_rate 37: dst_host_srv_diff_host_rate 38: dst_host_serror_rate 39: dst_host_srv_serror_rate 40: dst_host_rerror_rate 41: dst_host_srv_rerror_rate 42: interaction_type # crosstabs with columns created on-the-fly ! cat ../ data / raw / kdd . csv \\ | mlr -- csv -- rs lf put '$attack = $interaction_type!=\"normal.\"' \\ | datamash - t , - sHi crosstab 2 , 43 mean 1 | sed '1d' | csvlook |-------+-----------------+---------------------| | | false | true | |-------+-----------------+---------------------| | icmp | 0 | 0 | | tcp | 11.481294964029 | 22.602477657342 | | udp | 1061.2623387754 | 0.0013605442176871 | |-------+-----------------+---------------------| ! cat ../ data / raw / kdd . csv \\ | mlr -- csv -- rs lf put '$attack = $interaction_type!=\"normal.\"' \\ | datamash - t , - sHi crosstab 2 , 43 mean 1 sstdev 1 | csvlook # only one aggfunc allowed at a time datamash: crosstab supports one operation, found 2","title":"Checking for nulls"},{"location":"07_datamash/#transpose-reverse-columnsrows","text":"! cat fp2 . csv C00,C01,C02,C03,C04,C05,C06,C07,C08,C09 ,-0.65,,-0.51,,2.1,,-0.01,,-0.11 0.14,0.24,-0.91,-0.54,0.72,1.14,-0.07,0.02,-0.05,1.07 ,0.81,,-1.1,,1.01,,-1.88,,-3.18 1.14,-0.25,-2.19,0.4,1.34,-0.96,0.27,0.26,-1.02,-0.37 ,0.34,,-0.81,,0.8,,1.13,,-0.15 -2.23,-0.9,-0.52,0.54,-1.07,1.47,-0.58,-0.46,0.53,-1.79 ,-0.44,,-0.48,,-0.73,,-0.28,,-1.94 -2.73,-0.22,1.87,0.92,-0.02,-0.44,0.48,1.48,-1.62,0.92 ,-0.31,,-0.58,,0.46,,0.1,,0.81 -1.36,0.29,-0.73,0.34,-1.85,-1.64,1.15,-0.23,-1.35,1.11 ! datamash - t , transpose < fp2 . csv | csvlook |------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------| | C00 | | 0.14 | | 1.14 | | -2.23 | | -2.73 | | -1.36 | |------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------| | C01 | -0.65 | 0.24 | 0.81 | -0.25 | 0.34 | -0.9 | -0.44 | -0.22 | -0.31 | 0.29 | | C02 | | -0.91 | | -2.19 | | -0.52 | | 1.87 | | -0.73 | | C03 | -0.51 | -0.54 | -1.1 | 0.4 | -0.81 | 0.54 | -0.48 | 0.92 | -0.58 | 0.34 | | C04 | | 0.72 | | 1.34 | | -1.07 | | -0.02 | | -1.85 | | C05 | 2.1 | 1.14 | 1.01 | -0.96 | 0.8 | 1.47 | -0.73 | -0.44 | 0.46 | -1.64 | | C06 | | -0.07 | | 0.27 | | -0.58 | | 0.48 | | 1.15 | | C07 | -0.01 | 0.02 | -1.88 | 0.26 | 1.13 | -0.46 | -0.28 | 1.48 | 0.1 | -0.23 | | C08 | | -0.05 | | -1.02 | | 0.53 | | -1.62 | | -1.35 | | C09 | -0.11 | 1.07 | -3.18 | -0.37 | -0.15 | -1.79 | -1.94 | 0.92 | 0.81 | 1.11 | |------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------| ! head ../ data / raw / fromPandas . csv | csvlook |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | C00 | D01 | C02 | D03 | D04 | A05 | A06 | B07 | A08 | C09 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | PO | Alert | 0.08 | -0.29 | 1.04 | 0.5 | -0.19 | 0.92 | -1.18 | 0.46 | | PO | Alert | 0.86 | 1.81 | 2.28 | 1.58 | 0.79 | 1.19 | 0.99 | -1.17 | | PO | Critical | -0.04 | 0.52 | -0.52 | 0.34 | 2.09 | -0.6 | 0.85 | -1.14 | | AR | Critical | -0.28 | 0.56 | 0.69 | 0.62 | -0.28 | -0.25 | 0.64 | -2.03 | | AR | Critical | 0.04 | -0.38 | -0.66 | -1.76 | -0.13 | 0.6 | -2.06 | 0.96 | | PO | Critical | 0.77 | 1.48 | 0.16 | -0.59 | 0.94 | 0.48 | -0.32 | -0.55 | | AR | Alert | -0.15 | 0.03 | -2.05 | -0.56 | -1.04 | 1.29 | 0.51 | -0.01 | | PO | Alert | -0.17 | -0.4 | -0.16 | 2.65 | -0.48 | 0.25 | -1.1 | 0.77 | | AR | Alert | -0.57 | -0.31 | -0.14 | -2.89 | 0.52 | 0.18 | -0.03 | 0.47 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| # transpose ! head ../ data / raw / fromPandas . csv \\ | datamash - t , transpose | csvlook |------+-------+-------+----------+----------+----------+----------+-------+-------+--------| | C00 | PO | PO | PO | AR | AR | PO | AR | PO | AR | |------+-------+-------+----------+----------+----------+----------+-------+-------+--------| | D01 | Alert | Alert | Critical | Critical | Critical | Critical | Alert | Alert | Alert | | C02 | 0.08 | 0.86 | -0.04 | -0.28 | 0.04 | 0.77 | -0.15 | -0.17 | -0.57 | | D03 | -0.29 | 1.81 | 0.52 | 0.56 | -0.38 | 1.48 | 0.03 | -0.4 | -0.31 | | D04 | 1.04 | 2.28 | -0.52 | 0.69 | -0.66 | 0.16 | -2.05 | -0.16 | -0.14 | | A05 | 0.5 | 1.58 | 0.34 | 0.62 | -1.76 | -0.59 | -0.56 | 2.65 | -2.89 | | A06 | -0.19 | 0.79 | 2.09 | -0.28 | -0.13 | 0.94 | -1.04 | -0.48 | 0.52 | | B07 | 0.92 | 1.19 | -0.6 | -0.25 | 0.6 | 0.48 | 1.29 | 0.25 | 0.18 | | A08 | -1.18 | 0.99 | 0.85 | 0.64 | -2.06 | -0.32 | 0.51 | -1.1 | -0.03 | | C09 | 0.46 | -1.17 | -1.14 | -2.03 | 0.96 | -0.55 | -0.01 | 0.77 | 0.47 | |------+-------+-------+----------+----------+----------+----------+-------+-------+--------| By default, transpose verifies the input has the same number of fields in each line, and fails with an error otherwise Use --no-strict to allow missing values Use --filler to set the missing-field filler value # to reverse the order of columns ! head ../ data / raw / fromPandas . csv \\ | datamash - t , reverse | csvlook |--------+-------+-------+-------+-------+-------+-------+-------+----------+------| | C09 | A08 | B07 | A06 | A05 | D04 | D03 | C02 | D01 | C00 | |--------+-------+-------+-------+-------+-------+-------+-------+----------+------| | 0.46 | -1.18 | 0.92 | -0.19 | 0.5 | 1.04 | -0.29 | 0.08 | Alert | PO | | -1.17 | 0.99 | 1.19 | 0.79 | 1.58 | 2.28 | 1.81 | 0.86 | Alert | PO | | -1.14 | 0.85 | -0.6 | 2.09 | 0.34 | -0.52 | 0.52 | -0.04 | Critical | PO | | -2.03 | 0.64 | -0.25 | -0.28 | 0.62 | 0.69 | 0.56 | -0.28 | Critical | AR | | 0.96 | -2.06 | 0.6 | -0.13 | -1.76 | -0.66 | -0.38 | 0.04 | Critical | AR | | -0.55 | -0.32 | 0.48 | 0.94 | -0.59 | 0.16 | 1.48 | 0.77 | Critical | PO | | -0.01 | 0.51 | 1.29 | -1.04 | -0.56 | -2.05 | 0.03 | -0.15 | Alert | AR | | 0.77 | -1.1 | 0.25 | -0.48 | 2.65 | -0.16 | -0.4 | -0.17 | Alert | PO | | 0.47 | -0.03 | 0.18 | 0.52 | -2.89 | -0.14 | -0.31 | -0.57 | Alert | AR | |--------+-------+-------+-------+-------+-------+-------+-------+----------+------| # Remember that `tac` reverses the rows ! head ../ data / raw / fromPandas . csv \\ | tac | csvlook |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | AR | Alert | -0.57 | -0.31 | -0.14 | -2.89 | 0.52 | 0.18 | -0.03 | 0.47 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | PO | Alert | -0.17 | -0.4 | -0.16 | 2.65 | -0.48 | 0.25 | -1.1 | 0.77 | | AR | Alert | -0.15 | 0.03 | -2.05 | -0.56 | -1.04 | 1.29 | 0.51 | -0.01 | | PO | Critical | 0.77 | 1.48 | 0.16 | -0.59 | 0.94 | 0.48 | -0.32 | -0.55 | | AR | Critical | 0.04 | -0.38 | -0.66 | -1.76 | -0.13 | 0.6 | -2.06 | 0.96 | | AR | Critical | -0.28 | 0.56 | 0.69 | 0.62 | -0.28 | -0.25 | 0.64 | -2.03 | | PO | Critical | -0.04 | 0.52 | -0.52 | 0.34 | 2.09 | -0.6 | 0.85 | -1.14 | | PO | Alert | 0.86 | 1.81 | 2.28 | 1.58 | 0.79 | 1.19 | 0.99 | -1.17 | | PO | Alert | 0.08 | -0.29 | 1.04 | 0.5 | -0.19 | 0.92 | -1.18 | 0.46 | | C00 | D01 | C02 | D03 | D04 | A05 | A06 | B07 | A08 | C09 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------|","title":"transpose, reverse (columns/rows)"},{"location":"07_datamash/#binning-or-discretization","text":"! head ../ data / raw / fromPandas . csv | csvlook |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | C00 | D01 | C02 | D03 | D04 | A05 | A06 | B07 | A08 | C09 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| | PO | Alert | 0.08 | -0.29 | 1.04 | 0.5 | -0.19 | 0.92 | -1.18 | 0.46 | | PO | Alert | 0.86 | 1.81 | 2.28 | 1.58 | 0.79 | 1.19 | 0.99 | -1.17 | | PO | Critical | -0.04 | 0.52 | -0.52 | 0.34 | 2.09 | -0.6 | 0.85 | -1.14 | | AR | Critical | -0.28 | 0.56 | 0.69 | 0.62 | -0.28 | -0.25 | 0.64 | -2.03 | | AR | Critical | 0.04 | -0.38 | -0.66 | -1.76 | -0.13 | 0.6 | -2.06 | 0.96 | | PO | Critical | 0.77 | 1.48 | 0.16 | -0.59 | 0.94 | 0.48 | -0.32 | -0.55 | | AR | Alert | -0.15 | 0.03 | -2.05 | -0.56 | -1.04 | 1.29 | 0.51 | -0.01 | | PO | Alert | -0.17 | -0.4 | -0.16 | 2.65 | -0.48 | 0.25 | -1.1 | 0.77 | | AR | Alert | -0.57 | -0.31 | -0.14 | -2.89 | 0.52 | 0.18 | -0.03 | 0.47 | |------+----------+-------+-------+-------+-------+-------+-------+-------+--------| # returns the lower edge of the window ! head ../ data / raw / fromPandas . csv | csvcut - c C02 \\ | datamash - H -- full bin : 0.25 1 \\ | mlr -- csv -- ifs tab -- ofs ',' cat \\ | csvlook |--------+-----------| | C02 | bin(C02) | |--------+-----------| | 0.08 | 0 | | 0.86 | 0.75 | | -0.04 | -0.25 | | -0.28 | -0.5 | | 0.04 | 0 | | 0.77 | 0.75 | | -0.15 | -0.25 | | -0.17 | -0.25 | | -0.57 | -0.75 | |--------+-----------|","title":"binning (or discretization)"},{"location":"07_datamash/#round-trunc-ceil-floor-frac","text":"! head ../ data / raw / fromPandas . csv | csvcut - c C02 \\ | datamash - H -- full round 1 trunc 1 ceil 1 floor 1 frac 1 \\ | mlr -- csv -- ifs tab -- ofs ',' cat \\ | csvlook |--------+------------+------------+-----------+------------+------------| | C02 | round(C02) | trunc(C02) | ceil(C02) | floor(C02) | frac(C02) | |--------+------------+------------+-----------+------------+------------| | 0.08 | 0 | 0 | 1 | 0 | 0.08 | | 0.86 | 1 | 0 | 1 | 0 | 0.86 | | -0.04 | 0 | 0 | 0 | -1 | -0.04 | | -0.28 | 0 | 0 | 0 | -1 | -0.28 | | 0.04 | 0 | 0 | 1 | 0 | 0.04 | | 0.77 | 1 | 0 | 1 | 0 | 0.77 | | -0.15 | 0 | 0 | 0 | -1 | -0.15 | | -0.17 | 0 | 0 | 0 | -1 | -0.17 | | -0.57 | -1 | 0 | 0 | -1 | -0.57 | |--------+------------+------------+-----------+------------+------------|","title":"round, trunc, ceil, floor, frac"},{"location":"07_datamash/#datamash-analysis-examples","text":"! head ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 | csvlook |-----------+---------------+---------+------+-----------+-----------+-------------------| | duration | protocol_type | service | flag | src_bytes | dst_bytes | interaction_type | |-----------+---------------+---------+------+-----------+-----------+-------------------| | 0 | tcp | http | SF | 215 | 45076 | normal. | | 0 | tcp | http | SF | 162 | 4528 | normal. | | 0 | tcp | http | SF | 236 | 1228 | normal. | | 0 | tcp | http | SF | 233 | 2032 | normal. | | 0 | tcp | http | SF | 239 | 486 | normal. | | 0 | tcp | http | SF | 238 | 1282 | normal. | | 0 | tcp | http | SF | 235 | 1337 | normal. | | 0 | tcp | http | SF | 234 | 1364 | normal. | | 0 | tcp | http | SF | 239 | 1295 | normal. | |-----------+---------------+---------+------+-----------+-----------+-------------------| # frequency tables ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | datamash - sHi - t , -- group 7 count 7 | csvlook |----------------------------+--------------------------| | GroupBy(interaction_type) | count(interaction_type) | |----------------------------+--------------------------| | back. | 2203 | | buffer_overflow. | 30 | | ftp_write. | 8 | | guess_passwd. | 53 | | imap. | 12 | | ipsweep. | 12481 | | land. | 21 | | loadmodule. | 9 | | multihop. | 7 | | neptune. | 1072017 | | nmap. | 2316 | | normal. | 972781 | | perl. | 3 | | phf. | 4 | | pod. | 264 | | portsweep. | 10413 | | rootkit. | 10 | | satan. | 15892 | | smurf. | 2807886 | | spy. | 2 | | teardrop. | 979 | | warezclient. | 1020 | | warezmaster. | 20 | |----------------------------+--------------------------| # countunique ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | datamash - sHi - t , -- group 2 countunique 7 | csvlook |-------------------------+--------------------------------| | GroupBy(protocol_type) | countunique(interaction_type) | |-------------------------+--------------------------------| | icmp | 7 | | tcp | 20 | | udp | 5 | |-------------------------+--------------------------------| # groupby 1 ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | datamash - sHi - t , -- group 2 mean 5 sstdev 5 \\ | csvlook |-------------------------+-----------------+--------------------| | GroupBy(protocol_type) | mean(src_bytes) | sstdev(src_bytes) | |-------------------------+-----------------+--------------------| | icmp | 927.89168938556 | 216.02705205438 | | tcp | 3388.5699653266 | 1523443.9737618 | | udp | 97.227728938483 | 47.667234078966 | |-------------------------+-----------------+--------------------| # groupby 2 ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | mlr -- csv -- rs lf put '$attack = $interaction_type!=\"normal.\"' \\ | datamash - sHi - t , -- group 2 , 8 mean 1 sstdev 1 2 | csvlook |-------------------------+-----------------+--------------------+--------------------| | GroupBy(protocol_type) | GroupBy(attack) | mean(duration) | sstdev(duration) | |-------------------------+-----------------+--------------------+--------------------| | icmp | false | 0 | 0 | | icmp | true | 0 | 0 | | tcp | false | 11.481294964029 | 376.54007768793 | | tcp | true | 22.602477657342 | 824.78914040215 | | udp | false | 1061.2623387754 | 2799.4403718827 | | udp | true | 0.0013605442176871 | 0.073771111356332 | |-------------------------+-----------------+--------------------+--------------------| # groupby 2 ! cat ../ data / raw / kdd . csv | cut - d , - f1 - 6 , 42 \\ | mlr -- csv -- rs lf put '$attack = $interaction_type!=\"normal.\"' \\ | datamash - sHi - t , -- group 2 , 8 mean 1 sstdev 1 \\ | csvlook |-------------------------+-----------------+--------------------+--------------------| | GroupBy(protocol_type) | GroupBy(attack) | mean(duration) | sstdev(duration) | |-------------------------+-----------------+--------------------+--------------------| | icmp | false | 0 | 0 | | icmp | true | 0 | 0 | | tcp | false | 11.481294964029 | 376.54007768793 | | tcp | true | 22.602477657342 | 824.78914040215 | | udp | false | 1061.2623387754 | 2799.4403718827 | | udp | true | 0.0013605442176871 | 0.073771111356332 | |-------------------------+-----------------+--------------------+--------------------|","title":"datamash Analysis Examples"},{"location":"08_awk/","text":"AWK \u00b6 AWK 1 - Introduction 2 - Features 3 - Syntax and Execution Model Running awk programs 3.1 - Execution 3.2 - Examples 4 - Data Types 4.1 - Strings, Numbers 4.2 - Variables Built-in Variables 4.3 - Arrays 5 - Patterns 5.1 - Regular Expressions 5.2 - Boolean Expressions Note 5.3 - Special Patterns: BEGIN and END 6 - Actions 7 - Functions 8 - Sources of Errors 9 - Applications 10 - References 1 - Introduction \u00b6 awk is not just a command line tool . It is a tiny, but full-featured Turing-complete programming language modeled on C, used to process up to GBs of structured data . The benefits of awk are best realized when the data has some kind of structure . It extends the idea of text editing into data processing, analysis, extraction and reporting. A typical example of an awk program is one that transforms data into a formatted report, such as ingesting server log files. It extensively uses the string datatype, arrays indexed by key strings, and regular expressions. It lets you do stuff on the command line which you never imagined. It's a self-contained mini data analytics software . And it is relatively easy to learn. Quoting Wikipedia The AWK language is a **data-driven scripting language* consisting of a set of actions to be taken against streams of textual data \u2013 either run directly on files or used as part of a pipeline \u2013 for purposes of extracting or transforming text.* And quoting Alfred V., one of the creators of the language (the A in awk) \"**AWK* is a language for processing text files. A file is treated as a sequence of records, and by default each line is a record. Each line is broken up into a sequence of fields, so we can think of the first word in a line as the first field, the second word as the second field, and so on. An AWK program is a sequence of pattern-action statements. AWK reads the input a line at a time. A line is scanned for each pattern in the program, and for each pattern that matches, the associated action is executed.\"* import os os . chdir ( \"/home/data\" ) import subprocess as sbp run_on_bash = lambda i : print ( sbp . check_output ( \" {} \" . format ( i ), shell = True ) . decode ( 'utf-8' )) run_on_bash ( \"python -c 'import this' > zen.txt\" ) run_on_bash ( \"awk ' {print} ' zen.txt\" ) The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those. 2 - Features \u00b6 View a text file as a textual database made up of records and fields. Use variables to manipulate the database. Use arithmetic and string operators. Use common programming constructs such as loops and conditionals . Generate formatted reports. Define functions . Execute UNIX commands from a script. Process the result of UNIX commands. Process command-line arguments more gracefully. Work more easily with multiple input streams. 3 - Syntax and Execution Model \u00b6 An awk program consists of what we will call a main input loop . You don\u2019t write this loop, it is given\u2014it exists as the framework within which the code that you do write will be executed. The main input loop in awk is a routine that reads one line of input from a file and makes it available for processing. The actions you write to do the processing assume that there is a line of input available. In another programming language, you would have to create the main input loop as part of your program. It would have to open the input file and read one line at a time. The main input loop is executed as many times as there are lines of input. It terminates when there is no more input to be read. Inside the main input loop, your instructions are written as a series of pattern/action procedures. These procedures that you write will be applied to each input line, one line at a time. A pattern is a rule for testing the input line to determine whether or not the action should be applied to it. Usually a regex or a boolean expression used to match rows, or special expressions like BEGIN and END The actions can be quite complex, consisting of statements, functions, and expressions. Usually a series of awk commands applied to selected fields of rows that match the regex/where the boolean expression evaluates to True can include function calls, variable assignments, calculations, or any combination thereof. Awk allows you to write two special routines that can be executed before any input is read and after all input is read. These are the procedur es associated with the BEGIN and END rules , respectively. In other words, you can do some preprocessing before the main input loop is ever executed and you can do some post-processing after the main input loop has terminated. The BEGIN and END procedures are optional . In summary, an awk program consists of: BEGIN segment (optional) : to initialize our variables before we even start reading input pattern + action pairs : to process the input data, here we may place multiple pattern + action pairs to do multiple things with the same line. END segment : actions for when the EOF is reached, typically used to print results Running awk programs \u00b6 # as a file saved with the .awk extension BEGIN { actions ; } pattern { actions ; } pattern { actions ; } . . . pattern { actions ; } END { actions ; } # or on a line awk 'BEGIN {initial actions} {processing actions} END {ending actions}' file.txt run_on_bash ( \"awk '/better/ {print} ' zen.txt\" ) Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Now is better than never. Although never is often better than *right* now. 3.1 - Execution \u00b6 For each line of input, awk attempts each pattern-matching rule given in the script. The lines matching a particular pattern become the object of an action. If no action is specified, the line that matches the pattern is printed. In pseudocode, here's how a program runs 1. Perform BEGIN action 2. Read one line from file, check against all procedures 3. If a PATTERN matches, apply corresponding ACTION else pass 4. Read next line. Repeat 2-4 till EOF. 5. Perform END action Note that a line can match more than one rule. You can write a stricter rule set to prevent a line from matching more than one rule. Every line of the document to scan will have to go through each of the patterns, one at a time. Line 1 of the text file will be compared against Pattern1 , and if it matches, Action1 will be executed. If it doesn't match, Pattern2 will be checked, and Action2 will or won't be executed. This will continue until the input has been read completely. Note that either the condition or the action may be omitted. The condition defaults to matching every record. The default action is to print the record. awk statements can be run on the command-line, or inside a script awk [ options ] <pattern> <action> file ( s ) # statements are separated by ; awk [ options ] -f awk-script file ( s ) # OPTIONS # -f precedes name of awk script # -F changes delimiter # -v precedes assignment var=value 3.2 - Examples \u00b6 Here we look at a few examples, from super simple ones to some basic ones awk '{ print $1 }' foo.txt # no pattern; print the first field (use default delimiter) of every line in foo awk '/regex/' foo.txt # no action; print matching lines in foo awk '/regex/ { print $1 }' foo.txt # print the first field of each record that matches the regex awk -F, '{ print $1; print $2}' foo.txt # -F changes the delimiter to comma # for each record in foo, print the 1st and 2nd fields on their own lines awk '{print $3+$4}' foo.txt # return the sum of columns 3 and 4 awk '$1==\"bar\" {print $3+$4}' foo.txt # return the sum of columns 3 and 4 for rows where field 1 equals bar 4 - Data Types \u00b6 4.1 - Strings, Numbers \u00b6 awk only has two main data types : strings and numbers . And even then, Awk likes to convert them into each other. Numbers stored as strings are implicitly converted. If the string doesn't look like a numeral, it's 0 . String objects are enclosed within double quotes \"\" For string concatenation , simply place two variables next to each other. 4.2 - Variables \u00b6 Both types can be assigned to variables in the ACTIONS parts of your code with the = operator. Variables can be declared anywhere, at any time, and used even if they're not initialized (their default value is \"\" , the empty string. NOTE The variables are all global . Whatever variables you declare in a given block will be visible to other blocks, for each line Built-in Variables \u00b6 Variable Contains $0 represents the entire record $1, $2, $3 ... access respective fields NR row number of the current record NF number of fields in the current record ( $NF accesses the last field) FILENAME contains the name of the current input-file FS field-separator (default is whitespace(s), includes tabs) can be regex RS record-separator (default: newline ) OFS , ORS output delimiter (default: space), record separator (default: newline) OFMT format for numeric output. The default format is \"%.6g\". 4.3 - Arrays \u00b6 Finally, awk has arrays. They are unidimensional associative arrays that can be started dynamically . 'Associative' means that they may be indexed by strings or number and stores data in a key value format, like Python dictionaries. Their syntax is just var[key] = value . 5 - Patterns \u00b6 5.1 - Regular Expressions \u00b6 The regexes used with awk are general expressions that you use everyday with grep and sed . N ote that these regexs only exist to match lines, they do not capture fields where the match occurs. /foo/ { ... } # any line that contains 'foo' /^foo/ { ... } # lines that begin with 'foo' /foo$/ { ... } # lines that end with 'foo' /^ [ 0 -9. ] + / { ... } # lines beginning with series of numbers and/or periods / ( foo | bar | baz ) / # lines that contain specific words # test for integer, string or empty line. / [ 0 -9 ] +/ { print \"That is an integer\" } / [ A-Za-z ] +/ { print \"This is a string\" } /\u02c6$/ { print \"This is a blank line.\" } 5.2 - Boolean Expressions \u00b6 These are constructed using any regular data types with the comparison operators like ==, !=, >, >=, < and <= . Note that == does fuzzy matching, such that 80==\"80\" is True . Compound expressions can be constructed using operators && (AND), || (OR), and ! (NOT). Note \u00b6 Regexes and Booleans can be mixed , so the expression /foo/ && '$3==\"bar\"' is valid and will match rows that contain the string foo, and contain \"bar\" in their 3 rd field You can modify the line by assigning to its field. For example, if you write $1 = \"foobar\" in one block, the next patterns will now operate on that line instead of the original one. Could be used for imputation of missing data! 5.3 - Special Patterns: BEGIN and END \u00b6 The first one, BEGIN , matches only before any line has been input to the file. This is basically where you can initiate variables (like declaring delimiters) and all other kinds of state in your script. END will match after the whole input has been handled. This lets you clean up or do some final output before exiting. 6 - Actions \u00b6 The most useful ones are listed here # basics { print $0 ; } # prints $0. In this case, equivalent to 'print' alone { exit ; } # ends the program { next ; } # skips to the next line of input # assignments { a = $1 ; b = $0 } # variable assignment { c [ $1 ] = $2 } # variable assignment (array) # conditional processing { if ( BOOLEAN ) { ACTION } else if ( BOOLEAN ) { ACTION } else { ACTION } } # loops { for ( i = 1 ; i<x ; i++ ) { ACTION } } { for ( item in c ) { ACTION } } Examples awk '/system/ {count+=1} END {print count}' temp.txt # find the number of rows that contain 'system' awk '{sum+=$1; count+=1} END {print sum/count}' foo # print the mean of field 1 # mean can also be found using the in-built row counter NR awk 'BEGIN{ sum=0; FS=\",\"} {sum+=$1} END {print sum/NR}' foo.csv 7 - Functions \u00b6 Functions can be called with the following syntax: { somecall ( $2 ) } There is a somewhat restricted set of built-in functions available, so I like to point to regular documentation for these. User-defined functions are also fairly simple: # function arguments are call-by-value function name(parameter-list) { ACTIONS; # same actions as usual } # return is a valid keyword function add1(val) { return val+1; } 8 - Sources of Errors \u00b6 Can be caused by any of the following: Not enclosing a procedur e within braces {} Not surrounding the instructions within single quotes '' Not enclosing regular expressions within slashes // 9 - Applications \u00b6 a uniq -c substitute for big files big files with lots of dupes 10 - References \u00b6 awk in 20 minutes","title":"AWK"},{"location":"08_awk/#awk","text":"AWK 1 - Introduction 2 - Features 3 - Syntax and Execution Model Running awk programs 3.1 - Execution 3.2 - Examples 4 - Data Types 4.1 - Strings, Numbers 4.2 - Variables Built-in Variables 4.3 - Arrays 5 - Patterns 5.1 - Regular Expressions 5.2 - Boolean Expressions Note 5.3 - Special Patterns: BEGIN and END 6 - Actions 7 - Functions 8 - Sources of Errors 9 - Applications 10 - References","title":"AWK"},{"location":"08_awk/#1-introduction","text":"awk is not just a command line tool . It is a tiny, but full-featured Turing-complete programming language modeled on C, used to process up to GBs of structured data . The benefits of awk are best realized when the data has some kind of structure . It extends the idea of text editing into data processing, analysis, extraction and reporting. A typical example of an awk program is one that transforms data into a formatted report, such as ingesting server log files. It extensively uses the string datatype, arrays indexed by key strings, and regular expressions. It lets you do stuff on the command line which you never imagined. It's a self-contained mini data analytics software . And it is relatively easy to learn. Quoting Wikipedia The AWK language is a **data-driven scripting language* consisting of a set of actions to be taken against streams of textual data \u2013 either run directly on files or used as part of a pipeline \u2013 for purposes of extracting or transforming text.* And quoting Alfred V., one of the creators of the language (the A in awk) \"**AWK* is a language for processing text files. A file is treated as a sequence of records, and by default each line is a record. Each line is broken up into a sequence of fields, so we can think of the first word in a line as the first field, the second word as the second field, and so on. An AWK program is a sequence of pattern-action statements. AWK reads the input a line at a time. A line is scanned for each pattern in the program, and for each pattern that matches, the associated action is executed.\"* import os os . chdir ( \"/home/data\" ) import subprocess as sbp run_on_bash = lambda i : print ( sbp . check_output ( \" {} \" . format ( i ), shell = True ) . decode ( 'utf-8' )) run_on_bash ( \"python -c 'import this' > zen.txt\" ) run_on_bash ( \"awk ' {print} ' zen.txt\" ) The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those.","title":"1 - Introduction"},{"location":"08_awk/#2-features","text":"View a text file as a textual database made up of records and fields. Use variables to manipulate the database. Use arithmetic and string operators. Use common programming constructs such as loops and conditionals . Generate formatted reports. Define functions . Execute UNIX commands from a script. Process the result of UNIX commands. Process command-line arguments more gracefully. Work more easily with multiple input streams.","title":"2 - Features"},{"location":"08_awk/#3-syntax-and-execution-model","text":"An awk program consists of what we will call a main input loop . You don\u2019t write this loop, it is given\u2014it exists as the framework within which the code that you do write will be executed. The main input loop in awk is a routine that reads one line of input from a file and makes it available for processing. The actions you write to do the processing assume that there is a line of input available. In another programming language, you would have to create the main input loop as part of your program. It would have to open the input file and read one line at a time. The main input loop is executed as many times as there are lines of input. It terminates when there is no more input to be read. Inside the main input loop, your instructions are written as a series of pattern/action procedures. These procedures that you write will be applied to each input line, one line at a time. A pattern is a rule for testing the input line to determine whether or not the action should be applied to it. Usually a regex or a boolean expression used to match rows, or special expressions like BEGIN and END The actions can be quite complex, consisting of statements, functions, and expressions. Usually a series of awk commands applied to selected fields of rows that match the regex/where the boolean expression evaluates to True can include function calls, variable assignments, calculations, or any combination thereof. Awk allows you to write two special routines that can be executed before any input is read and after all input is read. These are the procedur es associated with the BEGIN and END rules , respectively. In other words, you can do some preprocessing before the main input loop is ever executed and you can do some post-processing after the main input loop has terminated. The BEGIN and END procedures are optional . In summary, an awk program consists of: BEGIN segment (optional) : to initialize our variables before we even start reading input pattern + action pairs : to process the input data, here we may place multiple pattern + action pairs to do multiple things with the same line. END segment : actions for when the EOF is reached, typically used to print results","title":"3 - Syntax and Execution Model"},{"location":"08_awk/#running-awk-programs","text":"# as a file saved with the .awk extension BEGIN { actions ; } pattern { actions ; } pattern { actions ; } . . . pattern { actions ; } END { actions ; } # or on a line awk 'BEGIN {initial actions} {processing actions} END {ending actions}' file.txt run_on_bash ( \"awk '/better/ {print} ' zen.txt\" ) Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Now is better than never. Although never is often better than *right* now.","title":"Running awk programs"},{"location":"08_awk/#31-execution","text":"For each line of input, awk attempts each pattern-matching rule given in the script. The lines matching a particular pattern become the object of an action. If no action is specified, the line that matches the pattern is printed. In pseudocode, here's how a program runs 1. Perform BEGIN action 2. Read one line from file, check against all procedures 3. If a PATTERN matches, apply corresponding ACTION else pass 4. Read next line. Repeat 2-4 till EOF. 5. Perform END action Note that a line can match more than one rule. You can write a stricter rule set to prevent a line from matching more than one rule. Every line of the document to scan will have to go through each of the patterns, one at a time. Line 1 of the text file will be compared against Pattern1 , and if it matches, Action1 will be executed. If it doesn't match, Pattern2 will be checked, and Action2 will or won't be executed. This will continue until the input has been read completely. Note that either the condition or the action may be omitted. The condition defaults to matching every record. The default action is to print the record. awk statements can be run on the command-line, or inside a script awk [ options ] <pattern> <action> file ( s ) # statements are separated by ; awk [ options ] -f awk-script file ( s ) # OPTIONS # -f precedes name of awk script # -F changes delimiter # -v precedes assignment var=value","title":"3.1 -  Execution"},{"location":"08_awk/#32-examples","text":"Here we look at a few examples, from super simple ones to some basic ones awk '{ print $1 }' foo.txt # no pattern; print the first field (use default delimiter) of every line in foo awk '/regex/' foo.txt # no action; print matching lines in foo awk '/regex/ { print $1 }' foo.txt # print the first field of each record that matches the regex awk -F, '{ print $1; print $2}' foo.txt # -F changes the delimiter to comma # for each record in foo, print the 1st and 2nd fields on their own lines awk '{print $3+$4}' foo.txt # return the sum of columns 3 and 4 awk '$1==\"bar\" {print $3+$4}' foo.txt # return the sum of columns 3 and 4 for rows where field 1 equals bar","title":"3.2 - Examples"},{"location":"08_awk/#4-data-types","text":"","title":"4 - Data Types"},{"location":"08_awk/#41-strings-numbers","text":"awk only has two main data types : strings and numbers . And even then, Awk likes to convert them into each other. Numbers stored as strings are implicitly converted. If the string doesn't look like a numeral, it's 0 . String objects are enclosed within double quotes \"\" For string concatenation , simply place two variables next to each other.","title":"4.1 - Strings, Numbers"},{"location":"08_awk/#42-variables","text":"Both types can be assigned to variables in the ACTIONS parts of your code with the = operator. Variables can be declared anywhere, at any time, and used even if they're not initialized (their default value is \"\" , the empty string. NOTE The variables are all global . Whatever variables you declare in a given block will be visible to other blocks, for each line","title":"4.2 - Variables"},{"location":"08_awk/#built-in-variables","text":"Variable Contains $0 represents the entire record $1, $2, $3 ... access respective fields NR row number of the current record NF number of fields in the current record ( $NF accesses the last field) FILENAME contains the name of the current input-file FS field-separator (default is whitespace(s), includes tabs) can be regex RS record-separator (default: newline ) OFS , ORS output delimiter (default: space), record separator (default: newline) OFMT format for numeric output. The default format is \"%.6g\".","title":"Built-in Variables"},{"location":"08_awk/#43-arrays","text":"Finally, awk has arrays. They are unidimensional associative arrays that can be started dynamically . 'Associative' means that they may be indexed by strings or number and stores data in a key value format, like Python dictionaries. Their syntax is just var[key] = value .","title":"4.3 - Arrays"},{"location":"08_awk/#5-patterns","text":"","title":"5 - Patterns"},{"location":"08_awk/#51-regular-expressions","text":"The regexes used with awk are general expressions that you use everyday with grep and sed . N ote that these regexs only exist to match lines, they do not capture fields where the match occurs. /foo/ { ... } # any line that contains 'foo' /^foo/ { ... } # lines that begin with 'foo' /foo$/ { ... } # lines that end with 'foo' /^ [ 0 -9. ] + / { ... } # lines beginning with series of numbers and/or periods / ( foo | bar | baz ) / # lines that contain specific words # test for integer, string or empty line. / [ 0 -9 ] +/ { print \"That is an integer\" } / [ A-Za-z ] +/ { print \"This is a string\" } /\u02c6$/ { print \"This is a blank line.\" }","title":"5.1 - Regular Expressions"},{"location":"08_awk/#52-boolean-expressions","text":"These are constructed using any regular data types with the comparison operators like ==, !=, >, >=, < and <= . Note that == does fuzzy matching, such that 80==\"80\" is True . Compound expressions can be constructed using operators && (AND), || (OR), and ! (NOT).","title":"5.2 - Boolean Expressions"},{"location":"08_awk/#note","text":"Regexes and Booleans can be mixed , so the expression /foo/ && '$3==\"bar\"' is valid and will match rows that contain the string foo, and contain \"bar\" in their 3 rd field You can modify the line by assigning to its field. For example, if you write $1 = \"foobar\" in one block, the next patterns will now operate on that line instead of the original one. Could be used for imputation of missing data!","title":"Note"},{"location":"08_awk/#53-special-patterns-begin-and-end","text":"The first one, BEGIN , matches only before any line has been input to the file. This is basically where you can initiate variables (like declaring delimiters) and all other kinds of state in your script. END will match after the whole input has been handled. This lets you clean up or do some final output before exiting.","title":"5.3 - Special Patterns: BEGIN and END"},{"location":"08_awk/#6-actions","text":"The most useful ones are listed here # basics { print $0 ; } # prints $0. In this case, equivalent to 'print' alone { exit ; } # ends the program { next ; } # skips to the next line of input # assignments { a = $1 ; b = $0 } # variable assignment { c [ $1 ] = $2 } # variable assignment (array) # conditional processing { if ( BOOLEAN ) { ACTION } else if ( BOOLEAN ) { ACTION } else { ACTION } } # loops { for ( i = 1 ; i<x ; i++ ) { ACTION } } { for ( item in c ) { ACTION } } Examples awk '/system/ {count+=1} END {print count}' temp.txt # find the number of rows that contain 'system' awk '{sum+=$1; count+=1} END {print sum/count}' foo # print the mean of field 1 # mean can also be found using the in-built row counter NR awk 'BEGIN{ sum=0; FS=\",\"} {sum+=$1} END {print sum/NR}' foo.csv","title":"6 - Actions"},{"location":"08_awk/#7-functions","text":"Functions can be called with the following syntax: { somecall ( $2 ) } There is a somewhat restricted set of built-in functions available, so I like to point to regular documentation for these. User-defined functions are also fairly simple: # function arguments are call-by-value function name(parameter-list) { ACTIONS; # same actions as usual } # return is a valid keyword function add1(val) { return val+1; }","title":"7 - Functions"},{"location":"08_awk/#8-sources-of-errors","text":"Can be caused by any of the following: Not enclosing a procedur e within braces {} Not surrounding the instructions within single quotes '' Not enclosing regular expressions within slashes //","title":"8 - Sources of Errors"},{"location":"08_awk/#9-applications","text":"a uniq -c substitute for big files big files with lots of dupes","title":"9 - Applications"},{"location":"08_awk/#10-references","text":"awk in 20 minutes","title":"10 - References"},{"location":"08_awk_v2/","text":"import os os . chdir ( \"/home/data\" ) ! man pr man: can't set the locale; make sure $LC_* and $LANG are correct No manual entry for pr See 'man 7 undocumented' for help when manual pages are not available. ! csvcut - n flights . csv | pr - t - 2 1: Year 16: DepDelay 2: Month 17: Origin 3: DayofMonth 18: Dest 4: DayOfWeek 19: Distance 5: DepTime 20: TaxiIn 6: CRSDepTime 21: TaxiOut 7: ArrTime 22: Cancelled 8: CRSArrTime 23: CancellationCode 9: UniqueCarrier 24: Diverted 10: FlightNum 25: CarrierDelay 11: TailNum 26: WeatherDelay 12: ActualElapsedTime 27: NASDelay 13: CRSElapsedTime 28: SecurityDelay 14: AirTime 29: LateAircraftDelay 15: ArrDelay import subprocess as sbp run_on_bash = lambda i : sbp . check_output ( \" {} \" . format ( i ), shell = True ) . decode ( 'utf-8' ) . strip () x = run_on_bash ( \"\"\" seq 1 20 | awk 'BEGIN{OFS=\",\"}{print rand(),rand(),rand()}' \"\"\" ) from io import StringIO pd . read_csv ( StringIO ( x ), header = None ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 0.237788 0.291066 0.845814 1 0.152208 0.585537 0.193475 2 0.810623 0.173531 0.484983 3 0.151863 0.366957 0.491736 4 0.910094 0.265257 0.893188 5 0.220351 0.631798 0.571077 6 0.332158 0.104455 0.502931 7 0.567394 0.854165 0.040141 8 0.108022 0.639396 0.013111 9 0.720184 0.101814 0.482945 10 0.254355 0.676697 0.896782 11 0.759896 0.720292 0.907623 12 0.928611 0.377663 0.899756 13 0.778880 0.324255 0.194231 14 0.995553 0.161296 0.708034 15 0.501519 0.936301 0.716323 16 0.105190 0.209205 0.559397 17 0.705432 0.078234 0.510530 18 0.196197 0.274211 0.638602 19 0.448208 0.039872 0.467251 Quick Reference for awk \u00b6 Basic Syntax \u00b6 awk <command-line options> <awk script> <parameters> <data file> Awk can be written in two ways short awk statements enclosed within single quotes 'pattern { action }' can be run directly awk [ -v var = value ] [ -Fr e ] [ - - ] \u2019pattern { action } \u2019 var = value datafile ( s ) long awk programs can be placed within a .awk file and run with the -f option # create the awk script as ________________________ #! /usr/bin/awk -f ...functions... ...statements... ________________________ # run it awk [ -v var = value ] [ -Fr e ] -f scriptfile [ - - ] var = value datafile ( s ) Options, Parameters \u00b6 The -v option sets the variable var to value before the script is executed The -F option is used to specify a delimiter This can also be done with the BEGIN statement inside a script the -- option marks the end of command-line options Parameters can be passed into awk by specifying them on the command line after the script Can be a literal, a shell variable, or the result of a bash command These are not available until the first line of input is read, and thus cannot be accessed in the BEGIN procedure. Records and Fields \u00b6 Each line of input is split into fields and becomes a record By default, the field delimiter is one or more spaces and/or tabs. The delimiter can be changed using -F or with OFS= The default record separator is a newline. Can be changed with the RS= option in the BEGIN procedure Each field can be referenced by its position in the record. $1 refers to the value of the first field; $2 to the second field, and so on. $0 refers to the entire record Writing awk Scripts \u00b6 A script is set of awk statements Each statement has patterns which filter records to which actions apply actions that are used for modifying or analysing data If no pattern is specified, the action is performed on every record If no action is specified, the default action, print , is performed on all matching records. Functions can be declared with the following syntax Variables specified in the parameter-list are treated as local variables within the function. All other variables are global and can be accessed outside the function. function some_func ( parameters ) { statements } A line in an awk script is terminated by a newline or a semicolon Flow control statements ( do, if, for, while ) continue on the next line if ( NF > 1 ) { name = $1 total += $2 } - A comment begins with a \u201c#\u201d and ends with a newline Patterns \u00b6 A pattern can be any of the following: /regular expression/ relational expression BEGIN END pattern, pattern Regular expressions must be enclosed in slashes Relational expressions use Operators like < <= > >= != == and && || ~ !~ The BEGIN pattern is applied before the first line of input is read the END pattern is applied after the last line of input is read. BEGIN and END patterns must be associated with actions. Use ! to negate a match Variables \u00b6 User Defined The name of a variable cannot start with a digit. Case matters Can contain a string (must be quoted) or a numeric value Does not need to be initialized (awk is a dynamically typed language) An uninitialized variable has the empty string (\u201c\u201d) as its string value and 0 as its numeric value. Awk attempts to decide whether a value should be processed as a string or a number depending upon the operation. Built-in or system variables - Names consist of all capital letters. \u00b6 Fields A field variable is referenced using $n , where n is any number 0 to NF n can be supplied by a variable, such as $NF (meaning the last field), a constant, such as $1 meaning the first field. Arrays Arrays are variables that store a set of indexed values Declared with Arrays are associative , ie. exist as key-value pairs The index can be string or numeric Values are not stored in a particular order Use a for loop to read the array Use an if statement to check if an index exists You can also delete individual elements of the array using the delete statement. # creating an array some_array [ idx ] = value # accessing items for ( idx in array ) { ...do something with idx or array [ idx ] ... } # check if idx exists if ( idx in array ) { ...do something... }","title":"08 awk v2"},{"location":"08_awk_v2/#quick-reference-for-awk","text":"","title":"Quick Reference for awk"},{"location":"08_awk_v2/#basic-syntax","text":"awk <command-line options> <awk script> <parameters> <data file> Awk can be written in two ways short awk statements enclosed within single quotes 'pattern { action }' can be run directly awk [ -v var = value ] [ -Fr e ] [ - - ] \u2019pattern { action } \u2019 var = value datafile ( s ) long awk programs can be placed within a .awk file and run with the -f option # create the awk script as ________________________ #! /usr/bin/awk -f ...functions... ...statements... ________________________ # run it awk [ -v var = value ] [ -Fr e ] -f scriptfile [ - - ] var = value datafile ( s )","title":"Basic Syntax"},{"location":"08_awk_v2/#options-parameters","text":"The -v option sets the variable var to value before the script is executed The -F option is used to specify a delimiter This can also be done with the BEGIN statement inside a script the -- option marks the end of command-line options Parameters can be passed into awk by specifying them on the command line after the script Can be a literal, a shell variable, or the result of a bash command These are not available until the first line of input is read, and thus cannot be accessed in the BEGIN procedure.","title":"Options, Parameters"},{"location":"08_awk_v2/#records-and-fields","text":"Each line of input is split into fields and becomes a record By default, the field delimiter is one or more spaces and/or tabs. The delimiter can be changed using -F or with OFS= The default record separator is a newline. Can be changed with the RS= option in the BEGIN procedure Each field can be referenced by its position in the record. $1 refers to the value of the first field; $2 to the second field, and so on. $0 refers to the entire record","title":"Records and Fields"},{"location":"08_awk_v2/#writing-awk-scripts","text":"A script is set of awk statements Each statement has patterns which filter records to which actions apply actions that are used for modifying or analysing data If no pattern is specified, the action is performed on every record If no action is specified, the default action, print , is performed on all matching records. Functions can be declared with the following syntax Variables specified in the parameter-list are treated as local variables within the function. All other variables are global and can be accessed outside the function. function some_func ( parameters ) { statements } A line in an awk script is terminated by a newline or a semicolon Flow control statements ( do, if, for, while ) continue on the next line if ( NF > 1 ) { name = $1 total += $2 } - A comment begins with a \u201c#\u201d and ends with a newline","title":"Writing awk Scripts"},{"location":"08_awk_v2/#patterns","text":"A pattern can be any of the following: /regular expression/ relational expression BEGIN END pattern, pattern Regular expressions must be enclosed in slashes Relational expressions use Operators like < <= > >= != == and && || ~ !~ The BEGIN pattern is applied before the first line of input is read the END pattern is applied after the last line of input is read. BEGIN and END patterns must be associated with actions. Use ! to negate a match","title":"Patterns"},{"location":"08_awk_v2/#variables","text":"User Defined The name of a variable cannot start with a digit. Case matters Can contain a string (must be quoted) or a numeric value Does not need to be initialized (awk is a dynamically typed language) An uninitialized variable has the empty string (\u201c\u201d) as its string value and 0 as its numeric value. Awk attempts to decide whether a value should be processed as a string or a number depending upon the operation. Built-in or system variables","title":"Variables"},{"location":"08_awk_v2/#-names-consist-of-all-capital-letters","text":"Fields A field variable is referenced using $n , where n is any number 0 to NF n can be supplied by a variable, such as $NF (meaning the last field), a constant, such as $1 meaning the first field. Arrays Arrays are variables that store a set of indexed values Declared with Arrays are associative , ie. exist as key-value pairs The index can be string or numeric Values are not stored in a particular order Use a for loop to read the array Use an if statement to check if an index exists You can also delete individual elements of the array using the delete statement. # creating an array some_array [ idx ] = value # accessing items for ( idx in array ) { ...do something with idx or array [ idx ] ... } # check if idx exists if ( idx in array ) { ...do something... }","title":"- Names consist of all capital letters."},{"location":"CLI-Fu/","text":"Introduction 00 - Import, Inspect conversion display count types column names 01 - Subset\u200b rename index subset columns subset rows sample split 02-Clean\u200b missing values duplicates 03-Mutate\u200b mutate format time conversions functions discretize reshape 04-Merge/Join/Concat join concat compare, intersect 05-Explore aggregate sort uniques frequencies crosstabs 06-Analyze/Visualize\u200b univariate bivariate visualize 07-Advanced generate query Introduction \u00b6 This document serves as an accompanying guide/index to the command-line tools discussed in the notebooks. Here we have presented the relevant functions/verbs from command-line-tools that work with tabular data in the context of data-analysis tasks. 00 - Import, Inspect \u00b6 conversion \u00b6 from/to csv in2csv, csv2json csvtk csv2tab, space2tab, tab2csv xsv fmt mlr cat ## compressed data? mlr --prepipe display \u00b6 head, tail csvlook csvtk pretty xsv table mlr head, tail count \u00b6 rows, columns wc xsv count types \u00b6 detect types, conversion mlr put is_* mlr put boolean, int, float, string column names \u00b6 csvcut -n xsv headers csvtk headers mlr label 01 - Subset\u200b \u00b6 rename \u00b6 one/many columns csvtk rename, rename2 mlr rename index \u00b6 create row names/indentifiers nl xsv index subset columns \u00b6 select/exclude cols cut csvcut csvtk select xsv select mlr cut mlr having-fields subset rows \u00b6 select/exclude rows cut csvgrep csvtk filter, filter2, grep xsv search mlr filter sample \u00b6 with (bootstrap) or without replacement (permutation) shuf csvtk sample xsv sample mlr bootstrap, sample, shuffle, decimate split \u00b6 large file into smaller files split csplit xsv split 02-Clean\u200b \u00b6 missing values \u00b6 awk csvstat --nulls mlr put is_null, is_not_null duplicates \u00b6 mlr repeat # create dups datamash rmdup 03-Mutate\u200b \u00b6 mutate \u00b6 create/drop rows, cols awk mlr put csvtk mutate format \u00b6 numerical formatting numfmt datamash round, ceil, floor, trunc, frac time conversions \u00b6 from/to epoch mlr put with strftime, strptime mlr sec2gmt, sec2gmtdate functions \u00b6 apply, map awk mlr put discretize \u00b6 cut numerics into categoricals datamash bin mlr histogram --nbins reshape \u00b6 long/wide to wide/long paste csvtk transpose datamash transpose # pandas-like reshape mlr reshape 04-Merge/Join/Concat \u00b6 join \u00b6 merge tables join csvtk join xsv join concat \u00b6 append/concat tables cat csvstack xsv cat # concat when cols are not same mlr unsparsify compare, intersect \u00b6 rows in A & B, rows in A not in B etc. comm csvtk intersect 05-Explore \u00b6 aggregate \u00b6 group-by, pivot datamash mlr sort \u00b6 sort csvsort uniques \u00b6 uniq csvtk uniq frequencies \u00b6 uniq -c csvtk freq xsv frequency mlr top mlr least-frequent, most-frequent mlr fraction # convert frequencies to percentages crosstabs \u00b6 datamash crosstab 06-Analyze/Visualize\u200b \u00b6 univariate \u00b6 mean/stddev, median, percentiles, skewness/kurtosis, mode, min/max csvstat csvtk stats, stats2 xsv stats mlr stats1, stats2 datamash bivariate \u00b6 correlation/covariance, regression, r-squared mlr stats2 visualize \u00b6 histograms, scatterplots csvtk plot mlr bar 07-Advanced \u00b6 generate \u00b6 random data seq, shuf, pr mlr seqgen mlr put \"urand(), urandint()\" query \u00b6 run sql queries csvsql q -H -d, \"\"\"query\"\"\" # generate a CREATE TABLE query for your csv # super useful when pushing data into a local db (mysql, postgresql etc.) csvsql -i sqlite joined.csv","title":"CLI-Fu"},{"location":"CLI-Fu/#introduction","text":"This document serves as an accompanying guide/index to the command-line tools discussed in the notebooks. Here we have presented the relevant functions/verbs from command-line-tools that work with tabular data in the context of data-analysis tasks.","title":"Introduction"},{"location":"CLI-Fu/#00-import-inspect","text":"","title":"00 - Import, Inspect"},{"location":"CLI-Fu/#conversion","text":"from/to csv in2csv, csv2json csvtk csv2tab, space2tab, tab2csv xsv fmt mlr cat ## compressed data? mlr --prepipe","title":"conversion"},{"location":"CLI-Fu/#display","text":"head, tail csvlook csvtk pretty xsv table mlr head, tail","title":"display"},{"location":"CLI-Fu/#count","text":"rows, columns wc xsv count","title":"count"},{"location":"CLI-Fu/#types","text":"detect types, conversion mlr put is_* mlr put boolean, int, float, string","title":"types"},{"location":"CLI-Fu/#column-names","text":"csvcut -n xsv headers csvtk headers mlr label","title":"column names"},{"location":"CLI-Fu/#01-subset","text":"","title":"01 - Subset\u200b"},{"location":"CLI-Fu/#rename","text":"one/many columns csvtk rename, rename2 mlr rename","title":"rename"},{"location":"CLI-Fu/#index","text":"create row names/indentifiers nl xsv index","title":"index"},{"location":"CLI-Fu/#subset-columns","text":"select/exclude cols cut csvcut csvtk select xsv select mlr cut mlr having-fields","title":"subset columns"},{"location":"CLI-Fu/#subset-rows","text":"select/exclude rows cut csvgrep csvtk filter, filter2, grep xsv search mlr filter","title":"subset rows"},{"location":"CLI-Fu/#sample","text":"with (bootstrap) or without replacement (permutation) shuf csvtk sample xsv sample mlr bootstrap, sample, shuffle, decimate","title":"sample"},{"location":"CLI-Fu/#split","text":"large file into smaller files split csplit xsv split","title":"split"},{"location":"CLI-Fu/#02-clean","text":"","title":"02-Clean\u200b"},{"location":"CLI-Fu/#missing-values","text":"awk csvstat --nulls mlr put is_null, is_not_null","title":"missing values"},{"location":"CLI-Fu/#duplicates","text":"mlr repeat # create dups datamash rmdup","title":"duplicates"},{"location":"CLI-Fu/#03-mutate","text":"","title":"03-Mutate\u200b"},{"location":"CLI-Fu/#mutate","text":"create/drop rows, cols awk mlr put csvtk mutate","title":"mutate"},{"location":"CLI-Fu/#format","text":"numerical formatting numfmt datamash round, ceil, floor, trunc, frac","title":"format"},{"location":"CLI-Fu/#time-conversions","text":"from/to epoch mlr put with strftime, strptime mlr sec2gmt, sec2gmtdate","title":"time conversions"},{"location":"CLI-Fu/#functions","text":"apply, map awk mlr put","title":"functions"},{"location":"CLI-Fu/#discretize","text":"cut numerics into categoricals datamash bin mlr histogram --nbins","title":"discretize"},{"location":"CLI-Fu/#reshape","text":"long/wide to wide/long paste csvtk transpose datamash transpose # pandas-like reshape mlr reshape","title":"reshape"},{"location":"CLI-Fu/#04-mergejoinconcat","text":"","title":"04-Merge/Join/Concat"},{"location":"CLI-Fu/#join","text":"merge tables join csvtk join xsv join","title":"join"},{"location":"CLI-Fu/#concat","text":"append/concat tables cat csvstack xsv cat # concat when cols are not same mlr unsparsify","title":"concat"},{"location":"CLI-Fu/#compare-intersect","text":"rows in A & B, rows in A not in B etc. comm csvtk intersect","title":"compare, intersect"},{"location":"CLI-Fu/#05-explore","text":"","title":"05-Explore"},{"location":"CLI-Fu/#aggregate","text":"group-by, pivot datamash mlr","title":"aggregate"},{"location":"CLI-Fu/#sort","text":"sort csvsort","title":"sort"},{"location":"CLI-Fu/#uniques","text":"uniq csvtk uniq","title":"uniques"},{"location":"CLI-Fu/#frequencies","text":"uniq -c csvtk freq xsv frequency mlr top mlr least-frequent, most-frequent mlr fraction # convert frequencies to percentages","title":"frequencies"},{"location":"CLI-Fu/#crosstabs","text":"datamash crosstab","title":"crosstabs"},{"location":"CLI-Fu/#06-analyzevisualize","text":"","title":"06-Analyze/Visualize\u200b"},{"location":"CLI-Fu/#univariate","text":"mean/stddev, median, percentiles, skewness/kurtosis, mode, min/max csvstat csvtk stats, stats2 xsv stats mlr stats1, stats2 datamash","title":"univariate"},{"location":"CLI-Fu/#bivariate","text":"correlation/covariance, regression, r-squared mlr stats2","title":"bivariate"},{"location":"CLI-Fu/#visualize","text":"histograms, scatterplots csvtk plot mlr bar","title":"visualize"},{"location":"CLI-Fu/#07-advanced","text":"","title":"07-Advanced"},{"location":"CLI-Fu/#generate","text":"random data seq, shuf, pr mlr seqgen mlr put \"urand(), urandint()\"","title":"generate"},{"location":"CLI-Fu/#query","text":"run sql queries csvsql q -H -d, \"\"\"query\"\"\" # generate a CREATE TABLE query for your csv # super useful when pushing data into a local db (mysql, postgresql etc.) csvsql -i sqlite joined.csv","title":"query"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/","text":"Working with Files Larger Than RAM \u00b6 Working with Files Larger Than RAM Start here 1. Break large flat file into many smaller files Links 2. Read file with chunksize and push to a DB Links 3. Use Dask Links 4. Use Spark Spark vs. Dask Blaze Start here \u00b6 Overview of tools When to use Unix tools, Dask, Spark 1. Break large flat file into many smaller files \u00b6 works well for time-series data allows for parallel processing Links \u00b6 Example on splitting a large file 2. Read file with chunksize and push to a DB \u00b6 Work entirely in Pandas Never hold more than, say 200MB in RAM Apply functions Clean - outliers and missing values Compress - use appropriate dtypes Transform - create or drop columns Load file into a DB and query it SQLite or PostgreSQL HDF5store (Pytables) Links \u00b6 The SO post that started it all, see for HDF5 workflow Pandas + SQLite Another SQLite And another, with good advice on compression SQLite vs. MySQL vs. PostgreSQL 3. Use Dask \u00b6 light-weight framework for working with chunked arrays or dataframes across a variety of computational backends. If you\u2019re running into memory issues or CPU boundaries on a single machine when using Pandas, NumPy, or other computations with Python, Dask can help you scale up on all of the cores on a single machine. Dask works well on a single machine to make use of all of the cores on your laptop and process larger-than-memory data Doesn't load the data into memory until you ask for an answer with .compute() the approach used here will straightforwardly scale to even larger datasets analyzed across multiple machines Useful, but for the long term a DB would be better Links \u00b6 THE dask tutorial in .ipynb files Read the Docs! Dask DataFrames and Machine Learning Simple Example Example with 7.5GB NYC 311 Service Requests Data Jake VDP Dask on Single Machines, Clusters -- Continuum Anaytics Distributed Pandas on a Cluster with Dask DataFrames 4. Use Spark \u00b6 Spark vs. Dask \u00b6 Apache Spark is an all-inclusive framework combining distributed computing, SQL queries, machine learning, and more that runs on the JVM and is commonly co-deployed with other Big Data frameworks like Hadoop. It was originally optimized for bulk data ingest and querying common in data engineering and business analytics but has since broadened out. Spark is typically used on small to medium sized cluster but also runs well on a single machine. Dask is a parallel programming library that combines with the Numeric Python ecosystem to provides parallel arrays, dataframes , machine learning, and custom algorithms. It is based on Python and the foundational C/Fortran stack. Dask was originally designed to complement other libraries with parallelism, particular for numeric computing and advanced analytics, but has since broadened out. Dask is typically used on a single machine , but also runs well on a distributed cluster. Dask is smaller, lighter weight than Spark. This means that it has fewer features and instead is intended to be used in conjunction with other libraries like Numpy and Pandas For more, see this Blaze \u00b6 slides tutorial Read the Docs Projects Example - Pandas code vs. Blaze code","title":"Larger Than RAM"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#working-with-files-larger-than-ram","text":"Working with Files Larger Than RAM Start here 1. Break large flat file into many smaller files Links 2. Read file with chunksize and push to a DB Links 3. Use Dask Links 4. Use Spark Spark vs. Dask Blaze","title":"Working with Files Larger Than RAM"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#start-here","text":"Overview of tools When to use Unix tools, Dask, Spark","title":"Start here"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#1-break-large-flat-file-into-many-smaller-files","text":"works well for time-series data allows for parallel processing","title":"1. Break large flat file into many smaller files"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#links","text":"Example on splitting a large file","title":"Links"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#2-read-file-with-chunksize-and-push-to-a-db","text":"Work entirely in Pandas Never hold more than, say 200MB in RAM Apply functions Clean - outliers and missing values Compress - use appropriate dtypes Transform - create or drop columns Load file into a DB and query it SQLite or PostgreSQL HDF5store (Pytables)","title":"2. Read file with chunksize and push to a DB"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#links_1","text":"The SO post that started it all, see for HDF5 workflow Pandas + SQLite Another SQLite And another, with good advice on compression SQLite vs. MySQL vs. PostgreSQL","title":"Links"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#3-use-dask","text":"light-weight framework for working with chunked arrays or dataframes across a variety of computational backends. If you\u2019re running into memory issues or CPU boundaries on a single machine when using Pandas, NumPy, or other computations with Python, Dask can help you scale up on all of the cores on a single machine. Dask works well on a single machine to make use of all of the cores on your laptop and process larger-than-memory data Doesn't load the data into memory until you ask for an answer with .compute() the approach used here will straightforwardly scale to even larger datasets analyzed across multiple machines Useful, but for the long term a DB would be better","title":"3. Use Dask"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#links_2","text":"THE dask tutorial in .ipynb files Read the Docs! Dask DataFrames and Machine Learning Simple Example Example with 7.5GB NYC 311 Service Requests Data Jake VDP Dask on Single Machines, Clusters -- Continuum Anaytics Distributed Pandas on a Cluster with Dask DataFrames","title":"Links"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#4-use-spark","text":"","title":"4. Use Spark"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#spark-vs-dask","text":"Apache Spark is an all-inclusive framework combining distributed computing, SQL queries, machine learning, and more that runs on the JVM and is commonly co-deployed with other Big Data frameworks like Hadoop. It was originally optimized for bulk data ingest and querying common in data engineering and business analytics but has since broadened out. Spark is typically used on small to medium sized cluster but also runs well on a single machine. Dask is a parallel programming library that combines with the Numeric Python ecosystem to provides parallel arrays, dataframes , machine learning, and custom algorithms. It is based on Python and the foundational C/Fortran stack. Dask was originally designed to complement other libraries with parallelism, particular for numeric computing and advanced analytics, but has since broadened out. Dask is typically used on a single machine , but also runs well on a distributed cluster. Dask is smaller, lighter weight than Spark. This means that it has fewer features and instead is intended to be used in conjunction with other libraries like Numpy and Pandas For more, see this","title":"Spark vs. Dask"},{"location":"Python__FilesLargerThanRAM_PandasDaskSpark/#blaze","text":"slides tutorial Read the Docs Projects Example - Pandas code vs. Blaze code","title":"Blaze"}]}